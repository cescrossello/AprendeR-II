[
["index.html", "AprendeR: Parte II Presentación", " AprendeR: Parte II The AprendeR team 2019-02-26 Presentación Esto es una edición preliminar en línea de la 2a parte del libro “AprendeR”. El libro está escrito en R Markdown, usando RStudio como editor de texto y el paquete bookdown para convertir los ficheros markdown en un libro. Este trabajo se publica bajo licencia Atribución-No Comercial-SinDerivadas 4.0 "],
["chap-distr.html", "Lección 1 Distribuciones de probabilidad 1.1 Ejercicios", " Lección 1 Distribuciones de probabilidad R conoce los tipos de distribución de probabilidad más importantes, incluyendo las que mostramos en la tabla siguiente: \\[ \\begin{array}{lll} \\hline \\textbf{Distribución} &amp; {\\textbf{Nombre en R}} &amp; {\\textbf{Parámetros}}\\\\ \\hline \\mbox{Binomial} &amp;{\\texttt{binom}} &amp; \\mbox{medida de la muestra $n$, probabilidad $p$}\\\\ \\mbox{Geométrica} &amp; {\\texttt{geom}} &amp; \\mbox{$p$}\\\\ \\mbox{Hipergeométrica} &amp; {\\texttt{hyper}} &amp; \\mbox{tamaño de la población $N$, número poblacional}\\\\[-0.75ex] &amp; &amp; \\mbox{de éxitos $M$, tamaño de la muestra $n$}\\\\ \\mbox{Poisson} &amp; {\\texttt{pois}} &amp; \\mbox{esperanza $\\lambda$}\\\\ \\mbox{Uniforme} &amp; {\\texttt{unif}} &amp; \\mbox{mínimo, máximo}\\\\ \\mbox{Exponencial} &amp; {\\texttt{exp}} &amp; \\lambda\\\\ \\mbox{Normal} &amp; {\\texttt{norm}} &amp; \\mbox{media $\\mu$, desviación típica $\\sigma$}\\\\ \\mbox{Khi cuadrado} &amp; {\\texttt{chisq}} &amp; \\mbox{número de grados de libertad $df$}\\\\ \\mbox{t de Student} &amp; {\\texttt{t}} &amp; \\mbox{número de grados de libertad $df$}\\\\ \\mbox{F de Fisher} &amp; {\\texttt{f}} &amp; \\mbox{los dos números de grados de libertad} \\\\ \\hline \\end{array} \\] Para cada una de estas distribuciones, R sabe calcular cuatro funciones, que se obtienen añadiendo un prefijo al nombre de la distribución: La función de densidad, con el prefijo d. La función de distribución de probabilidad, con el prefijo p; esta función dispone además del parámetro lower.tail que igualado a FALSE calcula la función de distribución de cola superior: la probabilidad de que una variable aleatoria con esta distribución de probabilidad tome un valor estrictamente mayor que uno dado. Los cuantiles, con el prefijo q. Vectores de números aleatorios con esta distribución, con el prefijo r. La función correspondiente se aplica entonces al valor sobre el que queremos calcular la función y a los parámetros de la distribución (en este orden, y los parámetros en el orden en que los damos en la tabla anterior, cuando hay más de uno). Por ejemplo, sea \\(X\\) una variable aleatoria binomial \\(B(20,0.3)\\), es decir, de tamaño \\(n=20\\) y probabilidad \\(p=0.3\\), y sean \\(f_X\\) su función de densidad y \\(F_X\\) su función de distribución. Calculemos algunos valores de funciones asociadas a esta variable aleatoria. \\(f_X(5)=P(X=5)\\): dbinom(5,20,0.3) ## [1] 0.1788631 Comprobémoslo, recordando que si \\(X\\sim B(n,k)\\), entonces \\(P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}\\): choose(20,5)*0.3^5*0.7^15 ## [1] 0.1788631 \\(f_X(8)=P(X=8)\\): dbinom(8,20,0.3) ## [1] 0.1143967 \\(F_X(5)=P(X\\leq 5)\\): pbinom(5,20,0.3) ## [1] 0.4163708 Comprobémoslo, usando que \\(P(X\\leq 5)=\\sum_{k=0}^5 P(X=k)\\): sum(dbinom(0:5,20,0.3)) ## [1] 0.4163708 \\(F_X(8)=P(X\\leq 8)\\): pbinom(8,20,0.3) ## [1] 0.8866685 \\(P(X&gt;8)\\) pbinom(8,20,0.3,lower.tail=FALSE) ## [1] 0.1133315 En efecto: 1-pbinom(8,20,0.3) ## [1] 0.1133315 El cuantil de orden \\(0.5\\) de \\(X\\), o sea, su mediana: el valor \\(x\\) más pequeño tal que \\(P(X\\leq x)\\geq 0.5\\) qbinom(0.5,20,0.3) ## [1] 6 Comprobemos que \\(P(X\\leq 6)\\geq 0.5\\) y en cambio \\(P(X\\leq 5)&lt; 0.5\\): pbinom(qbinom(0.5,20,0.3),20,0.3) ## [1] 0.6080098 pbinom(qbinom(0.5,20,0.3)-1,20,0.3) ## [1] 0.4163708 El cuantil de orden \\(0.25\\) de \\(X\\), es decir, su primer cuartil: qbinom(0.25,20,0.3) ## [1] 5 Un vector aleatorio de 10 valores generado con la variable aleatoria \\(X\\): rbinom(10,20,0.3) ## [1] 7 5 8 8 9 6 6 5 5 6 Dos vectores aleatorios más, de 10 valores cada uno, generados con la variable aleatoria \\(X\\): rbinom(10,20,0.3) ## [1] 7 6 3 6 5 8 7 9 4 7 rbinom(10,20,0.3) ## [1] 7 7 7 4 7 4 6 9 5 9 Del mismo modo, si estamos trabajando con una variable aleatoria \\(Y\\) de Poisson con parámetro \\(\\lambda=5\\): \\(P(Y=8)\\): dpois(8,5) ## [1] 0.06527804 \\(P(Y\\leq 8)\\): ppois(8,5) ## [1] 0.9319064 El cuantil de orden 0.6 de \\(Y\\): qpois(0.6,5) ## [1] 5 Un vector aleatorio de 20 valores generado con la variable aleatoria \\(Y\\): rpois(20,5) ## [1] 4 5 7 2 3 8 7 7 4 6 2 3 4 5 5 4 3 5 5 5 Si no entramos ningún parámetro en las funciones asociadas a la distribución normal, R entiende que se trata de la normal estándar (con media \\(\\mu=0\\) y desviación típica \\(\\sigma=1\\)): por ejemplo, las dos instrucciones siguientes nos dan el valor \\(f_Z(0.3)\\) de la función densidad de una normal estándar \\(Z\\) aplicada a 0.3 (que no es igual a \\(P(Z=0.3)\\)): dnorm(0.3) ## [1] 0.3813878 dnorm(0.3,0,1) ## [1] 0.3813878 Las funciones densidad y distribución de una variable aleatoria se pueden dibujar con la función curve. curve(dnorm(x,0,1.5),-5,5,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;&quot;) Figura 1.1: Función densidad de una variable N(0,1) curve(pnorm(x,0,1.5),-5,5,xlab=&quot;&quot;,ylab=&quot;&quot;,main=&quot;&quot;) Figura 1.2: Función distribución de una variable N(0,1) 1.1 Ejercicios Modelo de test (1) Sea \\(f\\) la función de densidad de una variable aleatoria normal con \\(\\mu=0.2\\) y \\(\\sigma=1.2\\). Dad el valor de \\(f(0.5)\\) redondeado a 4 cifras decimales. (2) Sea \\(X\\) una variable aleatoria normal con \\(\\mu=0.2\\) y \\(\\sigma=1.2\\). Dad el valor de \\(P(3\\leq X\\leq 7)\\) redondeado a 4 cifras decimales. (3) Sea \\(X\\) una variable aleatoria \\(B(10,0.2)\\). Dad el valor de \\(P(3\\leq X\\leq 7)\\) redondeado a 4 cifras decimales. (4) Dad una instrucción que calcule la mediana de una lista de 20 números aleatorios generados con distribución \\(B(10,0.2)\\). No deis el resultado, solo la instrucción. Respuestas al test (1) 0.3222 (2) 0.0098 (3) 0.3221 (4) median(rbinom(20,10,0.2)) "],
["chap-muestreo.html", "Lección 2 Conceptos básicos de muestreo 2.1 Tipos de muestreo 2.2 Muestreo aleatorio con R 2.3 Guía rápida 2.4 Ejercicios", " Lección 2 Conceptos básicos de muestreo En todo estudio estadístico hemos de distinguir entre población, que es un conjunto de sujetos con una o varias características que podemos medir y deseamos estudiar, y muestra, un subconjunto de una población. Por ejemplo, si quisiéramos estudiar alguna característica de los estudiantes de grado de la UIB, entenderíamos que estos forman la población de interés, y si entonces escogiéramos al azar 10 estudiantes de cada grado, obtendríamos una muestra de esta población. Pero también podríamos considerar los estudiantes de grado de la UIB como una muestra de la población de los estudiantes universitarios españoles: depende del estudio que queramos realizar. Recordad que, cuando disponemos de un conjunto de datos obtenidos midiendo una o varias características sobre los sujetos de una muestra, podemos llevar a cabo dos tipos de análisis estadístico: Exploratorio o descriptivo: su objetivo es resumir, representar y explicar los datos de la muestra. Para llevarlo a cabo, se usan técnicas de estadística descriptiva como las que hemos descrito en lecciones anteriores. Inferencial o confirmatorio: su objetivo es deducir (inferir), a partir de los datos de la muestra, información significativa sobre el total de la población. A menudo esta inferencia pasa por contrastar una hipótesis sobre alguna propiedad de las características de la población. Las técnicas que se usan en este caso forman la estadística inferencial. Por ejemplo, supongamos que hemos tomado una muestra de estudiantes de la UIB y sabemos sus calificaciones en un semestre concreto y sus números de hermanos. En un estudio exploratorio simplemente describiríamos estos datos mediante estadísticos y gráficos, mientras que usaríamos técnicas de estadística inferencial para deducir información sobre la población de todos los estudiantes de la UIB a partir de esta muestra: ¿Cuál estimamos que ha sido la nota media de los estudiantes de la UIB en el semestre en cuestión? La distribución de los números de hermanos en estudiantes de la UIB, ¿es similar a la del conjunto de la población española? ¿Es verdad que los estudiantes de la UIB con más hermanos tienen tendencia a tener mejores notas? Un estudio inferencial suele desglosarse en los pasos siguientes: Establecer la característica que se desea estimar o la hipótesis que se desea contrastar. Determinar la información (los datos) que se necesita para hacerlo. Diseñar un experimento que permita recoger estos datos; este paso incluye: Decidir qué tipo de muestra se va a tomar y su tamaño. Elegir las técnicas adecuadas para realizar las inferencias deseadas a partir de la muestra que se tomará. Tomar una muestra y medir los datos deseados sobre los individuos que la forman. Aplicar las técnicas de inferencia elegidas con el software adecuado. Obtener conclusiones. Si las conclusiones son fiables y suficientes, redactar un informe; en caso contrario, volver a empezar. En la próxima sección nos centraremos en las técnicas de muestreo: los métodos generales para seleccionar muestras representativas de una población que tenemos a nuestra disposición en el tercer paso de la lista anterior. 2.1 Tipos de muestreo Existen muchos tipos de muestreo, cada uno de los cuales proporciona una muestra representativa de la población en algún sentido. A continuación describimos de forma breve algunas de estas técnicas. Muestreo aleatorio con y sin reposición Un muestreo aleatorio consiste en seleccionar una muestra de la población de manera que todas las muestras del mismo tamaño sean equiprobables; es decir, que si fijamos el número de individuos de la muestra, cualquier conjunto de ese número de individuos tenga la misma probabilidad de ser seleccionado. Hay dos tipos básicos de muestreo aleatorio que vale la pena distinguir. Para ilustrarlos, supongamos que disponemos de una urna con 100 bolas numeradas del 1 al 100, de la que queremos extraer una muestra de 15 bolas. La Figura 2.1 representa dicha urna. Figura 2.1: Una urna de 100 bolas Una manera de hacerlo sería repetir 15 veces el proceso de sacar una bola de la urna, anotar su número y devolverla a la urna. El tipo de muestra obtenida de esta manera recibe el nombre de muestra aleatoria con reposición, o simple (una m.a.s., para abreviar). Observad que con este procedimiento una misma bola puede aparecer varias veces en una muestra, y que todos los subconjuntos de 15 bolas “con posibles repeticiones” tienen la misma probabilidad de obtenerse. Un posible resultado serían las bolas azules de la Figura 2.2; la bola azul más oscuro ha sido escogida dos veces en la muestra. Figura 2.2: Una muestra aleatoria simple Otra manera de extraer nuestra muestra sería repetir 15 veces el proceso de sacar una bola de la urna pero ahora sin devolverla. Esto es equivalente a extraer de golpe 15 bolas de la urna. Estas muestras no tienen bolas repetidas, y cualquier selección de 15 bolas diferentes tiene la misma probabilidad de ser la obtenida. En este caso se habla de una muestra aleatoria sin reposición. Un posible resultado serían las bolas azules de la Figura 2.3. Figura 2.3: Una muestra aleatoria sin reposición Cuando el tamaño de la población es muy grande en relación a la muestra, la probabilidad de que haya repeticiones en una muestra aleatoria simple es muy pequeña. Esto nos permite entender en este caso que los muestreos aleatorios con y sin reposición son equivalentes en el sentido siguiente: puesto que un muestreo con reposición da muy probablemente una muestra con todos sus elementos diferentes, aceptamos que una muestra obtenida sin reposición ha sido obtenida permitiendo repeticiones y que por tanto es simple. A modo de ejemplo, vamos a calcular la probabilidad de al menos una repetición en muestras aleatorias simples de diferentes tamaños de una población de 12,000 individuos (aproximadamente, el número de estudiantes de la UIB) y representar estas probabilidades en un gráfico. Recordad que la probabilidad de que los sujetos de una muestra aleatoria simple de tamaño \\(n\\) tomada de una población de \\(n\\) individuos sean todos diferentes es \\[ \\frac{N(N-1)(N-2)\\cdots (N-n+1)}{N^n} \\] y por lo tanto la probabilidad de que en una muestra haya algún elemento repetido es 1 menos este valor. Esta probabilidad es la que calcula la función f(N,n) del bloque de código siguiente. f=function(N,n){1-prod((N:(N-n+1))/N)} prob=sapply(1:200,f,N=12000) plot(1:200,prob,type=&quot;l&quot;,lwd=2,xlab=&quot;n&quot;,ylab=&quot;probabilidad&quot;, main=&quot;&quot;,xaxp=c(0,200,20),yaxp=c(0,1,10)) abline(h=0.01,col=&quot;red&quot;) text(160,0.04,labels=&quot;probabilidad 0.01&quot;,col=&quot;red&quot;,cex=0.7) Figura 2.4: Probabilidad de repetición en una m.a.s. de n estudiantes de la UIB La curva negra representa las probabilidades deseadas. Hemos añadido al gráfico una línea horizontal que marca la probabilidad 0.01 y que muestra que la probabilidad de alguna repetición en una m.a.s. de 16 o menos estudiantes de la UIB es inferior al 1%. Así, por ejemplo, una muestra aleatoria sin reposición de 10 estudiantes de la UIB podría haberse obtenido perfectamente tomando los estudiantes con reposición, porque la probabilidad de alguna repetición en una m.a.s. como esta es muy pequeña: 0.004. En cambio, es difícil de creer que una muestra aleatoria de 200 estudiantes diferentes de la UIB sea simple, porque la probabilidad de alguna repetición en una m.a.s. como esta es grande: 0.811. La mayoría de técnicas de estadística inferencial que se pueden usar para muestras aleatorias simples se pueden considerar igualmente válidas para muestras aleatorias sin reposición, siempre y cuando el tamaño de la población sea muy grande en relación al de la muestra (por dar una regla, al menos unas 1000 veces mayor). Si el tamaño de la población es relativamente pequeño por comparación a la muestra, algunas de estas técnicas se pueden salvar aplicando correcciones adecuadas para compensar el efecto del tamaño de la población, y otras directamente pierden toda validez. En todo caso, conviene remarcar que si queremos tomar una muestra aleatoria con o sin reposición de una población, es necesario disponer de una lista completa de todos sus individuos para poder sortear a quién vamos a seleccionar. Esto no siempre es posible. ¿Alguien tiene la lista completa de, pongamos, todos los diabéticos de España? ¿Que incluya los que aún no saben que lo son? Por lo tanto, en la vida real no siempre podemos tomar muestras aleatorias en el sentido que hemos explicado. Muestreo sistemático Una manera muy sencilla de obtener una muestra de una población cuando disponemos de una lista ordenada de sus individuos es tomarlos a intervalos constantes: cada quinto individuo, cada décimo individuo. Podemos añadir una componente aleatoria escogiendo al azar el primer individuo que elegimos, y a partir del cual empezamos a contar. Así, por ejemplo, si de una clase de 100 estudiantes quisiéramos escoger una muestra de 10, podríamos elegir un estudiante al azar, y a partir de él, por orden alfabético, elegir el décimo estudiante, el vigésimo, el trigésimo, etc.; si al llegar al final de la lista de clase no hubiéramos completado la muestra, volveríamos al principio de la misma. A esta técnica se la llama muestreo sistemático, aleatorio si además el primer sujeto se escoge de manera aleatoria. Por ejemplo, la Figura 2.5 describe una muestra aleatoria sistemática de 15 bolas de nuestra urna de 100 bolas: hemos empezado a escoger por la bola roja oscura, que ha sido elegida al azar, y a partir de ella hemos tomado 1 de cada 7 bolas, volviendo al principio cuando hemos llegado al final de la lista de bolas Figura 2.5: Una muestra aleatoria sistemática Cuando no disponemos de una lista de toda la población pero sí que tenemos una manera de acceder de manera ordenada a sujetos de la misma (por ejemplo, enfermos que acuden a un hospital), podemos realizar un muestreo sistemático tomando los sujetos a intervalos constantes a medida que los encontramos hasta completar el tamaño deseado de la muestra. Por ejemplo, para escoger una muestra de 10 estudiantes de la UIB, podríamos escoger cada décimo estudiante que entrase en un edificio del Campus por una puerta concreta hasta llegar a los 10. Cuando el orden de los individuos de la población en la lista es aleatorio, el muestreo sistemático aleatorio es equivalente al muestreo aleatorio sin reposición. Pero en general este no es el caso, y se pueden producir sesgos. Por poner un caso extremo, si una clase de 100 estudiantes estuviera formada por 50 parejas de hermanos y tomáramos una muestra sistemática de 50 estudiantes, eligiéndolos por orden alfabético de los apellidos uno sí, uno no, es seguro que no aparecería ninguna pareja de hermanos en la muestra (porque dos hermanos son siempre consecutivos en la lista, y en nuestra muestra no hay ningún par de sujetos consecutivos). En cambio, la probabilidad de que una muestra aleatoria sin reposición del mismo tamaño contuviera una pareja de hermanos es prácticamente 1; en concreto esta probabilidad sería \\[ \\frac{100\\times 98\\times 96\\times\\cdots\\times 2}{100\\times 99\\times 98\\times\\cdots\\times 51}=\\frac{2^{50}\\cdot 50!^2}{100!}=0.999999999999989. \\] Muestreo aleatorio estratificado Este tipo de muestreo se utiliza cuando la población está clasificada en estratos que son de interés para la propiedad estudiada. En este caso, se toma una muestra aleatoria de cada estrato y se unen en una muestra global. A este proceso se le llama muestreo aleatorio estratificado. Normalmente, se impone que la composición por estratos de la muestra global mantenga las proporciones de la población original; es decir, que el tamaño de la muestra de cada estrato represente el mismo porcentaje del total de la muestra que el estrato correspondiente en la población completa. Por ejemplo, los estratos podrían ser grupos de edad, y entonces la muestra de cada grupo de edad se tomaría proporcional a la fracción que representa dicho grupo de edad en la población total. O podrían ser los sexos anatómicos, y procuraríamos que nuestra muestra estuviera formada por un 50% de hombres y un 50% de mujeres. O, en las Islas Baleares, los estratos podrían ser las islas, de manera que la muestra tomada en cada isla fuera proporcional a la población relativa de la misma dentro del conjunto total de la comunidad autónoma. Por continuar con nuestra urna de 100 bolas, supongamos que contiene 40 bolas de un color y 60 de otro color según muestra la Figura 2.6. Figura 2.6: Nuestra urna ahora tiene 2 estratos Para tomar una muestra aleatoria estratificada de 15 bolas, considerando como estratos los dos colores, tomaríamos una muestra aleatoria de 6 bolas del primer color y una muestra aleatoria de 9 bolas del segundo color. De esta manera, los porcentajes de colores en la muestra serían los mismos que en la urna. La Figura 2.7 describe una muestra obtenida de esta manera. Figura 2.7: Una muestra aleatoria estratificada En todo caso, el muestreo por estratos solo es necesario si esperamos que las características de la propiedad poblacional que queremos estudiar varíen según el estrato. Por ejemplo, si queremos tomar una muestra para estimar la altura media de los españoles adultos y no creemos que la altura de un español adulto dependa de su provincia de origen, no hay ninguna necesidad de esforzarse en tomar una muestra de cada provincia de manera que todas las provincias estén representadas proporcionalmente en la muestra. Muestreo por conglomerados El proceso de obtener y estudiar una muestra aleatoria en algunos casos es caro o difícil, incluso aunque dispongamos de la lista completa de la población. Imaginemos que quisiéramos estudiar los hábitos de alimentación de los estudiantes de Primaria de Baleares. Para ello, previo permiso de la autoridad competente, tendríamos que seleccionar una muestra representativa de los escolares de Baleares. Seguramente podríamos disponer de su lista completa y por lo tanto podríamos tomar una muestra aleatoria, pero entonces acceder a las niñas y niños que la formasen seguramente significaría contactar con unos pocos alumnos de muchos centros de primaria, lo que volvería el proceso lento y costoso. Y eso si la Conselleria d’Educació nos facilitase la lista completa de alumnos. Una alternativa posible sería, en vez de extraer una muestra aleatoria de todos los estudiantes de Primaria, escoger primero al azar unas pocas aulas de primaria de colegios de las Baleares, a las que llamamos en este contexto conglomerados (clusters), y formar entonces nuestra muestra con todos los alumnos de estas aulas. Y es que es mucho más sencillo poseer la lista completa de estudiantes de unas pocas aulas que conseguir la lista completa de todos los estudiantes de todos los colegios, y mucho más barato ir a unos pocos colegios concretos que ir a todos los colegios de las Islas a entrevistar a unos pocos estudiantes en cada centro. Efectuamos también un muestreo por conglomerados cuando para medir algunas características de los ejemplares de una planta en un bosque concreto, cuadriculamos la superficie del bosque, escogemos una muestra aleatoria de sectores de la cuadrícula (serían los conglomerados de este ejemplo) y estudiamos las plantas de interés contenidas en los sectores elegidas. Volviendo de nuevo a nuestra urna, supongamos que sus 100 bolas se agrupan en 20 conglomerados de 5 bolas cada uno según las franjas verticales de la Figura 2.8 (donde mantenemos la clasificación en dos colores para poder comparar el resultado del muestreo por conglomerados con el estratificado). Figura 2.8: Nuestra urna ahora tiene 2 estratos y 20 conglomerados Para obtener una muestra aleatoria por conglomerados de tamaño 15, escogeríamos al azar 3 conglomerados y la muestra estaría formada por sus bolas. La Figura 2.9 describe una muestra obtenida de esta manera: los conglomerados escogidos están marcados en azul. Figura 2.9: Una muestra aleatoria por conglomerados Observad la diferencia entre el muestreo estratificado y el muestreo por conglomerados: En una muestra estratificada se escoge una muestra aleatoria de cada estrato existente. En una muestra por conglomerados se escogen algunos conglomerados al azar y se incluye en la muestra todos sus elementos. Muestreos no aleatorios Cuando la selección de la muestra no es aleatoria, se habla de muestreo no aleatorio. En realidad es el tipo más frecuente de muestreo porque, en muchos casos, nos tenemos que conformar con los sujetos disponibles. Por ejemplo, en la UIB, para estimar la opinión que de un profesor tienen los alumnos de una clase, se consulta solo a los estudiantes que voluntariamente rellenan la encuesta de opinión, que de ninguna manera forman una muestra aleatoria: el perfil del estudiante que contesta voluntariamente una encuesta de este tipo está muy definido y no viene determinado por el azar. En este caso se trataría de una muestra autoseleccionada. Otro tipo de muestreos no aleatorios son los oportunistas. Este es el caso, por ejemplo, si para estimar la opinión que de un profesor tienen los alumnos de una asignatura se visita un día la clase y se pasa la encuesta a los estudiantes que ese día asistieron a clase. De nuevo, puede que los alumnos presentes no sean representativos del alumnado de la asignatura (pueden ser los más aplicados, o los que no tienen la gripe, o a los que la asignatura no les coincide con otra). Veamos otros ejemplos de muestreo oportunista. Supongamos que queremos estudiar una característica de los animales de una determinada especie en un hábitat, y la medimos en los animales que capturamos u observamos. Estos ejemplares no tienen por qué ser representativos de la población: a lo mejor son los menos espabilados. O imaginad que tenéis una bolsa con bolas de diferentes tamaños. Si las removéis bien, las pequeñas tenderán a ir a parar al fondo y las grandes a quedar en la parte superior. Por lo tanto, si tomáis una muestra de la capa superior (que será lo más cómodo), no será representativa del total de la bolsa. La Figura 2.10 describe una muestra oportunista de nuestra urna: sus 15 primeras bolas. Aunque toda muestra de un mismo tamaño tiene la misma probabilidad de obtenerse por medio de un muestreo aleatorio sin reposición, es difícil de creer que esta muestra sea aleatoria; basta que calculéis cuál es la probabilidad de que en una muestra aleatoria de 15 bolas de nuestra urna todas tengan el mismo color: \\[ \\frac{40\\times 39\\times \\cdots\\times 26+60\\times 59\\times \\cdots\\times 46}{100\\times 99\\times \\cdots\\times 86}=0 \\] Figura 2.10: Una muestra oportunista Las técnicas de estadística inferencial no se pueden aplicar a muestras no aleatorias, pero normalmente son las únicas que podemos conseguir. En este caso, lo que se suele hacer es describir en detalle las características de la muestra para justificar que, pese a no ser aleatoria, es representativa de la población y podría haber sido aleatoria. Por ejemplo, la muestra oportunista anterior de nuestra urna no es de ninguna manera representativa de su contenido por lo que refiere al color de las bolas. Muestreo polietápico En el ejemplo de los estudiantes de Primaria, la muestra final de estudiantes ha estado formada por todos los individuos de las aulas elegidas. Otra opción podría haber sido, tras seleccionar la muestra aleatoria de conglomerados, tomar de alguna manera una muestra aleatoria de cada uno de ellos. Por ejemplo, algunos estudios poblacionales a nivel estatal se realizan solamente en algunas provincias escogidas aleatoriamente, en las que luego se encuesta una muestra aleatoria de habitantes. Este sería un ejemplo de muestreo polietápico, en el que la muestra no se obtiene en un solo paso, sino mediante diversas elecciones sucesivas. La Figura 2.11 muestra un ejemplo sencillo de muestreo polietápico de nuestra urna: hemos elegido al azar 5 conglomerados (marcados en azul) y de cada uno de ellos hemos elegido 3 bolas al azar sin reposición. Figura 2.11: Una muestra polietápica Otro ejemplo enrevesado (pero real) de muestreo polietápico sería, para elegir una muestra de adolescentes de una ciudad grande, escoger en primer lugar 4 secciones censales al azar; a continuación, escoger al azar 10 manzanas de cada una de estas secciones censales y una esquina de cada manzana; finalmente, recorrer cada manzana en sentido horario a partir de la esquina seleccionada y visitar un portal de cada tres, entrevistando todos los habitantes de 13 a 19 años en las casas o fincas visitadas. En este proceso, hemos realizado dos muestreos aleatorios sin reposición (de secciones censales y de manzanas) y un muestreo sistemático aleatorio (los portales). Si además los adolescentes que estudiamos al final no son todos los que viven en los portales seleccionados sino solo los que encontramos en casa el día que los visitamos, este muestreo oportunista significaría un cuarto paso en la formación de la muestra. Existen otros tipos de muestreo, solo hemos explicado los más comunes. En cualquier caso, lo importante es recordar que el estudio estadístico que se realice a posteriori deberá ser diferente según el muestreo usado. Por ejemplo, no se pueden usar las mismas técnicas para analizar una muestra aleatoria simple que una muestra por conglomerados. 2.2 Muestreo aleatorio con R En este curso estudiaremos las propiedades de las diferentes técnicas de estimación solamente para el caso de muestreo aleatorio simple, es decir, al azar y con reposición, o al azar sin reposición si la población es muy, muy grande en comparación con la muestra. Recordemos que un método de selección al azar de muestras de tamaño \\(n\\) (es decir, formadas por \\(n\\) individuos) de una cierta población produce muestras aleatorias simples (m.a.s.) cuando todas las muestras posibles de \\(n\\) individuos (con posibles repeticiones) tienen la misma probabilidad de ser elegidas. El tener una m.a.s. de una población junto con un tamaño muestral adecuado \\(n\\) nos asegurará que la estimación que hagamos sea muy probablemente correcta. La manera más sencilla de llevar a cabo un muestreo aleatorio simple es numerar todos los individuos de una población y sortearlos eligiendo números de uno en uno como si se tratase de una lotería, por ejemplo con algún generador de números aleatorios. Esto se puede llevar a cabo fácilmente con R. R dispone de un generador de muestras aleatorias de un vector. La función básica es sample(x, n, replace=...) donde: x es un vector o un número natural \\(x\\), en cuyo caso R entiende que representa el vector 1,2,…,\\(x\\); n es el tamaño de la muestra que deseamos extraer; el parámetro replace puede igualarse a TRUE, y será una muestra aleatoria simple, es decir, con reposición, o a FALSE, y será una muestra aleatoria sin reposición. Este último es su valor por defecto, por lo que no es necesario especificarlo si se quiere obtener una muestra sin reposición. Los dos primeros parámetros han de entrarse en este orden o igualados a los parámetros x y size, respectivamente. Así, por ejemplo, para obtener una m.a.s. de 15 números entre 1 y 100, podemos entrar: sample(100,15,replace=TRUE) ## [1] 16 63 62 23 16 84 68 16 32 61 10 20 59 12 30 Naturalmente, y como ya nos encontramos en la Lección 1 cuando generábamos vectores aleatorios con una distribución dada, cada ejecución de sample con los mismos parámetros puede dar lugar a muestras diferentes, y todas ellas tienen la misma probabilidad de aparecer: sample(100,15,replace=TRUE) ## [1] 95 96 5 3 54 16 93 88 22 96 25 83 27 97 55 sample(100,15,replace=TRUE) ## [1] 49 94 97 95 48 2 10 60 89 89 2 67 36 10 60 sample(100,15,replace=TRUE) ## [1] 84 68 38 25 22 62 40 55 50 17 62 67 32 86 16 Veamos cómo extraer una m.a.s de una tabla de datos. Recordemos el data frame iris, que recoge medidas de pétalos y sépalos de 150 flores de tres especies de iris. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Si queremos extraer una m.a.s. de 15 ejemplares (filas) de esta tabla de datos, podemos generar con sample una m.a.s. de índices de filas de la tabla y a continuación crear un data frame que contenga solo estas filas: x=sample(dim(iris)[1],15,replace=TRUE) #Los índices de la m.a.s. muestra_iris=iris[x,] #La m.a.s. de la tabla iris muestra_iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 5 5.0 3.6 1.4 0.2 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 33 5.2 4.1 1.5 0.1 setosa ## 42 4.5 2.3 1.3 0.3 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 98 6.2 2.9 4.3 1.3 versicolor ## 15 5.8 4.0 1.2 0.2 setosa ## 107 4.9 2.5 4.5 1.7 virginica ## 16 5.7 4.4 1.5 0.4 setosa ## 14 4.3 3.0 1.1 0.1 setosa ## 26 5.0 3.0 1.6 0.2 setosa ## 123 7.7 2.8 6.7 2.0 virginica ## 20 5.1 3.8 1.5 0.3 setosa ## 144 6.8 3.2 5.9 2.3 virginica Recordad que dim aplicado a un dataframe nos da un vector con sus dimensiones: sus números de filas y de columnas, en este orden. Por lo tanto, dim(iris)[1] es el número de filas de iris. Si solo quisiéramos una muestra aleatoria de longitudes de pétalos, podríamos aplicar directamente la función sample al vector correspondiente: muestra_long_pet=sample(iris$Petal.Length,15,replace=TRUE) muestra_long_pet ## [1] 4.4 4.2 6.6 1.4 3.9 1.5 6.6 1.5 5.8 4.7 1.5 1.6 4.5 5.6 1.3 El hecho de que funciones como sample o los generadores de vectores aleatorios con una cierta distribución de probabilidad fijada, como rnorm o rbinom, produzcan… pues eso, vectores aleatorios, puede tener inconvenientes a la hora de reproducir una simulación. R permite “fijar” el resultado de una función aleatoria con la instrucción set.seed. Sin entrar en detalles sobre cómo funcionan, los diferentes algoritmos que usa R para generar números aleatorios usan una semilla de aleatoriedad, que se modifica después de la ejecución del algoritmo, y por eso cada vez dan un resultado distinto. Pero, para una semilla fija, el algoritmo da el mismo resultado siempre. Lo que hace la función set.seed es igualar esta semilla al valor que le entramos. Si tras aplicar esta función a un número concreto ejecutamos una instrucción que genere un vector aleatorio de una longitud fija con una distribución fija, el resultado será siempre el mismo. Veamos un ejemplo de su efecto, generando muestras aleatorias simples de 10 longitudes de pétalos de flores iris con diferentes semillas de aleatoriedad: sample(iris$Petal.Length,10,replace=TRUE) ## [1] 4.7 4.5 4.5 3.6 1.3 1.4 5.6 5.6 1.4 5.9 set.seed(20) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 6.4 5.3 1.3 3.5 5.7 5.2 1.1 1.5 1.4 4.5 set.seed(20) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 6.4 5.3 1.3 3.5 5.7 5.2 1.1 1.5 1.4 4.5 sample(iris$Petal.Length,10,replace=TRUE) ## [1] 6.3 5.0 1.4 5.3 1.4 4.1 1.5 1.3 1.6 6.7 set.seed(10) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 4.8 1.6 3.6 5.6 1.4 1.4 1.3 1.3 4.0 3.6 set.seed(10) sample(iris$Petal.Length,10,replace=TRUE) ## [1] 4.8 1.6 3.6 5.6 1.4 1.4 1.3 1.3 4.0 3.6 Ejecutado inmediatamente después de set.seed(20), sample(iris$Petal.Length,10,replace=TRUE) siempre da lo mismo. Y ejecutado después de set.seed(10), sample(iris$Petal.Length,10,replace=TRUE) vuelve a dar siempre da lo mismo, pero diferente de con set.seed(20). La función set.seed no solo fija el resultado de la primera instrucción tras ella que genere un vector aleatorio, sino que, como fija la semilla de aleatoriedad y las funciones posteriores la modificarán de manera determinista, también fija los resultados de todas las instrucciones siguientes que generen vectores aleatorios. set.seed(100) sample(10,3) ## [1] 4 3 5 sample(10,3) ## [1] 1 5 4 sample(10,3) ## [1] 9 4 5 set.seed(100) sample(10,3) ## [1] 4 3 5 sample(10,3) ## [1] 1 5 4 sample(10,3) ## [1] 9 4 5 Si queréis volver a “reiniciar” la semilla de la aleatoriedad tras haber usado un set.seed, podéis usar set.seed(NULL). set.seed(100) sample(10,3) ## [1] 4 3 5 set.seed(NULL) sample(10,3) ## [1] 6 9 3 set.seed(100) sample(10,3) ## [1] 4 3 5 set.seed(NULL) sample(10,3) ## [1] 5 2 1 A veces querremos tomar diversas muestras aleatorias de una misma población y calcular algo sobre ellas. Para hacerlo podemos usar la función replicate. La sintaxis básica es replicate(n, instrucción) donde n es el número de repeticiones de la instrucción. Por ejemplo, para tomar 10 muestras aleatorias simples de 15 longitudes de pétalos de flores iris, podemos hacer: muestras=replicate(10, sample(iris$Petal.Length,15,replace=TRUE)) muestras ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 4.8 4.2 1.5 6.3 5.3 4.7 4.9 5.6 4.0 1.6 ## [2,] 4.8 1.3 5.2 5.7 3.5 1.4 1.4 4.1 3.7 1.5 ## [3,] 5.6 3.6 4.2 1.5 4.9 5.5 3.3 4.5 1.7 1.4 ## [4,] 1.7 1.5 4.0 5.4 1.7 1.9 5.8 1.6 1.9 1.4 ## [5,] 4.2 5.9 1.9 4.6 1.4 1.6 5.6 5.9 5.0 4.5 ## [6,] 5.9 6.0 3.9 1.6 5.7 4.8 1.3 4.1 5.4 1.6 ## [7,] 1.0 1.3 1.2 5.1 5.8 1.6 5.8 1.7 5.8 4.5 ## [8,] 4.9 1.4 3.7 6.7 5.9 1.4 4.9 4.4 1.7 1.6 ## [9,] 5.4 5.5 4.1 5.1 5.8 1.5 6.1 5.0 4.6 1.3 ## [10,] 4.4 1.5 5.1 4.0 1.0 1.3 4.5 1.3 4.4 6.6 ## [11,] 5.8 5.0 5.0 4.0 1.3 5.1 5.5 4.7 4.5 1.6 ## [12,] 1.3 4.6 4.4 4.5 3.5 4.4 1.7 5.6 5.7 3.9 ## [13,] 1.3 5.5 1.4 5.1 5.6 4.4 1.3 5.1 1.3 5.9 ## [14,] 1.4 5.4 4.0 5.4 4.0 5.1 4.9 1.4 5.8 1.9 ## [15,] 1.4 1.7 1.3 4.7 1.5 4.0 4.0 4.2 4.0 5.6 Observad que R ha organizado los 10 vectores generados con el replicate como columnas de una matriz. Si solo nos hubiera interesado calcular las medias, redondeadas a 2 cifras decimales, de 10 muestras aleatorias simples de 15 longitudes de pétalos de flores iris, podríamos haber hecho medias=replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) medias ## [1] 3.85 4.35 3.85 4.07 4.15 3.59 4.03 3.73 3.97 4.13 En este caso, como el resultado de la instrucción que iteramos es un solo número, los resultados del replicate forman un vector. ¿Y si quisiéramos la media y la desviación típica muestral de 10 muestras de estas? No podemos usar sin más dos replicate, como en replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 4.33 3.74 3.76 4.53 4.40 4.43 2.72 3.99 3.49 4.57 replicate(10,round(sd(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 1.87 1.38 1.66 1.77 2.02 1.87 1.87 1.86 1.87 1.74 porque el conjunto de muestras de las que hemos calculado la media en el primer replicate muy probablemente habrá sido diferente del conjunto de muestras de las que hemos calculado la desviación típica en el segundo replicate. Lo más adecuado es definir una función que calcule un vector con estos dos valores, y luego usarla dentro de un único replicate. info=function(x){round(c(mean(x),sd(x)),2)} info_lp=replicate(10,info(sample(iris$Petal.Length,15,replace=TRUE))) info_lp ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 4.21 3.27 4.02 3.95 3.29 3.62 4.59 3.47 3.40 2.88 ## [2,] 1.46 1.69 1.70 1.73 1.70 1.73 1.38 1.61 2.01 1.62 En este último caso, R ha organizado la información obtenida como columnas de una matriz: la primera fila son las medias y la segunda las desviaciones típicas. Naturalmente, la función set.seed permite “fijar” el resultado de un replicate que incluya la generación de números aleatorios: set.seed(1000) replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 3.63 3.18 4.05 4.59 4.13 2.51 3.61 3.31 3.63 3.96 set.seed(1000) replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2)) ## [1] 3.63 3.18 4.05 4.59 4.13 2.51 3.61 3.31 3.63 3.96 Para terminar esta lección, damos una función sencilla para efectuar muestreos sistemáticos aleatorios. El objetivo es, dado un vector de longitud \\(N\\), obtener una muestra de tamaño \\(n\\). Lo que haremos será tomar el cociente por exceso \\(k=\\lceil N/n\\rceil\\) de \\(N\\) entre \\(n\\) para determinar el período con el que tenemos que tomar los elementos de manera que todos los elementos puedan ser escogidos. A continuación elegimos al azar un elemento del vector con sample y a partir de él generamos una progresión aritmética de \\(n\\) elementos y paso \\(k\\), volviendo al inicio del vector si llegamos al final sin haber completado la muestra (lo que especificamos tomando los valores de la progresión aritmética módulo \\(N\\)). sist.sample=function(N,n){ k=ceiling(N/n) x0=sample(N,1) seq(x0,length.out=n,by=k)%%N } Por ejemplo, una muestra sistemática de 10 flores iris se podría obtener de la manera siguiente: x=sist.sample(dim(iris)[1],10) #Los índices de la muestra sistemática muestra_sist_iris=iris[x,] #La muestra de la tabla iris muestra_sist_iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 49 5.3 3.7 1.5 0.2 setosa ## 64 6.1 2.9 4.7 1.4 versicolor ## 79 6.0 2.9 4.5 1.5 versicolor ## 94 5.0 2.3 3.3 1.0 versicolor ## 109 6.7 2.5 5.8 1.8 virginica ## 124 6.3 2.7 4.9 1.8 virginica ## 139 6.0 3.0 4.8 1.8 virginica ## 4 4.6 3.1 1.5 0.2 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 34 5.5 4.2 1.4 0.2 setosa Como 150/10=15, podemos observar que los índices avanzan de 15 en 15 a partir de uno que ha sido generado al azar. Ejemplo 2.1 Por si tenéis curiosidad, el código siguiente ha producido las diferentes muestras aleatorias de las figuras de la Sección 2.1: La muestra aleatoria simple de la Figura 2.2: set.seed(42) sort(sample(100,15,rep=TRUE)) ## [1] 14 26 29 46 47 52 65 66 71 72 74 84 92 94 94 La muestra aleatoria sin reposición de la Figura 2.3: set.seed(42) sort(sample(100,15,rep=FALSE)) ## [1] 13 23 29 40 42 50 61 62 65 70 81 83 91 92 93 La muestra aleatoria sistemática de la Figura 2.5: set.seed(42) X0=sample(100,1) (X0+7*(0:14))%%100 ## [1] 92 99 6 13 20 27 34 41 48 55 62 69 76 83 90 La muestra aleatoria estratificada de la Figura 2.7: set.seed(42) c(sort(sample(40,6,replace=FALSE)),sort(sample(41:100,9, replace =FALSE))) ## [1] 11 19 24 31 37 40 48 54 65 66 79 80 81 85 91 La muestra aleatoria por conglomerados de la Figura 2.9: set.seed(42) Y=sort(sample(20,3,rep=FALSE)) #Los conglomerados sort(rep(Y,each=5)+20*(0:4)) ## [1] 6 18 19 26 38 39 46 58 59 66 78 79 86 98 99 Los conglomerados escogidos (ordenados de izquierda a derecha) han sido el 6, el 18 y el 19. La muestra aleatoria polietápica de la Figura 2.11: set.seed(42) Y=sample(20,5,replace=FALSE) sort(sapply(Y,FUN=function(x){sample(x+20*(0:4),3,replace=FALSE)})) ## [1] 6 11 19 31 38 55 58 59 66 71 75 78 86 95 99 Los conglomerados escogidos han sido el 6, el 11, el 15, el 18 y el 19. 2.3 Guía rápida sample(x, n, replace=...) genera una muestra aleatoria de tamaño n del vector x, con reposición si igualamos replace a TRUE y sin reposición si lo igualamos a FALSE (su valor por defecto). Si x es un número natural \\(x\\), representa el vector 1,2,…,\\(x\\). set.seed permite fijar la semilla de aleatoriedad. replicate(n,expresión) evalúa n veces la expresión, y organiza los resultados como las columnas de una matriz (o un vector, si el resultado de cada expresión es unidimensional). 2.4 Ejercicios Modelo de test (1) Queremos escoger 100 estudiantes de grado de la UIB para preguntarles cuántas horas semanales estudian. Como creemos que el tipo de estudio cursado influye en este dato, clasificamos los estudiantes según el centro (facultad o escuela) en el que están matriculados, y tomaremos una muestra al azar de cada centro, por sorteo a partir de la lista de todos los matriculados en ese centro y de manera que el tamaño de la muestra de cada centro sea proporcional al número de matriculados en el mismo. ¿De qué tipo de muestreo se tratará? Muestreo aleatorio simple Muestreo aleatorio estratificado Muestreo aleatorio sin reposición Muestreo aleatorio por conglomerados Muestreo aleatorio sistemático Ninguno de los anteriores (2) Con una sola instrucción, calculad la media de una muestra aleatoria sin reposición de 15 elementos escogidos de un vector numérico llamado \\(X\\). (3) Con una sola instrucción, extraed un subdataframe del dataframe iris formado por una muestra aleatoria sin reposición de 40 filas, y llamadlo muestra. Y antes de contestar, comprobad que funciona. (4) Con una sola instrucción, calculad un vector formado por las medias de 100 muestras aleatorias sin reposición de 20 elementos cada una escogidos de un vector numérico llamado \\(X\\) y llamadlo medias. Respuestas al test (1) b (2) mean(sample(X,15)) (3) muestra=iris[sample(dim(iris)[1],40),] (4) medias=replicate(100,mean(sample(X,20))) "],
["estimacion.html", "Lección 3 Estimación puntual 3.1 Estimación máximo verosímil 3.2 Guía rápida 3.3 Ejercicios", " Lección 3 Estimación puntual En un estudio inferencial, una vez tomada la muestra y obtenidos los datos sobre sus miembros, el siguiente paso es inferir, es decir, deducir información sobre la población a partir de estos datos. Dicha información se puede deducir de dos formas: Suponiendo que conocemos el modelo al que se ajusta la población: es decir, suponiendo que conocemos el tipo de distribución de la variable aleatoria que modela la característica de la población en la que estamos interesados, pero desconocemos uno o varios parámetros de los que depende dicha distribución (observad que si lo sabemos todo sobre esta distribución, ya no hace falta tomar muestras para inferir algo sobre ella). Así, podemos saber (o suponer) que las longitudes de los ejemplares adultos de una cierta especie se distribuyen según una variable aleatoria normal, pero desconocer sus parámetros \\(\\mu\\) (media) y \\(\\sigma\\) (desviación típica), y usar este conocimiento para inferir información sobre dichas longitudes a partir de las de una muestra: por ejemplo, para estimar con un cierto margen de error su longitud media. Si estamos en este caso, hablaremos de estimación paramétrica. Suponiendo que desconocemos qué tipo de distribución tiene la variable aleatoria que modela la característica que nos interesa (aunque a veces necesitaremos saber algo de esta distribución; por ejemplo, si es simétrica o no). En este caso, hablaremos de estimación no paramétrica. En ambos casos, existen tres vías para obtener información sobre los parámetros de la distribución (conocida o desconocida) de la variable aleatoria que nos interesa: Estimación puntual. Se trata de obtener expresiones matemáticas, llamadas estimadores puntuales, que aplicadas a los valores de una muestra nos dan una aproximación (el término exacto es una estimación) del valor de dicho parámetro para la población. A modo de ejemplo, la media aritmética de los datos \\(x_1,\\ldots,x_n\\) de una muestra, \\[ \\overline{x}=\\frac{x_1+\\cdots +x_n}{n}, \\] es un estimador del valor medio (valor esperado, esperanza) de la variable aleatoria de la que hemos extraído la muestra. Estimación por intervalos de confianza. Se trata de obtener intervalos que contengan con probabilidad alta el parámetro objeto de estudio. Trataremos este tema en la Lección 4. Contraste de hipótesis. Grosso modo, se establecen dos hipótesis opuestas sobre el parámetro o, más en general, sobre la distribución de la variable aleatoria, y se contrastan para intentar decidir cuál es la verdadera. Los estudiaremos en próximas lecciones. En esta lección hablaremos de la estimación puntual. Para empezar, es obvio que no toda fórmula matemática sirve para estimar de manera sensata el valor de un parámetro. Por ejemplo, si queréis estimar la media de las alturas de los habitantes de una población y disponéis de una muestra aleatoria de las mismas, no tomáis la raíz cuadrada de la altura máxima en la muestra como estimación de la altura media de la población, ¿verdad? Lo que habéis hecho toda la vida, y seguiréis haciendo en este curso, ha sido calcular la media de las alturas en la muestra y dar ese valor como estimación de la altura media poblacional. Y es lo correcto, porque la media muestral es siempre un estimador insesgado de la media poblacional y muy a menudo es además su estimador máximo verosímil, Veamos qué significan estas propiedades. Insesgado: Los valores de un estimador sobre muestras aleatorias de una población forman una variable aleatoria con una distribución de probabilidad propia, llamada genéricamente muestral. Decimos entonces que un estimador es insesgado cuando el valor esperado de la variable aleatoria que define coincide con el valor del parámetro poblacional que se quiere estimar. Por ejemplo, si se toman muestras aleatorias con o sin reposición, la media muestral es siempre un estimador insesgado del valor medio poblacional: su valor esperado es el valor medio poblacional. Máximo verosímil: Cada muestra aleatoria de una población tiene una probabilidad de obtenerse que no solo depende de la muestra, sino también de la distribución de probabilidad de la variable aleatoria poblacional. Si la distribución poblacional es de un tipo concreto (Bernoulli, normal, …), esta probabilidad depende de sus parámetros. Decimos entonces que un estimador es máximo verosímil cuando el resultado que da sobre cada muestra aleatoria es el valor del parámetro poblacional que maximiza la probabilidad de obtenerla. Por ejemplo, si lanzamos una moneda al aire \\(n\\) veces y calculamos la proporción de veces que obtenemos cara, esa proporción muestral \\(\\widehat{p}\\) es el estimador máximo verosímil de la probabilidad \\(p\\) de obtener cara con esa moneda. Esto quiere decir que, de entre todas las distribuciones binomiales \\(B(n,p)\\) que pueden modelar el número de caras que obtenemos al lanzar \\(n\\) veces nuestra moneda, aquella que asigna mayor probabilidad al número de caras que hemos obtenido es la que tiene como parámetro \\(p\\) la frecuencia relativa de caras \\(\\widehat{p}\\) que hemos observado. Para algunas distribuciones, el método de máxima verosimilitud de estimación de sus parámetros da lugar a fórmulas cerradas más o menos sencillas, pero en otros casos nos tenemos que conformar con un valor aproximado obtenido mediante algún método numérico. 3.1 Estimación máximo verosímil A continuación recordamos una lista de los estimadores máximo verosímiles de los parámetros de las distribuciones más comunes a partir de una muestra aleatoria simple: Para la familia Bernoulli, el estimador máximo verosímil del parámetro \\(p\\) es la proporción muestral de éxitos \\(\\widehat{p}\\). Este estimador es además insesgado. Para la familia Poisson, el estimador máximo verosímil del parámetro \\(\\lambda\\) es la media muestral \\(\\overline{X}\\). Este estimador es de nuevo insesgado. Para la familia geométrica, el estimador máximo verosímil del parámetro \\(p\\) es \\({1}/{\\overline{X}}\\). Este estimador es sesgado. Para la familia exponencial, el estimador máximo verosímil del parámetro \\(\\lambda\\) es \\({1}/{\\overline{X}}\\). Este estimador también es sesgado. Para la familia normal, los estimadores máximo verosímiles de la media \\(\\mu\\), la desviación típica \\(\\sigma\\) y la varianza \\(\\sigma^2\\) son, respectivamente, la media muestral \\(\\overline{X}\\), la desviación típica “verdadera” \\(S_X\\) y la varianza “verdadera” \\(S_X^2\\). Además, \\(\\overline{X}\\) es un estimador insesgado de \\(\\mu\\). La varianza verdadera \\(S_X^2\\) no es un estimador insesgado de \\(\\sigma^2\\), pero sí que lo es la varianza muestral \\(\\widetilde{S}^2\\). Y ninguna de las dos desviaciones típicas, ni la “verdadera” \\(S_X\\) ni la muestral \\(\\widetilde{S}_X\\), es un estimador insesgado de \\(\\sigma\\); si necesitáis un estimador insesgado de la desviación típica de una variable aleatoria normal a partir de una muestra aleatoria simple, lo podéis encontrar en la correspondiente entrada de la Wikipedia. No obstante, el beneficio de usar este estimador insesgado no suele compensar lo complicado de su cálculo. Cuando se estima algún parámetro de una distribución a partir de una muestra, es conveniente aportar el error típico, o estándar, como medida de la finura de la estimación. Recordemos que el error típico de un estimador es la desviación típica de su distribución muestral, y que el error típico de una estimación a partir de una muestra es la estimación del error típico del estimador usando dicha muestra. Como veremos en la próxima lección, estos errores típicos serán una ingrediente clave en el cálculo de intervalos de confianza. Veamos un ejemplo sencillo. Supongamos que tenemos una muestra aleatoria simple de tamaño \\(n\\) de una variable \\(X\\) que sigue una distribución Bernoulli de probabilidad poblacional \\(p\\) desconocida que queremos estimar. Por ejemplo, puede ser que tengamos una moneda posiblemente trucada, la hayamos lanzado 100 veces al aire y hayamos anotado los resultados (1, cara, 0, cruz), y a partir de este experimento queramos estimar la probabilidad de sacar cara con esta moneda. O que hayamos anotado para 100 individuos de una población elegidos al azar si tienen o no una determinada enfermedad (1 significa que sí, 0 que no) y a partir de esta muestra deseemos estimar la prevalencia de la enfermedad en la población, es decir, la proporción real de enfermos, que coincide con la probabilidad de que un individuo elegido al azar tenga dicha enfermedad. Tomemos, para fijar ideas, la siguiente muestra de tamaño 100: x=c(0,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0, 0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0, 1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0) En este caso, podemos estimar \\(p\\) mediante la proporción muestral de éxitos \\(\\widehat{p}\\), que coincide con la media muestral. El error típico de este estimador es \\(\\sqrt{p(1-p)/n}\\), y el error típico de una estimación concreta es \\(\\sqrt{\\widehat{p}(1-\\widehat{p})/n}\\). Por lo tanto, a mano podemos estimar \\(p\\) y calcular el error típico de dicha estimación de la manera siguiente: n=length(x) #Tamaño de la muestra estim.p=mean(x) #Proporción muestral estim.p ## [1] 0.22 error.tip.p=sqrt(estim.p*(1-estim.p)/n) #Error típico de la estimación error.tip.p ## [1] 0.0414 De esta manera, estimamos que \\(p\\)=0.22 con un error típico de 0.04. Con R podemos estimar un parámetro de una distribución por el método de máxima verosimilitud a partir de una muestra y además obtener el error típico de dicha estimación usando la función fitdistr del paquete MASS. Esta función calcula los estimadores máximo verosímiles de los parámetros de la mayoría de las familias de distribuciones disponibles en R. Su sintaxis básica es fitdistr(x, densfun=..., start=...) donde x es la muestra, un vector numérico. El valor de densfun ha de ser el nombre de la familia de distribuciones; se tiene que entrar entre comillas y puede tomar, entre otros, los valores siguientes: &quot;chi-squared&quot;, &quot;exponential&quot;, &quot;f&quot;, &quot;geometric&quot;, &quot;lognormal&quot;, &quot;normal&quot; y &quot;poisson&quot;. La lista de distribuciones a las que se puede aplicar, que podéis consultar en la Ayuda de la función, no incluye la Bernoulli ni la binomial. Si fitdistr no dispone de una fórmula cerrada para el estimador máximo verosímil de algún parámetro, usa un algoritmo numérico para aproximarlo que requiere de un valor inicial para arrancar. Este valor (o valores) se puede especificar igualando el parámetro start a una list con cada parámetro a estimar igualado a un valor inicial. Para algunas distribuciones, como la &quot;t&quot;, fitdistr sabe tomar valores iniciales razonables, y no es necesario especificar el parámetro start. Pero para otras distribuciones, como por ejemplo la &quot;chi-squared&quot;, es obligatorio especificarlo. Para las distribuciones que disponen de fórmula cerrada, como la &quot;normal&quot; o la &quot;poisson&quot;, se tiene que omitir el parámetro start. Como no podemos usar fitdistr para estimar el parámetro \\(p\\) de una Bernoulli (los autores del paquete debieron de considerar que era más fácil estimarlo directamente), vamos a usarla en otro ejemplo. Consideremos la siguiente muestra y de 100 valores generados con distribución de Poisson de parámetro \\(\\lambda=10\\): set.seed(100) y=rpois(100,10) set.seed(NULL) y ## [1] 8 10 9 12 10 11 8 12 7 11 11 12 7 10 9 8 11 7 6 12 10 12 7 ## [24] 7 10 6 7 15 9 9 7 8 15 11 12 5 14 4 7 14 8 14 10 4 9 8 ## [47] 11 11 10 12 7 14 7 9 10 3 10 7 9 21 14 6 13 3 10 6 3 13 9 ## [70] 12 8 11 10 11 11 8 6 17 7 8 10 12 15 12 13 10 9 12 8 11 12 4 ## [93] 10 8 5 8 8 10 8 11 Vamos a estimar el parámetro \\(\\lambda\\) de una distribución Poisson que haya generado este vector: library(MASS) fitdistr(y, densfun=&quot;poisson&quot;) ## lambda ## 9.560 ## (0.309) El resultado dice que el valor estimado de \\(\\lambda\\) es 9.56, con un error típico en esta estimación de 0.31. Veámoslo directamente: el estimador máximo verosímil de \\(\\lambda\\) es la media aritmética \\(\\overline{X}\\) y el error típico de este estimador es \\(\\sqrt{\\lambda}/\\sqrt{n}\\) (recordad que la desviación típica de una Poisson de parámetro \\(\\lambda\\) es \\(\\sqrt{\\lambda}\\) y que el error típico de la media muestral es la desviación típica poblacional dividida por la raíz cuadrada del tamaño de la muestra), por lo que el error típico de una estimación es \\(\\sqrt{\\overline{X}}/\\sqrt{n}\\). mean(y) ## [1] 9.56 sqrt(mean(y)/length(y)) ## [1] 0.309 También podemos estimar la media y la desviación típica de una variable normal que hubiera producido esta muestra. fitdistr(y, densfun=&quot;normal&quot;) ## mean sd ## 9.560 3.083 ## (0.308) (0.218) Observad que la estimación de la desviación típica que nos da fitdistr es la desviación típica “verdadera” (que es su estimador máximo verosímil) y no la muestral: sd(y) ## [1] 3.1 sqrt((length(y)-1)/length(y))*sd(y) ## [1] 3.08 Vamos a estimar ahora el número de grados de libertad de una t de Student que hubiera producido esta muestra. fitdistr(y, densfun=&quot;t&quot;) ## m s df ## 9.509 2.752 9.923 ## (0.300) (0.307) (8.489) ¡Vaya!, aparte del número de grados de libertad, df, han aparecido parámetros que no esperábamos. Los parámetros m y s son los parámetros de posición, \\(\\mu\\), y de escala, \\(\\sigma\\), respectivamente, que definen una familia más general de distribuciones t de Student (si os interesa, consultad esta entrada de la Wikipedia). Las que usamos en este curso tienen \\(\\mu=0\\) y \\(\\sigma=1\\). ¿Cómo podríamos estimar los grados de libertad de una t de Student de las nuestras? Especificando dentro de fitdistr los valores de los parámetros que queremos que tomen un valor concreto: en este caso, añadiendo m=0 y s=1. fitdistr(y, densfun=&quot;t&quot;, m=0, s=1) ## Error in fitdistr(y, densfun = &quot;t&quot;, m = 0, s = 1): &#39;start&#39; must be a named list Ahora R nos pide que demos un valor inicial al número de grados de libertad, df, para poder arrancar el algoritmo numérico que usará. Vamos a inicializarlo a 1, y de paso veremos cómo se usa este parámetro: fitdistr(y, densfun=&quot;t&quot;,m=0,s=1,start=list(df=1)) ## Warning in stats::optim(x = c(8L, 10L, 9L, 12L, 10L, 11L, 8L, 12L, 7L, 11L, : one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly ## Warning in dt((x - m)/s, df, log = TRUE): NaNs produced ## df ## 0.3727 ## (0.0428) Obtenemos un número estimado de grados de libertad de la t de Student de aproximadamente 0.37 grados de libertad (sí, los grados de libertad de una t de Student pueden ser un número real positivo cualquiera). Por otro lado, R nos avisa de que el resultado es poco de fiar, pero tampoco nos importa mucho, porque el objetivo era mostrar un ejemplo de cómo fijar valores de parámetros, igualándolos a dichos valores, y cómo especificar el parámetro start, como una list donde asignamos a cada parámetro un valor inicial. El resultado de fitdistr es una list, y por lo tanto el valor de cada estimador y su error típico se pueden obtener con los sufijos adecuados. En concreto, los valores estimados forman la componente estimate y los errores típicos la componente sd. Para obtenerlos directamente, basta usar los sufijos $estimate y $sd, respectivamente: fitdistr(y,&quot;poisson&quot;)$estimate #Estimación de lambda ## lambda ## 9.56 fitdistr(y,&quot;poisson&quot;)$sd #Error típico ## lambda ## 0.309 fitdistr(y,&quot;normal&quot;)$estimate #Estimaciones ## mean sd ## 9.56 3.08 fitdistr(y,&quot;normal&quot;)$estimate[1] #Estimación de mu ## mean ## 9.56 fitdistr(y,&quot;normal&quot;)$estimate[2] #Estimación de sigma ## sd ## 3.08 3.2 Guía rápida fitdistr del paquete MASS, sirve para calcular los estimadores máximo verosímiles de los parámetros de una distribución a partir de una muestra. El resultado es una list que incluye los objetos estimate (los valores estimados) y sd (los errores típicos de las estimaciones). Sus parámetros principales son: densfun: el nombre de la familia de distribuciones, entre comillas. start: permite fijar el valor inicial del algoritmo numérico para calcular el estimador, si la función lo requiere. 3.3 Ejercicios Modelo de test (1) Las distribuciones de Weibull tienen dos parámetros, forma, shape, y escala, scale. Supongamos que los datos siguientes siguen una distribución de Weibull: 2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72. Calculad el estimador máximo verosímil del parámetro de escala de esta distribución, redondeado a 3 cifras decimales. Tenéis que dar el resultado (sin ceros innecesarios a la derecha), no cómo lo habéis calculado. (2) Generad, con semilla de aleatoriedad igual a 42, una secuencia aleatoria de 100 valores con distribución geométrica Ge(0.6). A continuación estimad por máxima verosimilitud el parámetro \\(p\\) de una distribución geométrica que haya generado dicha muestra y dad como respuesta a esta pregunta el error típico de esta estimación redondeado a 3 cifras decimales. Tenéis que dar el resultado (sin ceros innecesarios a la derecha), no cómo lo habéis calculado. Respuestas al test (1) 2.116 (2) 0.038 "],
["chap-IC.html", "Lección 4 Intervalos de confianza 4.1 Intervalo de confianza para la media basado en la t de Student 4.2 Intervalos de confianza para la proporción poblacional 4.3 Intervalo de confianza para la varianza de una población normal 4.4 Bootstrap 4.5 Guía rápida 4.6 Ejercicios", " Lección 4 Intervalos de confianza En esta lección explicamos cómo calcular con R algunos intervalos de confianza básicos. Recordad que un intervalo de confianza del \\(q\\times 100\\%\\) (con \\(q\\) entre 0 y 1) para un parámetro poblacional (la media, la desviación típica, la proporción poblacional de una variable Bernoulli, …) es un intervalo obtenido aplicando a una muestra una fórmula que garantiza (si se cumplen una serie de condiciones necesarias sobre la distribución de la variable aleatoria poblacional que en cada caso dependen del parámetro y de la fórmula) que el \\(q\\times 100\\%\\) de las veces que la aplicásemos a una muestra aleatoria simple de la misma población, el intervalo resultante contendría el parámetro poblacional. Esto es lo que significa lo de “confianza del \\(q\\times 100\\%\\)”: que suponemos que nuestra muestra es aleatoria simple y confiamos en que pertenece al \\(q\\times 100\\%\\) de las ocasiones en las que la fórmula acierta y da un intervalo que contiene el parámetro deseado. Algunas de las funciones que aparecen en esta lección volverán a salir en la próxima, ya que aunque calculan intervalos de confianza, su función principal es en realidad efectuar contrastes de hipótesis. 4.1 Intervalo de confianza para la media basado en la t de Student Supongamos que queremos estimar a partir de una m.a.s. la media \\(\\mu\\) de una población que sigue una distribución normal o tomando la muestra grande (por fijar una cota, de tamaño 40 o mayor). En esta situación, si \\(\\overline{X}\\), \\(\\widetilde{S}_{X}\\) y \\(n\\) son, respectivamente, la media muestral, la desviación típica muestral y el tamaño de la muestra, un intervalo de confianza del \\(q\\times 100\\%\\) para \\(\\mu\\) es \\[\\begin{equation} \\overline{X}\\pm t_{n-1,(1+q)/2} \\cdot \\frac{\\widetilde{S}_{X}}{\\sqrt{n}} \\tag{4.1} \\end{equation}\\] donde \\(t_{n-1,(1+q)/2}\\) es el cuantil de orden \\((1+q)/2\\) de una variable aleatoria con distribución t de Student con \\(n-1\\) grados de libertad. Fijaos en que \\(\\widetilde{S}_{X}/\\sqrt{n}\\) es el error típico de la estimación de la media. A la hora de calcular este intervalo de confianza, tenemos dos posibles situaciones. Una, típica de ejercicios, es cuando de la muestra sólo conocemos su media muestral \\(\\overline{X}\\), su desviación típica muestral \\(\\widetilde{S}_X\\) y su tamaño \\(n\\). Si los denotamos por x.b, sdm y n, respectivamente, y denotamos el nivel de confianza en tanto por uno \\(q\\) por conf.level, podemos calcular los extremos de este intervalo de confianza con la expresión siguiente: x.b+(qt((1+conf.level)/2,n-1)*sdm/sqrt(n))*c(-1,1) Ahora bien, “en la vida real” lo usual es disponer de un vector numérico X con los valores de la muestra. En este caso, podemos usar la función t.test de R, que, entre otra información, calcula estos intervalos de confianza para \\(\\mu\\). Si solo nos interesa el intervalo de confianza, podemos usar la sintaxis siguiente: t.test(X,conf.level=...)$conf.int donde tenemos que igualar el parámetro conf.level al nivel de confianza \\(q\\) en tanto por uno. Si \\(q=0.95\\), no hace falta entrarlo, porque es su valor por defecto. Ejemplo 4.1 Tenemos una muestra de pesos en gramos de 28 recién nacidos con luxación severa de cadera: pesos=c(2466,3941,2807,3118,3175,3515,3317,3742,3062,3033,2353,3515,3260,2892, 4423,3572,2750,3459,3374,3062,3205,2608,3118,2637,3438,2722,2863,3513) Vamos a suponer que nuestra muestra es aleatoria simple y que los pesos al nacer de los bebés con esta patología siguen una distribución normal. A partir de esta muestra, queremos calcular un intervalo de confianza del 95% para el peso medio de un recién nacido con luxación severa de cadera, y ver si contiene el peso medio de la población global de recién nacidos, que es de unos 3400 g. Como suponemos que la variable aleatoria poblacional es normal, para calcular un intervalo de confianza del 95% para su valor medio podemos usar la fórmula basada en la distribución t de Student, y por lo tanto la función t.test: t.test(pesos)$conf.int ## [1] 2998 3355 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 El intervalo que obtenemos es (2997.8, 3355) y está completamente a la izquierda del peso medio global de 3400 g, por lo que tenemos evidencia (a un 95% de confianza) de que los niños con luxación severa de cadera pesan de media al nacer por debajo de la media global. La apostilla entre paréntesis “a un 95% de confianza” aquí significa que hemos basado esta conclusión en un intervalo obtenido con una fórmula que acierta con una probabilidad del 95%, en el sentido de que el 95% de las ocasiones que aplicamos esta fórmula a una m.a.s. de una variable aleatoria normal, produce un intervalo que contiene la media de esta variable. Observad que el resultado de t.test(pesos)$conf.int tiene un atributo, conf.level, que indica su nivel de confianza. En principio este atributo no molesta para nada en cálculos posteriores con los extremos de este intervalo de confianza, pero si os molesta, lo podéis quitar igualándolo a NULL. IC.lux=t.test(pesos)$conf.int attr(IC.lux,&quot;conf.level&quot;)=NULL IC.lux ## [1] 2998 3355 Veamos cómo podríamos haber obtenido este intervalo directamente con la fórmula (4.1): x=mean(pesos) sdm=sd(pesos) n=length(pesos) conf.level=0.95 x+(qt((1+conf.level)/2,n-1)*sdm/sqrt(n))*c(-1,1) ## [1] 2998 3355 Como podéis ver, coincide con el intervalo obtenido con la función t.test. Ejemplo 4.2 Vamos a comprobar con un experimento esto de la “confianza” de los intervalos de confianza, y en concreto de la fórmula (4.1). Vamos a generar al azar una Población de 10,000,000 “individuos” con distribución normal estàndard. Vamos a tomar 200 muestras aleatorias simples de tamaño 50 de esta población y calcularemos el intervalo de confianza para la media poblacional usando dicha fórmula. Finalmente, contaremos cuántos de estos intervalos de confianza contienen la media de la Población. Fijaremos la semilla de aleatoriedad para que el experimento sea reproducible y podáis comprobar que no hacemos trampa. En otras simulaciones habríamos obtenido resultados mejores o peores, es lo que tienen las simulaciones aleatorias. set.seed(42) Poblacion=rnorm(10^7) #La población mu=mean(Poblacion) # La media poblacional M=replicate(200, sample(Poblacion,50,replace=TRUE)) # Las muestras dim(M) ## [1] 50 200 Tenemos una matriz M de 200 columnas y 50 filas, donde cada columna es una m.a.s. de nuestra población. Vamos a aplicar a cada una de estas muestras la función t.test para calcular un intervalo de confianza del 95% y luego contaremos los aciertos, es decir, cuántos de ellos contienen la media poblacional IC.t=function(X){t.test(X)$conf.int} ICs=apply(M,FUN=IC.t,MARGIN=2) Aciertos=length(which((mu&gt;=ICs[1,]) &amp; (mu&lt;=ICs[2,]))) Aciertos ## [1] 189 Hemos acertado 189 veces, es decir, un 94.5% de los intervalos obtenidos contienen la media poblacional. No hemos quedado muy lejos del 95% predicho por la teoría. Para visualizar mejor los aciertos, vamos a dibujar los intervalos apilados en un gráfico, donde aparecerán en azul claro los que aciertan y en rojo los que no aciertan. plot(1,type=&quot;n&quot;,xlim=c(-0.8,0.8),ylim=c(0,200),xlab=&quot;Valores&quot;,ylab=&quot;Repeticiones&quot;,main=&quot;&quot;) seg.int=function(i){ color=&quot;light blue&quot;; if((mu&lt;ICs[1,i]) | (mu&gt;ICs[2,i])){color = &quot;red&quot;} segments(ICs[1,i],i,ICs[2,i],i,col=color,lwd=2) } sapply(1:200,FUN=seg.int) abline(v=mu,lwd=2) Figura 4.1: Aciertos y errores en 200 Intervalos de confianza al 95% Fijaos en que los errores no se distribuyen por igual a los dos lados, hay muchos más intervalos que dejan la media poblacional a su izquierda que a su derecha, mientras que, en teoría, tendríamos que esperar que en la mitad de los errores la media poblacional estuviera a la izquierda del intervalo calculado y en la otra mitad a la derecha. Cosas de la aleatoriedad. 4.2 Intervalos de confianza para la proporción poblacional En esta sección consideramos el caso en que la población objeto de estudio sigue una distribución Bernoulli y queremos estimar su probabilidad de éxito (o proporción poblacional) \\(p\\). Para ello, tomamos una muestra aleatoria simple de tamaño \\(n\\) y número de éxitos \\(x\\), y, por lo tanto, de proporción muestral de éxitos \\(\\widehat{p}_X=x/n\\). El método “exacto” de Clopper-Pearson para calcular un intervalo de confianza del \\(q\\times 100\\%\\) para \\(p\\) se basa en el hecho de que, en estas condiciones, el valor de \\(x\\) sigue una distribución binomial \\(B(n,p)\\). Este método se puede usar siempre, sin ninguna restricción sobre la muestra, y consiste básicamente en encontrar los valores \\(p_0\\) y \\(p_1\\) tales que \\[ \\sum_{k=x}^n\\binom{n}{k}p_0^k(1-p_0)^{n-k}=(1-q)/2,\\qquad \\displaystyle\\sum_{k=0}^x\\binom{n}{k}p_1^k(1-p_1)^{n-k}=(1-q)/2 \\] y dar el intervalo \\((p_0,p_1)\\). Para calcular este intervalo se puede usar la función binom.exact del paquete epitools. Su sintaxis es binom.exact(x,n,conf.level) donde x y n representan, respectivamente, el número de éxitos y el tamaño de la muestra, y conf.level es \\(q\\), el nivel de confianza en tanto por uno. El valor por defecto de conf.level es 0.95. Ejemplo 4.3 Supongamos que, de una muestra de 15 enfermos tratados con un cierto medicamento, solo 1 ha desarrollado taquicardia. Queremos conocer un intervalo de confianza del 95% para la proporción de enfermos tratados con este medicamento que presentan este efecto adverso. Tenemos una población Bernoulli, formada por los enfermos tratados con el medicamento en cuestión, donde los éxitos son los enfermos que desarrollan taquicardia. La fracción de éstos es la fracción poblacional \\(p\\) para la que queremos calcular el intervalo de confianza del 95%. Para ello cargamos el paquete epitools y usamos binom.exact: library(epitools) binom.exact(1,15) ## x n proportion lower upper conf.level ## 1 1 15 0.0667 0.00169 0.319 0.95 El resultado de la función binom.exact es un data frame; el intervalo de confianza deseado está formado por los números en las columnas lower (extremo inferior) y upper (extremo superior): binom.exact(1,15)$lower ## [1] 0.00169 binom.exact(1,15)$upper ## [1] 0.319 Hemos obtenido el intervalo de confianza (0.002,0.319): podemos afirmar con un nivel de confianza del 95% que el porcentaje de enfermos tratados con este medicamento que presentan este efecto adverso está entre el 0.2% y el 31.9%. Supongamos ahora que el tamaño \\(n\\) de la muestra aleatoria simple es grande; de nuevo, pongamos, \\(n\\geq 40\\). En esta situación, podemos usar el Método de Wilson para aproximar, a partir del Teorema Central del Límite, un intervalo de confianza del parámetro \\(p\\) al nivel de confianza \\(q\\times 100\\%\\), mediante la fórmula \\[ \\frac{\\widehat{p}_{X}+\\frac{z_{(1+q)/2}^2}{2n}\\pm z_{(1+q)/2}\\sqrt{\\frac{\\widehat{p}_{X}(1-\\widehat{p}_{X})}{n}+\\frac{z_{(1+q)/2}^2}{4n^2}}}{1+\\frac{z_{(1+q)/2}^2}{n}} \\] donde \\(z_{(1+q)/2}\\) es el cuantil de orden \\((1+q)/2\\) de una variable aleatoria normal estándar. Para calcular este intervalo se puede usar la función binom.wilson del paquete epitools. Su sintaxis es binom.wilson(x,n,conf.level) con los mismos parámetros que binom.exact. Ejemplo 4.4 Supongamos que tratamos 45 ratones con un agente químico, y 10 de ellos desarrollan un determinado cáncer de piel. Queremos calcular un intervalo de confianza al 90% para la proporción \\(p\\) de ratones que desarrollan este cáncer de piel al ser tratados con este agente químico. Como 45 es relativamente grande, usaremos el método de Wilson. Para comparar los resultados, usaremos también el método exacto. Fijaos que, en este ejemplo, \\(q=0.9\\). binom.wilson(10,45,0.9) ## x n proportion lower upper conf.level ## 1 10 45 0.222 0.138 0.338 0.9 binom.exact(10,45,0.9) ## x n proportion lower upper conf.level ## 1 10 45 0.222 0.126 0.348 0.9 Con el método de Wilson obtenemos el intervalo (0.138,0.338) y con el método de Clopper-Pearson, el intervalo (0.126,0.348), un poco más ancho: hay una diferencia en los extremos de alrededor de un punto porcentual. Supongamos finalmente que la muestra aleatoria simple es considerablemente más grande que la usada en el método de Wilson y que, además, la proporción muestral de éxitos \\(\\widehat{p}_{X}\\) está alejada de 0 y de 1. Una posible manera de formalizar estas condiciones es requerir que \\(n\\geq 100\\) y que \\(n\\widehat{p}_{X}\\geq 10\\) y \\(n(1-\\widehat{p}_{X})\\geq 10\\); observaréis que estas dos últimas condiciones son equivalentes a que tanto el número de éxitos como el número de fracasos en la muestra sean como mínimo 10. En este caso, se puede usar la fórmula de Laplace, que simplifica la de Wilson (aunque, en realidad, la precede en más de 100 años): un intervalo de confianza del parámetro \\(p\\) al nivel de confianza \\(q\\times 100\\%\\) viene dado aproximadamente por la fórmula \\[ \\widehat{p}_{X}\\pm z_{(1+q)/2}\\sqrt{\\frac{\\widehat{p}_{X} (1-\\widehat{p}_{X})}{n}} \\] Esta fórmula está implementada en la función binom.approx del paquete epitools, de uso similar al de las dos funciones anteriores. Ejemplo 4.5 En una muestra aleatoria de 500 familias con niños en edad escolar de una determinada ciudad se ha observado que 340 introducen fruta de forma diaria en la dieta de sus hijos. A partir de este dato, queremos encontrar un intervalo de confianza del 95% para la proporción real de familias de esta ciudad con niños en edad escolar que incorporan fruta fresca de forma diaria en la dieta de sus hijos. Tenemos una población Bernoulli donde los éxitos son las familias que aportan fruta de forma diaria a la dieta de sus hijos, y la fracción de estas familias en el total de la población es la proporción poblacional \\(p\\) para la que queremos calcular el intervalo de confianza. Como \\(n\\) es muy grande y los números de éxitos y fracasos también lo son, podemos emplear el método de Laplace. binom.approx(340,500) ## x n proportion lower upper conf.level ## 1 340 500 0.68 0.639 0.721 0.95 Por lo tanto, según la fórmula de Laplace, un intervalo de confianza al 95% para la proporción poblacional es (0.639,0.721). ¿Qué hubiéramos obtenido con los otros dos métodos? binom.wilson(340,500) ## x n proportion lower upper conf.level ## 1 340 500 0.68 0.638 0.719 0.95 binom.exact(340,500) ## x n proportion lower upper conf.level ## 1 340 500 0.68 0.637 0.721 0.95 Como podéis ver, los resultados son muy parecidos, con diferencias de unas pocas milésimas. 4.3 Intervalo de confianza para la varianza de una población normal Supongamos ahora que queremos estimar la varianza \\(\\sigma^2\\), o la desviación típica \\(\\sigma\\), de una población que sigue una distribución normal. Tomamos una muestra aleatoria simple de tamaño \\(n\\), y sea \\(\\widetilde{S}_{X}\\) su desviación típica muestral. En esta situación, un intervalo de confianza del \\(q\\times 100\\%\\) para \\(\\sigma^2\\) es \\[ \\left( \\frac{(n-1)\\widetilde{S}_{X}^2}{\\chi_{n-1,(1+q)/2}^2},\\ \\frac{(n-1)\\widetilde{S}_{X}^2}{\\chi_{n-1,(1-q)/2}^2}\\right), \\] donde \\(\\chi_{n-1,(1-q)/2}^2\\) y \\(\\chi_{n-1,(1+q)/2}^2\\) son, respectivamente, los cuantiles de orden \\((1-q)/2\\) y \\((1+q)/2\\) de una variable aleatoria que sigue una distribución \\(\\chi^2\\) con \\(n-1\\) grados de libertad. Si conocéis la varianza muestral \\(\\widetilde{S}_{X}^2\\), que denotaremos por varm, podéis calcular este intervalo con la fórmula c((n-1)*varm/qchisq((1+q)/2,n-1),(n-1)*varm/qchisq((1-q)/2,n-1)) Si, en cambio, disponéis de la muestra, podéis calcular este intervalo de confianza con la función varTest del paquete EnvStats. La sintaxis es similar a la usada con t.test: varTest(X,conf.level)$conf.int donde X es el vector que contiene la muestra y conf.level el nivel de confianza, que por defecto es igual a 0.95. Ejemplo 4.6 Un índice de calidad de un reactivo químico es el tiempo que tarda en actuar. Se supone que la distribución de este tiempo de actuación del reactivo es aproximadamente normal. Se realizaron 30 pruebas independientes, que forman una muestra aleatoria simple, en las que se midió el tiempo de actuación del reactivo. Los tiempos obtenidos fueron reactivo = c(12,13,13,14,14,14,15,15,16,17,17,18,18,19,19,25,25,26,27,30,33,34,35, 40,40,51,51,58,59,83) Queremos usar estos datos para calcular un intervalo de confianza del 95% para la desviación típica de este tiempo de actuación. El siguiente código calcula un intervalo de confianza al 95% para la varianza a partir del vector reactivo: library(EnvStats) varTest(reactivo)$conf.int ## LCL UCL ## 191 545 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Este intervalo de confianza es para la varianza. Como la desviación típica es la raíz cuadrada de la varianza, para obtener un intervalo de confianza al 95% para la desviación típica, tenemos que tomar la raíz cuadrada de este intervalo para la varianza: sqrt(varTest(reactivo)$conf.int) ## LCL UCL ## 13.8 23.3 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Por lo tanto un intervalo de confianza del 95% para la desviación típica poblacional es (13.83, 23.34). De nuevo, si os molesta, podéis eliminar el atributo conf.level igualándolo a NULL. 4.4 Bootstrap Cuando no tiene sentido usar un método paramétrico como los explicados en las secciones anteriores para calcular un intervalo de confianza porque no se satisfacen las condiciones teóricas que garantizan que el intervalo obtenido contiene el 95% de las veces el parámetro poblacional deseado, podemos recurrir a un método no paramétrico. El más utilizado es el bootstrap, que básicamente consiste en: Remuestrear la muestra: tomar muchas muestras aleatorias simples de la muestra de la que disponemos, cada una de ellas del mismo tamaño que la muestra original (pero simples, es decir, con reposición). Calcular el estimador sobre cada una de estas submuestras. Organizar los resultados en un vector. Usar este vector para calcular un intervalo de confianza. La manera más sencilla de llevar a cabo el cálculo final del intervalo de confianza es el llamado método de los percentiles, en el que se toman como extremos del intervalo de confianza del \\(q\\times 100\\%\\) los cuantiles de orden \\((1-q)/2\\) y \\((1+q)/2\\) del vector de estimadores, pero hay mucho otros métodos; encontraréis algunos en la correspondiente entrada de la Wikipedia. Ejemplo 4.7 Volvamos a la muestra de pesos del Ejemplo 4.1, pero supongamos ahora que la variable aleatoria poblacional de la que la hemos extraído no es normal (o que no queremos suponer que lo sea). Vamos a usar el método bootstrap de los percentiles para calcular un intervalo de confianza del 95% para el peso medio poblacional. Para ello, vamos a general 1000 muestras aleatorias simples de la muestra del mismo tamaño que la muestra, calcularemos la media de cada muestra, construiremos un vector con estas medias muestrales, y daremos como extremos del intervalo de confianza los cuantiles de orden 0.025 y 0.975 del vector así obtenido. set.seed(42) n=length(pesos) X=replicate(1000,mean(sample(pesos,n,replace=TRUE))) IC.boot=c(quantile(X,0.025),quantile(X,0.975)) round(IC.boot,1) ## 2.5% 97.5% ## 3000.0 3348.2 El intervalo obtenido en este caso es (3000, 3348.2); como se trata de un método basado en una simulación aleatoria, seguramente con otra semilla de aleatoriedad daría un intervalo diferente. Para comparar, recordad que el intervalo de confianza obtenido con la fórmula basada en la t de Student ha sido (2997.8, 3355). El paquete boot dispone de la función boot para llevar a cabo simulaciones bootstrap. Aplicando luego la función boot.ci al resultado de la función boot obtenemos diversos intervalos de confianza basados en el enfoque bootstrap. La sintaxis básica de la función boot es boot(X,estadístico,R) donde: X es el vector que forma la muestra de la que disponemos R es el número de muestras que queremos extraer de la muestra original El estadístico es la función que calcula el estadístico deseado de la submuestra, y tiene que tener dos parámetros: el primero representa la muestra original X y el segundo representa el vector de índices de una m.a.s. de X. Por ejemplo, si vamos a usar la función boot para efectuar una simulación bootstrap de medias muestrales, podemos tomar como estadístico la función: media.boot=function(X,índices){mean(X[índices])} Por otro lado, el nivel de confianza se especifica en la función boot.ci mediante el parámetro conf (no conf.level, como hasta ahora), cuyo valor por defecto es, eso sí, el de siempre: 0.95. A modo de ejemplo, vamos a usar las funciones del paquete boot para calcular un intervalo de confianza del 95% para la media de la variable aleatoria que ha producido el vector pesos. library(boot) set.seed(42) simulacion=boot(pesos,media.boot,1000) El resultado simulacion de esta última instrucción es una list que incluye, en su componente t, el vector de 1000 medias muestrales obtenido mediante la simulación; sus 10 primeros valores son: simulacion$t[1:10] ## [1] 3344.57 3107.43 3162.89 3038.86 3094.64 3202.71 3151.25 3020.39 ## [9] 3072.75 3284.75 Calculemos ahora el intervalo de confianza deseado: boot.ci(simulacion) ## Warning in boot.ci(simulacion): bootstrap variances needed for studentized ## intervals ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = simulacion) ## ## Intervals : ## Level Normal Basic ## 95% (3022, 3345 ) (3021, 3336 ) ## ## Level Percentile BCa ## 95% (3017, 3332 ) (3028, 3359 ) ## Calculations and Intervals on Original Scale Obtenemos cuatro intervalos de confianza para \\(\\mu\\), calculados con cuatro métodos a partir de la simulación realizada (y un aviso de que no ha podido calcular un quinto intervalo). El intervalo Percentile es el calculado con el método de los percentiles que hemos explicado antes.1 No vamos a entrar en detalle sobre los métodos que usa para calcular el resto de intervalos, en realidad todos tienen ventajas e inconvenientes. Ejemplo 4.8 ¿Realmente funciona el enfoque bootstrap? Vamos a retomar el experimento realizado en el Ejemplo 4.2, donde construimos una matriz M cuyas columnas son 200 muestras aleatorias simples de tamaño 50 de una población que sigue una distribución normal estándard. En dicho ejemplo calculamos para cada una de estas muestras el intervalo de confianza del 95% para la media poblacional usando la fórmula (4.1), que es la recomendada por la teoría en este caso. De los 200 intervalos calculados, 189 contuvieron la media poblacional, lo que representa un 94.5% de aciertos. Ahora vamos a calcular para cada una de estas muestras un intervalo de confianza del 95% por el método bootstrap de los percentiles y compararemos las tasas de aciertos. Aunque se puede comprobar fácilmente que no es el caso, para mayor seguridad vamos a volver a generar en las mismas condiciones las 200 muestras de la población, no sea que a lo largo de la lección hayamos modificado inadvertidamente el contenido de la matriz M (y así de paso fijamos la semilla de aleatoriedad). set.seed(42) Poblacion=rnorm(10^7) #La población mu=mean(Poblacion) # La media poblacional M=replicate(200, sample(Poblacion,50,replace=TRUE)) # Las muestras IC.b=function(X){boot.ci(boot(X,media.boot,1000))$percent[4:5]} ICs.bootstrap=apply(M,FUN=IC.b,MARGIN=2) Aciertos.bootstrap=length(which((mu&gt;=ICs.bootstrap[1,]) &amp; (mu&lt;=ICs.bootstrap[2,]))) Aciertos.bootstrap ## [1] 186 Con el bootstrap, hemos acertado en 186 ocasiones, lo que supone un 93% de aciertos. No está nada mal,2 para ser un método ad hoc en el que el intervalo de confianza se obtiene a base de remuestrear la muestra original. 4.5 Guía rápida t.test(X, conf.level=...)$conf.int calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la media poblacional usando la fórmula basada en la t de Student aplicada a la muestra X. binom.exact(x,n,conf.level=...) del paquete epitools, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la proporción poblacional aplicando el método de Clopper-Pearson a una muestra de tamaño n con x éxitos. binom.wilson(x,n,conf.level=...) del paquete epitools, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la proporción poblacional aplicando el método de Wilson a una muestra de tamaño n con x éxitos. binom.approx(x,n,conf.level=...) del paquete epitools, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la proporción poblacional aplicando la fórmula de Laplace a una muestra de tamaño n con x éxitos. varTest(X,conf.level=...)$conf.int del paquete EnvStats, calcula el intervalo de confianza del conf.level\\(\\times 100\\%\\) para la varianza poblacional usando la fórmula basada en la khi cuadrado aplicada a la muestra X. boot(X,E,R) del paquete boot, lleva a cabo una simulación bootstrap, tomando R submuestras del vector X y calculando sobre ellas el estadístico representado por la función E. boot.ci del paquete boot, aplicado al resultado de una función boot, calcula diversos intervalos de confianza a partir del resultado de la simulación efectuada con boot. El nivel de confianza se especifica con el parámetro conf. 4.6 Ejercicios Modelo de test (1) Tomad la muestra de todas las longitudes de pétalos de flores Iris setosa contenida en la tabla de datos iris y usadla para calcular un intervalo de confianza del 95% para el valor medio de las longitudes de pétalos de esta especie de flores usando la fórmula basada en la t de Student. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 2 cifras decimales (sin ceros innecesarios a la derecha). (2) Tenemos una población de media \\(\\mu\\) desconocida. Tomamos una muestra aleatoria simple de tamaño 80 y obtenemos una media muestral de 6.2 y una desviación típica muestral de 1.2. Usad estos datos y la fórmula del intervalo de confianza para la media basado en la t de Student para calcular un intervalo de confianza al 95% para \\(\\mu\\). Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 2 cifras decimales (sin ceros innecesarios a la derecha). (3) Tenemos una población Bernoulli de proporción poblacional \\(p\\) desconocida. Tomamos una muestra aleatoria simple de 80 individuos y obtenemos una proporción muestral de 35% de éxitos. Calculad un intervalo de confianza para \\(p\\) a un nivel de confianza del 95% usando el método de Wilson. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 3 cifras decimales (sin ceros innecesarios a la derecha). (4) Tomad la muestra de todas las longitudes de pétalos de flores Iris setosa contenida en la tabla de datos iris y usadla para calcular un intervalo de confianza del 95% para la varianza de las longitudes de pétalos de esta especie de flores. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 3 cifras decimales (sin ceros innecesarios a la derecha). Respuestas al test (1) 1.41,1.51 (2) 5.933,6.467 (3) 0.255,0.459 Fijaos en que no hay que entrar la proporción de éxitos en la función binom.wilson sino el número de éxitos. (4) 0.021,0.047 Pese a usar la misma semilla de aleatoriedad, el resultado ha sido distinto porque el procedimiento interno usado por la función boot para remuestrear el vector pesos ha sido diferente.↩ En el Ejemplo ?? de la próxima lección veremos que la diferencia entre los números de aciertos con los dos métodos ha sido tan pequeña que no permite concluir que el método de los percentiles sea menos eficaz que el recomendado por la teoría.↩ "],
["chap-contrastes.html", "Lección 5 Contrastes de hipótesis 5.1 Contrastes para medias 5.2 Contrastes para varianzas 5.3 Contrastes para proporciones 5.4 Cálculo de la potencia de un contraste 5.5 Guía rápida 5.6 Ejercicios", " Lección 5 Contrastes de hipótesis En esta lección explicamos algunas instrucciones de R que permiten llevar a cabo contrastes de hipótesis sobre parámetros poblacionales. Antes de empezar, repasemos el vocabulario básico relacionado con los contrastes de hipótesis: Hipótesis alternativa, \\(H_1\\): Aquella de la que buscamos evidencia en nuestro estudio. Hipótesis nula, \\(H_0\\): La hipótesis que estamos dispuestos a aceptar si no encontramos evidencia suficiente de la hipótesis alternativa. Suele plantearse en términos de “no hay diferencia”. Error de tipo I, o Falso positivo: Concluir que la hipótesis alternativa es verdadera cuando en realidad es falsa. Nivel de significación, \\(\\alpha\\): La probabilidad de cometer un error de tipo I. Nivel de confianza, \\(1-\\alpha\\): La probabilidad de no cometer un error de tipo I. Error de tipo II, o Falso negativo: Concluir que la hipótesis alternativa es falsa cuando en realidad es verdadera. Potencia, \\(1-\\beta\\): La probabilidad de no cometer un error de tipo II. Estadístico de contraste: El valor que se calcula a partir de la muestra obtenida en el estudio y que se usará para tomar la decisión en el contraste planteado. p-valor: La probabilidad, si la hipótesis nula es verdadera, de que el estadístico de contraste tome un valor tan o más extremo en el sentido de la hipótesis alternativa que el obtenido en el estudio Intervalo de confianza al nivel de confianza \\(1-\\alpha\\): Un intervalo en el que el parámetro poblacional que contrastamos tiene probabilidad \\(1-\\alpha\\) de pertenecer en el sentido de los intervalos de confianza de la Lección 4: es decir, porque se ha obtenido con un procedimiento que produce intervalos que, en el \\((1-\\alpha)\\times 100\\%\\) de las ocasiones que lo aplicamos a muestras aleatorias simples, contiene el parámetro poblacional. Está formado por los valores del parámetro poblacional que, si fueran los que contrastáramos en nuestro contraste, producirían un p-valor como mínimo \\(\\alpha\\). Los intervalos de confianza de los contrastes bilaterales coinciden con los definidos en la Lección 4. Regla de rechazo: Rechazamos la hipótesis nula en favor de la alternativa con un nivel de significación \\(\\alpha\\) dado cuando se da alguna de las dos condiciones siguientes (que son equivalentes, es decir, se dan las dos o ninguna): El p-valor és menor que el nivel de significación. El valor contrastado del parámetro poblacional pertenece al intervalo de confianza del nivel \\(1-\\alpha\\). Ejemplo 5.1 Tenemos una moneda y creemos que está trucada a favor de Cara, es decir, que al lanzarla al aire produce más caras que cruces. Para intentar decidir si esto es cierto o no, lanzamos la moneda al aire 20 veces y obtenemos 15 caras. Llamemos \\(p\\) a la probabilidad de obtener cara al lanzar al aire esta moneda. Entonces: La hipótesis alternativa, de la que buscamos evidencia, es que la moneda está trucada a favor de cara: \\(H_1: p&gt;0.5\\). La hipótesis nula, que aceptaremos por defecto si no encontramos evidencia de que la alternativa sea verdadera, es que la moneda no está trucada: \\(H_0: p=0.5\\). Cometeríamos un error de tipo I si la moneda fuera honrada y nosotros concluyéramos que está trucada. Cometeríamos un error de tipo II si la moneda estuviera trucada y nosotros concluyéramos que no lo está. El estadístico de contraste en este experimento va a ser simplemente el número de caras obtenidas. El p-valor es la probabilidad de obtener 15 o más caras al lanzar al aire 20 veces la moneda si fuera verdad que \\(p=0.5\\). Como en este caso el número de caras seguiría una distribución binomial \\(B(20,0.5)\\), podemos calcular fácilmente esta probabilidad: 1-pbinom(14,20,0.5) ## [1] 0.0206947 Por lo tanto, el p-valor es 0.021. El intervalo de confianza del 95% de este contraste está formado por los valores para la \\(p\\) para los que la probabilidad de obtener 15 o más caras al lanzar al aire 20 veces la moneda es mayor o igual que el 5%, y es [0.5444, 1]. Y bueno, tras todo este vocabulario, ¿cuál sería la conclusión? El p-valor obtenido significa que si la moneda no estuviera trucada, la probabilidad de obtener el número de caras que hemos obtenido o más es muy pequeña, lo que hace difícil de creer que la moneda no esté trucada. En particular, si trabajamos con un nivel de significación del 5%, como el p-valor es más pequeño que 0.05, rechazamos la hipótesis nula. Equivalentemente, como el intervalo de confianza del 95% para la \\(p\\) está completamente a la derecha del valor que contrastamos, 0.5, con este nivel de confianza hemos de concluir que \\(p&gt;0.5\\). En resumen, aceptando una probabilidad de error de tipo I (de decidir que una moneda honrada está trucada) del 5%, rechazamos la hipótesis nula y concluimos que la moneda está trucada a favor de cara. Figura 5.1: “Hipótesis Nula” (recuperado de http://imgs.xkcd.com/comics/null_hypothesis.png (CC-BY-NC 2.5)) 5.1 Contrastes para medias El test t El test t para contrastar una o dos medias basado en la t de Student está implementado en la función t.test. Este test usa diferentes estadísticos según que el contraste sea de una media o de dos; en este último caso, según se usen muestras emparejadas o independientes; y en este último caso, según las poblaciones tengan varianzas iguales o diferentes. Aunque este test solo es exacto (en el sentido de que da la conclusión con el nivel de significación requerido) cuando las poblaciones involucradas siguen distribuciones normales, el Teorema Central del Límite garantiza que también da resultados aproximadamente correctos cuando las muestras son grandes, aunque las poblaciones no sean normales, por lo que en esta situación también se recomienda su uso. Así pues, aunque en la práctica el test t se use como test “de talla única” para contrastar una o dos medias en cualquier situación, hay que tener claro que su resultado es fiable tan solo: cuando las variables poblacionales involucradas son (aproximadamente) normales, o cuando todas las muestras usadas son grandes. Al final de esta sección explicamos las funciones asociadas a algunos contrastes no paramétricos que pueden usarse cuando estas condiciones no se cumplen. La sintaxis básica de la función t.test es t.test(x, y, mu=..., alternative=..., conf.level=..., paired=..., var.equal=..., na.omit=...) donde: x es el vector de datos que forma la muestra que analizamos. y es un vector opcional; si lo entramos, R entiende que estamos realizando un contraste de dos medias, con hipótesis nula la igualdad de estas medias. Podemos sustituir los vectores x e y por una fórmula variable1~variable2 que indique que separamos la variable numérica variable1 en dos vectores definidos por los niveles de un factor variable2 de dos niveles (o de otra variable asimilable a un factor de dos niveles, como por ejemplo una variable numérica que solo tome dos valores diferentes). Con esta construcción, R tomará estos vectores en el orden natural de los niveles de variable2: x será el vector correspondiente al primer nivel e y el correspondiente al segundo. Hay que tener esto en cuenta a la hora de especificar la hipótesis alternativa si es unilateral. Si las dos variables de la fórmula son columnas de un dataframe, se puede usar el parámetro data=... para indicarlo. Solamente tenemos que especificar el parámetro mu si hemos entrado una sola muestra, y en este caso lo hemos de igualar al valor \\(\\mu_0\\) que queremos contrastar, de manera que la hipótesis nula será \\(H_0: \\mu=\\mu_0\\). El parámetro alternative puede tomar tres valores: &quot;two.sided&quot;, para contrastes bilaterales, y &quot;less&quot; y &quot;greater&quot;, para contrastes unilaterales. En esta función, y en todas las que explicamos en esta lección, su valor por defecto, que no hace falta especificar, es &quot;two.sided&quot;. El significado de estos valores depende del tipo de test que efectuemos: Si el test es de una sola muestra, &quot;two.sided&quot; representa la hipótesis alternativa \\(H_1: \\mu\\neq \\mu_0\\), &quot;less&quot; corresponde a \\(H_1: \\mu&lt; \\mu_0\\), y &quot;greater&quot; corresponde a \\(H_1: \\mu&gt; \\mu_0\\). Si hemos entrado dos muestras y llamamos \\(\\mu_x\\) y \\(\\mu_y\\) a las medias de las poblaciones de las que hemos extraído las muestras \\(x\\) e \\(y\\), respectivamente, entonces &quot;two.sided&quot; representa la hipótesis alternativa \\(H_1: \\mu_x \\neq \\mu_y\\); &quot;less&quot; indica que la hipótesis alternativa es \\(H_1: \\mu_x&lt; \\mu_y\\); y &quot;greater&quot;, que la hipótesis alternativa es \\(H_1: \\mu_x&gt; \\mu_y\\). El valor del parámetro conf.level es el nivel de confianza \\(1-\\alpha\\). En esta función, y en todas las que explicamos en esta lección, su valor por defecto, que no es necesario especificar, es 0.95, que corresponde a un nivel de confianza del 95%, es decir, a un nivel de significación \\(\\alpha=0.05\\). El parámetro paired solo lo tenemos que especificar si llevamos a cabo un contraste de dos medias. En este caso, con paired=TRUE indicamos que las muestras son emparejadas, y con paired=FALSE (que es su valor por defecto) que son independientes. Si se trata de muestras emparejadas, los vectores x e y tienen que tener la misma longitud, naturalmente. El parámetro var.equal solo lo tenemos que especificar si llevamos a cabo un contraste de dos medias usando muestras independientes, y en este caso sirve para indicar si queremos considerar las dos varianzas poblacionales iguales (igualándolo a TRUE) o diferentes (igualándolo a FALSE, que es su valor por defecto). El parámetro na.action sirve para especificar qué queremos hacer con los valores NA. Es un parámetro genérico que se puede usar en casi todas las funciones de estadística inferencial y análisis de datos. Sus valores más útiles son: na.omit, su valor por defecto, elimina las entradas NA de los vectores (o los pares que contengan algún NA, en el caso de muestras emparejadas). Por ahora, esta opción por defecto es la adecuada, por lo que no hace falta usar este parámetro, pero conviene saber que hay alternativas. na.fail hace que la ejecución pare si hay algún NA en los vectores. na.pass no hace nada con los NA y permite que las operaciones internas de la función sigan su curso y los manejen como les corresponda. La función t.test tiene otros parámetros que se pueden consultar en su Ayuda. Veamos varios ejemplos de uso de esta función. Ejemplo 5.2 Consideremos el siguiente vector de longitud 25: x=c(2.2,2.66,2.74,3.41,2.46,2.96,3.34,2.16,2.46,2.71,2.04,3.74,3.24,3.92,2.38,2.82,2.2, 2.42,2.82,2.84,4.22,3.64,1.77,3.44,1.53) Supongamos que esta muestra ha sido extraída de una población normal. Postulamos que el valor medio \\(\\mu\\) de la población no es 2. Para confirmarlo, vamos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu=2\\\\ H_{1}:\\mu\\neq 2 \\end{array}\\right. \\] con nivel de significación \\(\\alpha=0.05\\): t.test(x, mu=2, alternative=&quot;two.sided&quot;, conf.level=0.95) ## ## One Sample t-test ## ## data: x ## t = 5.912, df = 24, p-value = 4.23e-06 ## alternative hypothesis: true mean is not equal to 2 ## 95 percent confidence interval: ## 2.52384 3.08576 ## sample estimates: ## mean of x ## 2.8048 (Como los parámetros alternative=&quot;two.sided&quot; y conf.level=0.95 eran los que toma R por defecto, en realidad no hacía falta especificarlos.) Observad la información que obtenemos con esta instrucción: Información sobre la muestra \\(x\\): su media muestral (mean of x) \\(\\overline{x}\\), que vale 2.8048. La hipótesis alternativa (alternative hypothesis), en este caso true mean is not equal to 2: la media verdadera, o poblacional, \\(\\mu\\) es diferente de 2. El valor t que toma el estadístico de contraste, \\(T=\\frac{\\overline{X}-\\mu_0}{\\widetilde{S}_X/\\sqrt{n}}\\), sobre la muestra, en este caso 5.912, y los grados de libertad df (degrees of freedom) de su distribución t de Student cuando la hipótesis nula es verdadera, df =24. El p-valor (p-value) de nuestro test, en este caso p-value = 4.232e-06, es decir, \\(4.232\\times 10^{-6}\\). Un intervalo de confianza de nivel \\((1-\\alpha)\\times 100\\%\\) (en nuestro caso, 95 percent confidence interval) para la \\(\\mu\\): en nuestro ejemplo, [2.523844, 3.085756]. Lo único que no nos dice directamente es si tenemos que rechazar o no la hipótesis nula, pero esto lo deducimos del p-valor: como es más pequeño que el nivel de significación (de hecho, es muy pequeño), podemos rechazar la hipótesis nula, \\(\\mu=2\\), en favor de la alternativa, \\(\\mu \\neq 2\\). Es decir, hay evidencia estadísticamente significativa de que \\(\\mu \\neq 2\\). Otra manera de decidir si rechazamos o no la hipótesis nula es mirar si el valor que contrastamos pertenece al intervalo de confianza del contraste. Puesto que \\(2 \\notin [2.523844, 3.085756]\\), podemos rechazar la hipótesis nula en favor de la alternativa y concluir que \\(\\mu\\neq 2\\). Hagamos ahora el test cambiando la hipótesis alternativa por \\(H_{1}:\\mu&lt; 3\\), es decir, \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu=3\\\\ H_{1}:\\mu&lt; 3 \\end{array}\\right. \\] y tomando como nivel de significación \\(\\alpha=0.1\\): t.test(x, mu=3, alternative=&quot;less&quot;, conf.level=0.9) ## ## One Sample t-test ## ## data: x ## t = -1.434, df = 24, p-value = 0.0822 ## alternative hypothesis: true mean is less than 3 ## 90 percent confidence interval: ## -Inf 2.9842 ## sample estimates: ## mean of x ## 2.8048 En este caso, el p-valor es 0.082, por lo que podemos rechazar la hipótesis nula con un nivel de significación del 10% y concluir, con este nivel de significación (es decir, asumiendo esta probabilidad de equivocarnos), que \\(\\mu&lt;3\\); pero fijaos en que con un nivel de significación del 5% no podríamos rechazar la hipótesis nula. El intervalo de confianza del 90% es ahora \\((-\\infty,2.984]\\) (Inf representa \\(\\infty\\)). Que no contenga el 3 (aunque por muy poco) también indica que podemos rechazar la hipótesis nula \\(\\mu=3\\) en favor de la alternativa \\(\\mu&lt; 3\\) con este nivel de confianza. El p-valor y el intervalo de confianza se pueden obtener directamente, añadiendo a la instrucción t.test los sufijos $p.value o $conf.int, respectivamente. t.test(x, mu=2)$p.value ## [1] 4.23159e-06 t.test(x, mu=2)$conf.int ## [1] 2.52384 3.08576 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 t.test(x, mu=2)$conf.int[1] ## [1] 2.52384 t.test(x, mu=2)$conf.int[2] ## [1] 3.08576 Podéis consultar los sufijos necesarios para obtener las otras componentes del resultado en la Ayuda de la función. Ejemplo 5.3 Queremos contrastar si el valor medio del nivel de colesterol en una población es de 220 mg/dl o no, a un nivel de significación del 5%. Es decir, si llamamos \\(\\mu\\) a la media de la variable aleatoria “Nivel de colesterol de un individuo de esta población, en mg/dl”, queremos realizar el contraste bilateral \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu=220\\\\ H_{1}:\\mu \\neq 220 \\end{array}\\right. \\] Para ello, hemos tomado una muestra del nivel de colesterol en plasma de 9 individuos de la población. Los datos obtenidos, en mg/dl, son los siguientes: colesterol=c(203,229,215,220,223,233,208,228,209) Suponemos que el nivel de colesterol en plasma sigue una ley normal y que por lo tanto nos podemos fiar del resultado de un test t: t.test(colesterol, mu=220) ## ## One Sample t-test ## ## data: colesterol ## t = -0.3801, df = 8, p-value = 0.714 ## alternative hypothesis: true mean is not equal to 220 ## 95 percent confidence interval: ## 210.577 226.756 ## sample estimates: ## mean of x ## 218.667 El p-valor es 0.714, muy grande y en particular superior a 0.05, por lo tanto no podemos rechazar la hipótesis nula de que el valor medio sea 220 mg/dl. Además, el intervalo de confianza del 95% del contraste es [210.58, 226.76], y contiene holgadamente el valor 220. Más adelante en esta misma sección discutiremos qué podemos hacer si el nivel de colesterol en plasma no sigue una ley aproximadamente normal, en cuyo caso el resultado de este test t no sirve para nada. Ejemplo 5.4 Recordad el dataframe iris, que recoge datos de las flores de 50 ejemplares de cada una de tres especies de iris. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Queremos estudiar si la longitud media \\(\\mu_v\\) de los sépalos de las Iris virginica es mayor que la longitud media \\(\\mu_s\\) de los sépalos de las Iris setosa usando las muestras contenidas en esta tabla de datos. Para ello realizamos el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_s=\\mu_v\\\\ H_{1}:\\mu_s&lt; \\mu_v \\end{array}\\right. \\] En este caso, se trata de un contraste de dos muestras independientes. Como las muestras son grandes, podemos usar con garantías un test t. Como no sabemos nada de las varianzas de las longitudes de los sépalos de estas dos especies, y no nos supone apenas esfuerzo realizar los tests, llevaremos a cabo el contraste en los dos casos: varianzas iguales y varianzas diferentes.3 Más adelante, en el Ejemplo 5.11, explicamos cómo contrastar si estas dos varianzas son iguales o diferentes. S=iris[iris$Species==&quot;setosa&quot;,]$Sepal.Length V=iris[iris$Species==&quot;virginica&quot;,]$Sepal.Length El test suponiendo que las dos varianzas son iguales: t.test(S, V, alternative=&quot;less&quot;, var.equal=TRUE) ## ## Two Sample t-test ## ## data: S and V ## t = -15.39, df = 98, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -1.41126 ## sample estimates: ## mean of x mean of y ## 5.006 6.588 El test suponiendo que las dos varianzas son diferentes: t.test(S, V, alternative=&quot;less&quot;, var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: S and V ## t = -15.39, df = 76.52, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -1.4108 ## sample estimates: ## mean of x mean of y ## 5.006 6.588 En los dos casos el p-valor es prácticamente 0 y por lo tanto podemos rechazar la hipótesis nula en favor de la alternativa: tenemos evidencia estadísticamente muy significativa de que, en promedio, las flores de la especie setosa tienen sépalos más cortos que las de la especie virginica. El intervalo de confianza del 95% para la diferencia de medias \\(\\mu_s-\\mu_v\\) en este contraste es en ambos casos \\((-\\infty, -1.41]\\) y no contiene el 0, que sería el valor de esta diferencia si la hipótesis nula \\(\\mu_s=\\mu_v\\) fuera verdad. Ejemplo 5.5 En un experimento clásico de la primera década del siglo XX, Student quiso comparar el efecto de dos compuestos químicos, la hiosciamina y la hioscina, sobre el sueño: la hipótesis a contrastar era que la hioscina tiene un mayor efecto somnífero que la hiosciamina. Para ello, tomó 10 sujetos, midió su promedio de horas de sueño durante períodos de entre 3 y 9 días en condiciones normales, tomando antes de acostarse 0.6 mg de hiosciamina y tomando antes de acostarse 0.6 mg de hioscina, y apuntó para cada sujeto y cada compuesto la diferencia “promedio de horas de sueño tomando el compuesto menos promedio de horas de sueño en condiciones normales”. Las diferencias obtenidas fueron las siguientes:4 Sujeto Hiosciamina Hioscina 1 0.7 1.9 2 -1.6 0.8 3 -0.2 1.1 4 -1.2 0.1 5 -0.1 -0.1 6 3.4 4.4 7 3.7 5.5 8 0.8 1.6 9 0.0 4.6 10 2.0 3.4 Una manera de comparar el efecto en las horas de sueño de estos compuestos es comparando las medias de estas diferencias de promedios de horas de sueño: una diferencia mayor significa que el compuesto “ha añadido” más horas de sueño al promedio normal. Digamos \\(\\mu_1\\) a la media de las diferencias individuales del promedio de horas de sueño tomando hiosciamina menos el promedio en condiciones normales y \\(\\mu_2\\) a la media de las diferencias individuales del promedio de horas de sueño tomando hioscina menos el promedio en condiciones normales. Tomaremos como hipótesis nula \\(H_0: \\mu_1= \\mu_2\\) (ambos compuestos tienen el mismo efecto medio sobre las horas de sueño de los individuos) e hipótesis alternativa \\(H_1: \\mu_1&lt;\\mu_2\\) (la hioscina aumenta más las horas de sueño que la hiosciamina). Si podemos rechazar la hipótesis nula en favor de la alternativa, concluiremos que la hioscina tiene un mayor efecto somnífero que la hiosciamina. Observad que se trata de un contraste de dos muestras emparejadas, porque los datos refieren a los mismos 10 pacientes. Vamos a suponer que las diferencias medias en horas de sueño en ambos casos siguen leyes normales (Student así lo hizo) y que, por lo tanto, el resultado de un test t es fiable. Hiosciamina=c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0,2.0) Hioscina=c(1.9,0.8,1.1,0.1,-0.1,4.4,5.5,1.6,4.6,3.4) t.test(Hiosciamina, Hioscina, alternative=&quot;less&quot;, paired=TRUE) ## ## Paired t-test ## ## data: Hiosciamina and Hioscina ## t = -4.062, df = 9, p-value = 0.00142 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.866995 ## sample estimates: ## mean of the differences ## -1.58 El p-valor es 0.001, mucho menor que 0.05, y por lo tanto podemos rechazar la hipótesis nula con un nivel de significación del 5%. Observad también que el intervalo de confianza del 95% para la diferencia de medias \\(\\mu_1-\\mu_2\\) es \\((-\\infty,-0.867]\\) y está totalmente a la izquierda del 0. La conclusión es, pues, que efectivamente la hioscina tiene un mayor efecto somnífero que la hiosciamina y que con un 95% de confianza podemos afirmar que añade, de media, como mínimo 52 minutos (-0.867 horas) diarios más de sueño. Ejemplo 5.6 Veamos un ejemplo de aplicación de t.test a una fórmula. Queremos contrastar si es cierto que fumar durante el embarazo está asociado a un peso menor del recién nacido. Si llamamos \\(\\mu_n\\) y \\(\\mu_f\\) al peso medio de un recién nacido de madre no fumadora y fumadora, respectivamente, el contraste que queremos realizar es \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_n=\\mu_f\\\\ H_{1}:\\mu_n&gt; \\mu_f \\end{array}\\right. \\] Vamos a usar los datos incluidos en la tabla de datos birthwt incluida en el paquete MASS, que recoge algunos datos sobre una muestra de madres y sus hijos. library(MASS) str(birthwt) ## &#39;data.frame&#39;: 189 obs. of 10 variables: ## $ low : int 0 0 0 0 0 0 0 0 0 0 ... ## $ age : int 19 33 20 21 18 21 22 17 29 26 ... ## $ lwt : int 182 155 105 108 107 124 118 103 123 113 ... ## $ race : int 2 3 1 1 1 3 1 3 1 1 ... ## $ smoke: int 0 0 1 1 1 0 0 0 1 1 ... ## $ ptl : int 0 0 0 0 0 0 0 0 0 0 ... ## $ ht : int 0 0 0 0 0 0 0 0 0 0 ... ## $ ui : int 1 0 0 1 1 0 0 0 0 0 ... ## $ ftv : int 0 3 1 2 0 0 1 1 1 0 ... ## $ bwt : int 2523 2551 2557 2594 2600 2622 2637 2637 2663 2665 ... En la Ayuda de birthwt nos enteramos de que la variable smoke indica si la madre ha fumado durante el embarazo (1) o no (0), y que la variable bwt da el peso del recién nacido en gramos. Lo primero que haremos será mirar si las muestras de madres fumadoras y no fumadoras contenidas en esta tabla son lo suficientemente grandes como para que el resultado del test t sea fiable. table(birthwt$smoke) ## ## 0 1 ## 115 74 Vemos que sí, que ambas son suficientemente grandes. Para entrar en la instrucción t.test los vectores de pesos de hijos de fumadoras y no fumadoras, usaremos la fórmula bwt~smoke especificando que data=birthwt. Fijaos en que los valores de smoke son 0 y 1, y que R los considera ordenados en este orden (basta ver el resultado de la función table anterior). Por consiguiente, bwt~smoke representa, en este orden, el vector de pesos de recién nacidos de madres no fumadoras (smoke=0) y el vector de pesos de recién nacidos de madres fumadoras (smoke=1). Como la hipótesis alternativa es \\(\\mu_n&gt;\\mu_f\\), deberemos especificar en la función t.test que alternative=&quot;greater&quot;. Como en el Ejemplo 5.4, vamos a llevar a cabo el test t suponiendo que las varianzas son iguales y que son diferentes, y cruzaremos los dedos para que la conclusión sea la misma. Otra posibilidad es contrastar antes la igualdad de estas varianzas. t.test(bwt~smoke, data=birthwt, alternative=&quot;greater&quot;,paired=FALSE, var.equal=TRUE) ## ## Two Sample t-test ## ## data: bwt by smoke ## t = 2.653, df = 187, p-value = 0.00433 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 106.953 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3055.70 2771.92 t.test(bwt~smoke, data=birthwt, alternative=&quot;greater&quot;,paired=FALSE, var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: bwt by smoke ## t = 2.73, df = 170.1, p-value = 0.0035 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 111.855 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3055.70 2771.92 En ambos casos hemos obtenido un p-valor en un orden de magnitud inferior a 0.05, lo que nos permite concluir que, en efecto, los hijos de las madres no fumadoras pesan más al nacer que los de las fumadoras. En vez de especificar los vectores de pesos con bwt~smoke,data=birthwt, hubiéramos podido usar birthwt$bwt~birthwt$smoke. Por ejemplo: t.test(birthwt$bwt~birthwt$smoke, alternative=&quot;greater&quot;,paired=FALSE, var.equal=TRUE) ## ## Two Sample t-test ## ## data: birthwt$bwt by birthwt$smoke ## t = 2.653, df = 187, p-value = 0.00433 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 106.953 Inf ## sample estimates: ## mean in group 0 mean in group 1 ## 3055.70 2771.92 Tests no paramétricos Cuando comparamos dos medias, o una media con un valor, usando un test t sobre muestras pequeñas, suponemos que las variables poblacionales que han producido las muestras son normales. En la Lección 6 estudiaremos los contrastes que nos permiten aceptar o rechazar que una muestra provenga de una variable aleatoria con una distribución concreta, pero en estos momentos ya tendría que ser claro que nos podemos encontrar con conjuntos de datos para los cuales el supuesto de normalidad de la variable poblacional no esté justificado: por ejemplo, porque sean datos cuantitativos discretos o porque la variable sea claramente muy asimétrica. En las situaciones en las que no estamos seguros de que las variables poblacionales satisfagan aproximadamente las hipótesis de los teoremas que nos garantizan la fiabilidad de las conclusiones de un contraste, por ejemplo de un test t, una salida razonable es usar un test no paramétrico alternativo. En el caso de los contrastes de medias, los tests no paramétricos para comparar medias en realidad lo que comparan son las medianas. Los más populares son los siguientes: El test de signos, que permite contrastar si la mediana de una variable aleatoria cualquiera (incluso ordinal) es un valor dado \\(M_0\\) estudiando la distribución de los signos de las diferencias entre este valor y los de una muestra (si la mediana fuera \\(M_0\\), los números de diferencias positivas y negativas en muestras aleatorias seguirían distribuciones binomiales con \\(p=0.5\\)). En R está implementado en la función SIGN.test del paquete BSDA. Su sintaxis es similar a la de t.test para una muestra, cambiando el parámetro mu, que en t.test sirve para especificar el valor de la media que contrastamos, por md, que en SIGN.test sirve para especificar el valor de la mediana que contrastamos. Esta función también se puede aplicar a dos muestras emparejadas: en este caso, la hipótesis nula del contraste que realiza es que “la mediana de las diferencias de las dos variables es 0”. El test de Wilcoxon para comparar la media de una variable continua simétrica con un valor dado o las medias de dos variables continuas cuya diferencia sea simétrica por medio de muestras emparejadas. Más en general, se puede usar para comparar la mediana de una variable continua con un valor dado o para comparar la mediana de la diferencia de dos variables continuas (medidas sobre muestras emparejadas) con 0. Observad que cuando las variables en juego son simétricas, las medianas coinciden con las medias y el contraste de medianas es también un contraste de medias. En R está implementado en la función wilcox.test y su sintaxis es la misma que la de t.test para una muestra o para dos muestras emparejadas (en este último caso, hay que especificar paired=TRUE). El test de Mann-Whitney para comparar las medianas de dos variables aleatorias por medio de muestras independientes. En R también está implementado en la función wilcox.test y su sintaxis es la misma que la de t.test para dos muestras independientes (especificando paired=FALSE), salvo que aquí no hay que especificar si las varianzas son iguales o diferentes, puesto que esto no se usa en este test. Ejemplo 5.7 Si los niveles de colesterol no siguen una distribución normal, el test t realizado en el Ejemplo 5.3 no sirve para nada. Una posibilidad es entonces no contrastar si el nivel medio de colesterol es 220, sino si el nivel mediano es 220. Para ello vamos realizar un test de signos. Los parámetros alternative=&quot;two.sided&quot; y conf.level=0.95 son los que usa la función SIGN.test por defecto, así que no haría falta especificarlos; los incluimos para que los veáis. library(BSDA) SIGN.test(colesterol, md=220, alternative=&quot;two.sided&quot;, conf.level=0.95) ## ## One-sample Sign-Test ## ## data: colesterol ## s = 4, p-value = 1 ## alternative hypothesis: true median is not equal to 220 ## 95 percent confidence interval: ## 208.078 228.922 ## sample estimates: ## median of x ## 220 ## ## Achieved and Interpolated Confidence Intervals: ## ## Conf.Level L.E.pt U.E.pt ## Lower Achieved CI 0.8203 209.000 228.000 ## Interpolated CI 0.9500 208.078 228.922 ## Upper Achieved CI 0.9609 208.000 229.000 Observad que la salida de la función es muy similar a la de t.test (salvo por los últimos intervalos de confianza, que no vamos a explicar). El p-valor ha dado directamente 1 y el intervalo de confianza al 95% para la mediana ha dado [208.1, 228.9]: por lo tanto, no podemos rechazar que la mediana del nivel de colesterol en la población de la que hemos extraído la muestra sea 220. También podríamos usar el test de Wilcoxon para realizar este contraste de una mediana: wilcox.test(colesterol, mu=220, alternative=&quot;two.sided&quot;,conf.level=0.95) ## Warning in wilcox.test.default(colesterol, mu = 220, alternative = ## &quot;two.sided&quot;, : cannot compute exact p-value with zeroes ## ## Wilcoxon signed rank test with continuity correction ## ## data: colesterol ## V = 15, p-value = 0.726 ## alternative hypothesis: true location is not equal to 220 El p-valor es 0.726, la conclusión es la misma. El mensaje de advertencia nos avisa de que la muestra ha contenido valores iguales al valor de la mediana contrastado, por lo que el p-valor obtenido no es exacto. Solo os tenéis que preocupar de un mensaje como este si el p-valor fuera muy cercano al nivel de significación deseado, que no es el caso. Ejemplo 5.8 Si las diferencias en promedios de horas de sueño no siguen distribuciones normales, el test t realizado en el Ejemplo 5.5 no sirve para nada. En este caso, vamos a usar un test de Wilcoxon para muestras emparejadas. Este test en realidad contrastará la hipótesis nula de que si para cada individuo calculamos la diferencia entre el aumento promedio de horas de sueño cuando toma hiosciamina y el aumento promedio tomando hioscina, la mediana de la variable aleatoria que define estas diferencias es 0, y como hipótesis alternativa que esta mediana es menor que 0 (y que por lo tanto más de la mitad de las veces es negativa, es decir, que a más de la mitad de la población la hioscina le añade más tiempo promedio de sueño que la hiosciamina). Si las variables “aumento de horas de sueño” en juego son simétricas, estas medianas coinciden con las correspondientes medias y llevamos a cabo el contraste del Ejemplo 5.5. Si no son simétricas, igualmente estamos contrastando si la hioscina es más efectiva que la hiosciamina, solo que planteándolo de otra manera. wilcox.test(Hiosciamina, Hioscina, alternative=&quot;less&quot;, paired=TRUE) ## Warning in wilcox.test.default(Hiosciamina, Hioscina, alternative = ## &quot;less&quot;, : cannot compute exact p-value with ties ## Warning in wilcox.test.default(Hiosciamina, Hioscina, alternative = ## &quot;less&quot;, : cannot compute exact p-value with zeroes ## ## Wilcoxon signed rank test with continuity correction ## ## data: Hiosciamina and Hioscina ## V = 0, p-value = 0.00455 ## alternative hypothesis: true location shift is less than 0 En este caso R nos avisa de nuevo de que el p-valor no es exacto, pero esto no afecta a la conclusión dado que el p-valor es muy pequeño: rechazamos la hipótesis nula en favor de la alternativa y también concluimos con este test no paramétrico que la hioscina tiene un mayor efecto somnífero que la hiosciamina. Ejemplo 5.9 Nos preguntamos si los hijos de madres de 20 años tienen el mismo peso medio al nacer que los de madres de 30 años, o no. Vamos a responder esta pregunta con un contraste bilateral de estos pesos medios usando la muestra recogida en la tabla de datos birthwt del paquete MASS, que contiene la variable age con la edad de las madres. hijos.20=birthwt[birthwt$age==20,&quot;bwt&quot;] hijos.30=birthwt[birthwt$age==30,&quot;bwt&quot;] c(length(hijos.20),length(hijos.30)) ## [1] 18 7 Las muestras no son lo suficientemente grandes como para usar un test t si no estamos seguros de que las variables poblacionales sean normales. Como las muestras son independientes, vamos a usar un test de Mann-Whitney para comparar los pesos medianos: wilcox.test(hijos.20, hijos.30, alternative=&quot;two.sided&quot;,paired=FALSE) ## Warning in wilcox.test.default(hijos.20, hijos.30, alternative = ## &quot;two.sided&quot;, : cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: hijos.20 and hijos.30 ## W = 43.5, p-value = 0.25 ## alternative hypothesis: true location shift is not equal to 0 El p-valor es 0.25, por lo que no podemos rechazar que las medianas de los pesos al nacer de los hijos de madres de 20 años y de 30 sean iguales.5 5.2 Contrastes para varianzas El test \\(\\chi^2\\) para comparar la varianza \\(\\sigma^2\\) (o la desviación típica \\(\\sigma\\)) de una población normal con un valor dado \\(\\sigma_0^2\\) (o \\(\\sigma_0\\)) usa el estadístico \\[ \\frac{(n-1)\\widetilde{S}_X^2}{\\sigma_0^2} \\] que, si la hipótesis nula \\(\\sigma^2=\\sigma_0^2\\) es verdadera, sigue una distribución \\(\\chi^2_{n-1}\\), de ahí su nombre. Dicho test está convenientemente implementado en la función sigma.test del paquete TeachingDemos. Su sintaxis es la misma que la de la función t.test para una muestra, substituyendo el parámetro mu de t.test por el parámetro sigma (para especificar el valor de la desviación típica que contrastamos, \\(\\sigma_0\\)) o sigmasq (por “sigma al cuadrado”, para especificar el valor de la varianza que contrastamos, \\(\\sigma_0^2\\)). Como siempre, los valores por defecto de alternative y conf.level son &quot;two.sided&quot; y 0.95, respectivamente. La salida de la función es también similar a la de t.test. Veamos un ejemplo. Ejemplo 5.10 Se ha realizado un experimento para estudiar el tiempo \\(X\\) (en minutos) que tarda un lagarto del desierto en llegar a los 45o partiendo de su temperatura normal mientras está a la sombra. Los tiempos obtenidos (en minutos) en una muestra aleatoria de lagartos fueron los siguientes: TL45=c(10.1,12.5,12.2,10.2,12.8,12.1,11.2,11.4,10.7,14.9,13.9,13.3) Supongamos que estos tiempos siguen una ley normal. ¿Aporta este experimento evidencia de que la desviación típica \\(\\sigma\\) de \\(X\\) es inferior a 1.5 minutos? Para responder esta pregunta, hemos de realizar el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\sigma= 1.5 \\\\ H_{1}:\\sigma&lt; 1.5 \\end{array}\\right. \\] Para ello, usaremos la función sigma.test aplicada a esta muestra y a sigma=1.5: library(TeachingDemos) sigma.test(TL45, sigma=1.5, alternative=&quot;less&quot;) ## ## One sample Chi-squared test for variance ## ## data: TL45 ## X-squared = 10.69, df = 11, p-value = 0.53 ## alternative hypothesis: true variance is less than 2.25 ## 95 percent confidence interval: ## 0.00000 5.25686 ## sample estimates: ## var of TL45 ## 2.18629 El p-valor que obtenemos es 0.53, muy grande, por lo que no tenemos evidencia que nos permita concluir que \\(\\sigma&lt;1.5\\). El test \\(\\chi^2\\) no se usa mucho en la práctica. En parte, porque realmente es poco interesante ya que suele ser difícil conjeturar la desviación típica a contrastar, y en parte porque su validez depende fuertemente de la hipótesis de que la variable aleatoria poblacional sea normal. En cambio, el contraste de las desviaciones típicas de dos poblaciones sí que es muy utilizado. Por ejemplo, en un contraste de dos medias usando un test t sobre dos muestras independientes, nos puede interesar conocer a priori si las varianzas poblacionales son iguales o diferentes, en lugar de realizar el test bajo ambas suposiciones. Si no las conocemos, ¿cómo podemos saber cuál es el caso? Si las dos variables poblacionales son normales, podemos contrastar la igualdad de las varianzas con el test F, basado en el estadístico \\[ \\frac{\\widetilde{S}_{X_1}^2} {\\widetilde{S}_{X_2}^2} \\] que, si las dos poblaciones tienen la misma varianza, sigue una distribución F de Fisher-Snedecor. Por desgracia, este test es también muy sensible a la no normalidad de las poblaciones objeto de estudio: a la que una de ellas se aleja un poco de la normalidad, el test deja de dar resultados fiables.6 La función para efectuar este test es var.test y su sintaxis básica es la misma que la de t.test para dos muestras: var.test(x, y, alternative=..., conf.level=...) donde x e y son los dos vectores de datos, que se pueden especificar mediante una fórmula como en el caso de t.test, y el parámetro alternative puede tomar los tres mismos valores que en los tests anteriores: su valor por defecto es, como siempre, &quot;two.sided&quot;, que es el que nos permite contrastar si las varianzas son iguales o diferentes. Ejemplo 5.11 Suponiendo que las longitudes de los sépalos de las flores de las diferentes especies de iris siguen leyes normales, ¿hubiéramos podido considerar a priori iguales las varianzas de las dos muestras en el Ejemplo 5.4? Veamos: S=iris[iris$Species==&quot;setosa&quot;,]$Sepal.Length V=iris[iris$Species==&quot;virginica&quot;,]$Sepal.Length var.test(S,V) ## ## F test to compare two variances ## ## data: S and V ## F = 0.3073, num df = 49, denom df = 49, p-value = 6.37e-05 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.174378 0.541496 ## sample estimates: ## ratio of variances ## 0.307286 El p-valor es \\(6.4\\times 10^{-5}\\), muy pequeño. Por lo tanto, podemos rechazar la hipótesis nula de que las dos varianzas son iguales, en favor de la hipótesis alternativa de que las dos varianzas son diferentes. Así, pues, bastaba realizar solo el t.test con var.equal=FALSE. Puede ser conveniente remarcar aquí que el intervalo de confianza obtenido con var.test es para el cociente de varianzas poblacionales \\(\\sigma^2_x/\\sigma^2_y\\), no para su diferencia. Por lo tanto, para contrastar si las varianzas son iguales o diferentes, hay que mirar si el 1 pertenece o no al intervalo obtenido. En este ejemplo, el intervalo de confianza al 95% ha sido [0.174, 0.541] y no contiene el 1, lo que confirma la evidencia de que las varianzas son diferentes. Ejemplo 5.12 Queremos contrastar si los gatos adultos macho pesan más que los gatos adultos hembra. Para ello usaremos los datos recogidos en el dataframe cats del paquete MASS, que contiene información sobre el peso de una muestra de gatos adultos, separados por su sexo. str(cats) ## &#39;data.frame&#39;: 144 obs. of 3 variables: ## $ Sex: Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Bwt: num 2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ... ## $ Hwt: num 7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ... table(cats$Sex) ## ## F M ## 47 97 Consultando la Ayuda de cats nos enteramos de que la variable Bwt contiene el peso de cada gato en kg, y la variable Sex contiene el sexo de cada gato: F para hembra (female) y M para macho (male). Como vemos en la tabla de frecuencias, los números de ejemplares de cada sexo son diferentes y grandes. Así pues, si llamamos \\(\\mu_m\\) al peso medio de un gato macho adulto y \\(\\mu_h\\) al peso medio de un gato hembra adulto, el contraste que vamos a realizar es \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_m=\\mu_h\\\\ H_{1}:\\mu_m&gt;\\mu_h \\end{array}\\right. \\] y para ello antes vamos a contrastar si las varianzas de ambas poblaciones son iguales o diferentes, para luego poder aplicar la función t.test con el valor de var.equal adecuado. Vamos a suponer que los pesos en ambos sexos siguen leyes normales. Para que el contraste de las varianzas sea fiable es necesario que esta suposición sea cierta; para el de los pesos medios, no, ya que ambas muestras son grandes. El contraste de la igualdad de varianzas es el siguiente: var.test(Bwt~Sex, data=cats) ## ## F test to compare two variances ## ## data: Bwt by Sex ## F = 0.3435, num df = 46, denom df = 96, p-value = 0.000116 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.212628 0.580348 ## sample estimates: ## ratio of variances ## 0.343501 El p-valor es \\(1.2\\times 10^{-4}\\), y por lo tanto podemos rechazar la hipótesis nula de que las varianzas son iguales y concluir que son diferentes. Así que en el test t las consideraremos diferentes. Recordemos ahora que la hipótesis alternativa que queremos contrastar es \\(H_{1}:\\mu_m&gt;\\mu_h\\). En el factor cats$Sex, la F (hembra) va antes que la M (macho), y, por tanto, si entramos los vectores de pesos mediante Bwt~Sex,data=cats, el primer vector corresponderá a las gatas y el segundo a los gatos. Así pues, la hipótesis alternativa que tenemos que especificar es que la media del primer vector es inferior a la media del segundo vector: alternative=&quot;less&quot;. t.test(Bwt~Sex, data=cats, alternative=&quot;less&quot;,var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: Bwt by Sex ## t = -8.709, df = 136.8, p-value = 4.42e-15 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.437666 ## sample estimates: ## mean in group F mean in group M ## 2.35957 2.90000 Como el p-valor es prácticamente 0, podemos concluir que, efectivamente, de media, los gatos adultos pesan más que las gatas adultas. Hemos insistido en que el test F solo es válido si las dos poblaciones cuyas varianzas comparamos son normales. ¿Qué podemos hacer si dudamos de su normalidad? Usar un test no paramétrico que no presuponga esta hipótesis. Hay diversos tests no paramétricos para realizar contrastes bilaterales de dos varianzas. Aquí os recomendamos el test de Fligner-Killeen, implementado en la función fligner.test. Se aplica o bien a una list formada por las dos muestras, o bien a una fórmula que separe un vector numérico en dos muestras por medio de un factor de dos niveles. Ejemplo 5.13 Si queremos contrastar si las varianzas de las longitudes de los sépalos de las flores iris setosa y virginica son iguales o no sin presuponer que siguen leyes normales, podemos usar el test de Fligner-Killeen de la manera siguiente: fligner.test(list(S,V)) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: list(S, V) ## Fligner-Killeen:med chi-squared = 9.984, df = 1, p-value = 0.00158 El p-valor es 0.0016, por lo que podemos concluir que las varianzas son diferentes. Ejemplo 5.14 Si queremos contrastar si las varianzas de los pesos de los gatos y las gatas adultos son iguales o no sin presuponer que dichos pesos tienen distribuciones normales, podemos usar el test de Fligner-Killeen de la manera siguiente: fligner.test(Bwt~Sex, data=cats) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: Bwt by Sex ## Fligner-Killeen:med chi-squared = 16.91, df = 1, p-value = ## 3.92e-05 El p-valor es \\(4\\times 10^{-5}\\), por lo que podemos concluir que las varianzas son diferentes. 5.3 Contrastes para proporciones Cuando tenemos que efectuar un contraste sobre una probabilidad de éxito \\(p\\) de una variable Bernoulli, podemos emplear el test binomial exacto. Este test se basa en que, si la hipótesis nula \\(H_0: p=p_0\\) es verdadera, el número de éxitos en una muestra aleatoria simple de tamaño \\(n\\) de la variable sigue una ley binomial \\(B(n,p_0)\\). Este test está implementado en la función binom.test, cuya sintaxis es binom.test(x, n, p=..., alternative=..., conf.level=...) donde x y n son números naturales: el número de éxitos y el tamaño de la muestra. p es la probabilidad de éxito que queremos contrastar. El significado de alternative y conf.level, y sus posibles valores, son los usuales. Fijaos en particular que binom.test no se aplica directamente al vector de una muestra, sino a su número de éxitos y a su longitud. Si la muestra es un vector binario X, el número de éxitos será sum(X) y la longitud length(X). Puede ser útil saber que el intervalo de confianza para la \\(p\\) que da binom.test en un contraste bilateral es el de Clopper-Pearson. Ejemplo 5.15 Recordemos el Ejemplo 5.1, donde, en una serie de 20 lanzamientos de una moneda, había obtenido 15 caras. ¿Podemos sospechar que la moneda está trucada a favor de cara? Como comentamos en ese ejemplo, si llamamos \\(p\\) a la probabilidad de obtener cara con esta moneda, el contraste que queremos realizar es \\[ \\left\\{\\begin{array}{l} H_{0}:p=0.5\\\\ H_{1}:p&gt; 0.5 \\end{array}\\right. \\] Usaremos la función binom.test: binom.test(15,20, p=0.5, alternative=&quot;greater&quot;) ## ## Exact binomial test ## ## data: 15 and 20 ## number of successes = 15, number of trials = 20, p-value = 0.0207 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.544418 1.000000 ## sample estimates: ## probability of success ## 0.75 El p-valor del test es 0.0207 y el intervalo de confianza que nos da este test para la \\(p\\) es [0.5444, 1]. Ambos valores coinciden con los dados en el ejemplo original Cuando la muestra es grande, pongamos de 40 o más sujetos, podemos usar también el test aproximado, basado en la aproximación de la distribución de la proporción muestral por medio de una normal dada por el Teorema Central del Límite. En R está implementado en la función prop.test, que además también sirve para contrastar dos proporciones por medio de muestras independientes grandes. Su sintaxis es prop.test(x, n, p =..., alternative=..., conf.level=...) donde: x puede ser dos cosas: Un número natural: en este caso, R entiende que es el número de éxitos en una muestra. Un vector de dos números naturales: en este caso, R entiende que es un contraste de dos proporciones y que estos son los números de éxitos en las muestras. Cuando trabajamos con una sola muestra, n es su tamaño. Cuando estamos trabajando con dos muestras, n es el vector de dos entradas de sus tamaños. Cuando trabajamos con una sola muestra, p es la proporción poblacional que contrastamos. En el caso de un contraste de dos muestras, no hay que especificarlo. El significado de alternative y conf.level, y sus posibles valores, son los usuales. Veamos algunos ejemplos más. Ejemplo 5.16 Queremos contrastar si la proporción de estudiantes zurdos en la UIB es diferente del 10%, el porcentaje estimado de zurdos en España. Es decir, si llamamos \\(p\\) a la proporción de estudiantes zurdos en la UIB, queremos realizar el contraste \\[ \\left\\{ \\begin{array}{l} H_0:p=0.1\\\\ H_1:p\\neq 0.1 \\end{array} \\right. \\] Para ello, tomamos una muestra de 50 estudiantes de la UIB encuestados al azar y resulta que 3 son zurdos. Vamos a suponer que forman una muestra aleatoria simple. Como la muestra es grande (\\(n=50\\)) usaremos la función prop.test. prop.test(3, 50, p=0.1) ## ## 1-sample proportions test with continuity correction ## ## data: 3 out of 50, null probability 0.1 ## X-squared = 0.5, df = 1, p-value = 0.48 ## alternative hypothesis: true p is not equal to 0.1 ## 95 percent confidence interval: ## 0.0156246 0.1754187 ## sample estimates: ## p ## 0.06 El p-valor obtenido en el test es 0.48, muy superior a 0.05. Por lo tanto, no podemos rechazar que un 10% de los estudiantes de la UIB sean zurdos. El intervalo de confianza del 95% para \\(p\\) que hemos obtenido es [0.016, 0.175]. La conclusión usando el test binomial hubiera sido la misma: binom.test(3, 50, p=0.1) ## ## Exact binomial test ## ## data: 3 and 50 ## number of successes = 3, number of trials = 50, p-value = 0.48 ## alternative hypothesis: true probability of success is not equal to 0.1 ## 95 percent confidence interval: ## 0.0125486 0.1654819 ## sample estimates: ## probability of success ## 0.06 Comprovemos que el intervalo de confianza del 95% obtenido con binom.test es efectivamente el de Clopper-Pearson: epitools::binom.exact(3,50) ## x n proportion lower upper conf.level ## 1 3 50 0.06 0.0125486 0.165482 0.95 Ejemplo 5.17 Una empresa que fabrica trampas para cucarachas ha producido una nueva versión de su trampa más popular y afirma que la nueva trampa mata más cucarachas que la vieja. Hemos llevado a cabo un experimento para comprobarlo. Hemos situado dos trampas en dos habitaciones. En cada habitación hemos soltado 60 cucarachas. La versión vieja de la trampa ha matado 40 y la nueva, 48. ¿Es suficiente evidencia de que la nueva trampa es más efectiva que la vieja? Digamos \\(p_v\\) y \\(p_n\\) a las proporciones de cucarachas que matan la trampa vieja y la trampa nueva, respectivamente. La hipótesis nula será \\(H_0:p_v=p_n\\), y la hipótesis alternativa \\(H_1:p_v&lt;p_n\\). Los tamaños de las muestras nos permiten usar la función prop.test. prop.test(c(40,48),c(60,60),alternative=&quot;less&quot;) ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: c(40, 48) out of c(60, 60) ## X-squared = 2.088, df = 1, p-value = 0.0742 ## alternative hypothesis: less ## 95 percent confidence interval: ## -1.0000000 0.0146167 ## sample estimates: ## prop 1 prop 2 ## 0.666667 0.800000 El p-valor es 0.074, y el intervalo de confianza que nos da el test, [-1, 0.015], es para la diferencia de proporciones \\(p_v-p_n\\) y contiene el 0, aunque por poco. En resumen, a un nivel de significación de 0.05 no encontramos evidencia de que la trampa nueva sea mejor que la vieja, pero el resultado no es concluyente y convendría llevar a cabo otro experimento con más cucarachas para aumentar la potencia (cf. Ejemplo 5.23 en la próxima sección). La función prop.test solo sirve para contrastar dos proporciones cuando las dos muestras son independientes y grandes. Un test que se puede usar siempre para contrastar dos proporciones usando muestras independientes es el test exacto de Fisher, que usa una distribución hipergeométrica. Supongamos que evaluamos una característica dicotómica (es decir, que solo puede tomar dos valores y por tanto define distribuciones de Bernoulli) sobre dos poblaciones y tomamos dos muestras independientes, una de cada población. Resumimos los resultados en una tabla como la que sigue: \\[ \\begin{array}{r|c} &amp; \\quad\\mbox{Población}\\quad \\\\ \\mbox{Característica} &amp;\\quad 1 \\qquad 2\\quad \\\\\\hline \\mbox{Sí} &amp;\\quad a \\qquad b\\quad \\\\ \\mbox{No} &amp;\\quad c \\qquad d\\quad \\end{array} \\] Llamemos \\(p_{1}\\) a la proporción de individuos con la característica bajo estudio en la población 1 y \\(p_{2}\\) a su proporción en la población 2. Queremos contrastar la hipótesis nula \\(H_{0}:p_1=p_2\\) contra alguna hipótesis alternativa. Por ejemplo, en el experimento de las trampas para cucarachas, las poblaciones vendrían definidas por el tipo de trampa, y la característica que tendríamos en cuenta sería si la cucaracha ha muerto o no, lo que nos daría la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\qquad\\mbox{Trampas}\\quad \\\\ &amp;\\quad \\mbox{Viejas}\\qquad \\mbox{Nuevas}\\\\\\hline \\mbox{Muertas} &amp;\\qquad 40 \\qquad\\qquad 48\\quad \\\\ \\mbox{Vivas} &amp;\\qquad 20 \\qquad\\qquad 12\\quad \\end{array} \\] El test exacto de Fisher está implementado en la función fisher.test. Su sintaxis es fisher.test(x, alternative=..., conf.level=...) donde x es la matriz \\(\\left(\\begin{array}{cc} a &amp; b\\\\ c &amp; d\\end{array}\\right)\\), en la que los números de éxitos van en la primera fila y los de fracasos en la segunda, y las poblaciones se ordenan por columnas. El significado de alternative y conf.level, y sus posibles valores, son los usuales. Así, en el ejemplo de las trampas para cucarachas, entraríamos: Datos=rbind(c(40,48),c(20,12)) Datos ## [,1] [,2] ## [1,] 40 48 ## [2,] 20 12 fisher.test(Datos, alternative=&quot;less&quot;) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Datos ## p-value = 0.0739 ## alternative hypothesis: true odds ratio is less than 1 ## 95 percent confidence interval: ## 0.00000 1.08414 ## sample estimates: ## odds ratio ## 0.502909 y obtenemos de nuevo un p-valor cercano a 0.074. Hay que ir con cuidado con la interpretación del intervalo de confianza que da esta función: no es ni para la diferencia de las proporciones ni para su cociente, sino para su odds ratio: el cociente \\[ \\Big({\\frac{p_v}{1-p_v}}\\Big)\\Big/\\Big({\\frac{p_n}{1-p_n}}\\Big). \\] Recordad que si la probabilidad de un suceso \\(A\\) es \\(P(A)\\), sus odds son el cociente \\[ \\mbox{Odds}(A)=\\frac{P(A)}{1-P(A)} \\] que mide cuántas veces es más probable \\(A\\) que su contrario. Las odds son una función creciente de la probabilidad, y por lo tanto \\[ \\mbox{Odds}(A)&lt;\\mbox{Odds}(B)\\Longleftrightarrow P(A)&lt;P(B). \\] Esto permite comparar odds en vez de probabilidades, con la misma conclusión. Por ejemplo, en nuestro caso, como el intervalo de confianza para la odds ratio va de 0 a 1.084, en particular contiene el 1, por lo que no podemos rechazar que \\[ \\Big({\\frac{p_v}{1-p_v}}\\Big)\\Big/\\Big({\\frac{p_n}{1-p_n}}\\Big)=1, \\] es decir, no podemos rechazar que \\[ \\frac{p_v}{1-p_v}=\\frac{p_n}{1-p_n} \\] y esto es equivalente a \\(p_v=p_n\\). Si, por ejemplo, el intervalo de confianza hubiera ido de 0 a 0.8, entonces la conclusión a este nivel de confianza hubiera sido que \\[ \\Big({\\frac{p_v}{1-p_v}}\\Big)\\Big/\\Big({\\frac{p_n}{1-p_n}}\\Big)&lt;1 \\] es decir, que \\[ \\frac{p_v}{1-p_v}&lt;\\frac{p_n}{1-p_n} \\] y esto es equivalente a \\(p_v&lt;p_n\\). Ejemplo 5.18 Para determinar si el Síndrome de Muerte Súbita del Recién Nacido (SIDS, por sus siglas en inglés) tiene algún componente genético, se estudiaron parejas de gemelos y mellizos en las que se dio algún caso de SIDS. Sean \\(p_1\\) la proporción de casos con exactamente una muerte por SIDS entre las parejas de gemelos con algún caso de SIDS, y \\(p_2\\) la proporción de casos con exactamente una muerte por SIDS entre las parejas de mellizos con algún caso de SIDS. La hipótesis de trabajo es que si el SIDS tiene componente genético, será más probable que un gemelo de un muerto por SIDS también lo sufra que si solo es mellizo, y por lo tanto que en las parejas de gemelos ha de ser más raro que haya exactamente un caso de SIDS que en las parejas de mellizos. Es decir, que \\(p_1&lt;p_2\\). Así pues, queremos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_0:p_1=p_2\\\\ H_1:p_1&lt; p_2 \\end{array}\\right. \\] En un estudio de 1980 se obtuvieron los datos siguientes: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Tipo de gemelos}\\ \\\\ \\mbox{Casos de SIDS} &amp;\\ \\mbox{Gemelos}\\qquad \\mbox{Mellizos}\\\\\\hline \\mbox{Uno} &amp; \\quad\\ 23 \\qquad\\quad\\quad\\quad \\ 35\\quad \\\\ \\mbox{Dos} &amp; \\quad\\ 1 \\quad\\quad\\qquad\\quad \\ \\hphantom{3} 2 \\quad \\end{array} \\] Vamos a realizar el contraste. Observad que damos la tabla de manera que \\(p_1\\) es la proporción de parejas con un solo caso de SIDS entre las de la población 1 (gemelos), y \\(p_{2}\\) es la proporción de parejas con un solo caso de SIDS entre las de la población 2 (mellizos). Por tanto hemos de aplicar fisher.test a esta matriz y \\(p_1&lt;p_2\\) corresponderá a alternative=&quot;less&quot;. Datos=rbind(c(23,35),c(1,2)) Datos ## [,1] [,2] ## [1,] 23 35 ## [2,] 1 2 fisher.test(Datos, alternative=&quot;less&quot;) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Datos ## p-value = 0.784 ## alternative hypothesis: true odds ratio is less than 1 ## 95 percent confidence interval: ## 0.0000 39.7395 ## sample estimates: ## odds ratio ## 1.30859 El p-valor es 0.784, muy grande, por lo que no obtenemos evidencia de componente genético en el SIDS. Supongamos ahora que queremos comparar dos proporciones usando muestras emparejadas. Por ejemplo, supongamos que evaluamos dos características dicotómicas sobre una misma muestra de \\(n\\) sujetos. Resumimos los resultados obtenidos en la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Característica 1}\\ \\\\ \\mbox{Característica 2} &amp;\\ \\ \\, \\mbox{Sí}\\qquad \\mbox{No}\\\\\\hline \\mbox{Sí} &amp; \\quad\\ \\ a \\qquad \\ \\ \\, b\\quad \\\\ \\mbox{No} &amp; \\quad\\ \\ c \\qquad \\ \\ \\, d\\quad \\end{array} \\] donde \\(a+b+c+d=n\\). Esta tabla quiere decir, naturalmente, que \\(a\\) sujetos de la muestra tuvieron la característica 1 y la característica 2, que \\(b\\) sujetos de la muestra tuvieron la característica 2 y pero no tuvieron la característica 2, etc. Vamos a llamar \\(p_{1}\\) a la proporción poblacional de individuos con la característica 1, y \\(p_{2}\\) a la proporción poblacional de individuos con la característica 2. Queremos contrastar la hipótesis nula \\(H_{0}:p_1=p_2\\) contra alguna hipótesis alternativa. En este caso, no pueden usarse las funciones prop.test o fisher.test. Tenemos dos soluciones posibles. La primera nos permite realizar el contraste bilateral \\[ \\left\\{\\begin{array}{l} H_{0}:p_1=p_2\\\\ H_{1}:p_1\\neq p_2 \\end{array}\\right. \\] cuando \\(n\\) es grande y el número \\(b+c\\) de casos discordantes (en los que una característica da Sí y la otra da No) es razonablemente grande, pongamos \\(\\geq 20\\). En esta situación podemos usar el test de McNemar, que se lleva a cabo en R con la instrucción mcnemar.test. Su sintaxis básica es mcnemar.test(X) donde X es la matriz \\(\\left(\\begin{array}{cc} a &amp; b\\\\ c&amp; d \\end{array}\\right)\\) que corresponde a la tabla anterior. Ejemplo 5.19 Para comparar la efectividad de dos tratamientos del asma, se escogieron 200 pacientes con asma severo, y a cada uno se le trató durante un mes con el tratamiento A o el tratamiento B, decidiéndose cada tratamiento al azar; tras esta fase de tratamiento, se les dejó sin tratamiento durante un mes, y a continuación a cada uno se le trató durante un mes con el otro tratamiento (B si antes había recibido A, A si antes había recibido B). Se anotó si durante cada periodo de tratamiento cada enfermo visitó o no el servicio de urgencias por dificultades respiratorias. Los resultados del experimento se resumen en la tabla siguiente (“Sí” significa que sí que acudió a urgencias por dificultades respiratorias): \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Tratamiento A}\\ \\\\ \\mbox{Trat. B} &amp;\\quad \\ \\mbox{ Sí}\\qquad\\quad \\mbox{No}\\quad \\\\\\hline \\mbox{Sí} &amp; \\quad \\ 71 \\qquad\\quad 48\\quad \\\\ \\mbox{No} &amp; \\quad \\ 30 \\qquad\\quad 51\\quad \\end{array} \\] Queremos determinar si hay diferencia en la efectividad de los dos tratamientos. Datos=matrix(c(71,48,30,51),nrow=2,byrow=TRUE) Datos ## [,1] [,2] ## [1,] 71 48 ## [2,] 30 51 mcnemar.test(Datos) ## ## McNemar&#39;s Chi-squared test with continuity correction ## ## data: Datos ## McNemar&#39;s chi-squared = 3.705, df = 1, p-value = 0.0542 El p-valor del test es 0.054, ligeramente superior a 0.05, por lo tanto no permite concluir a un nivel de significación del 5% que haya evidencia de que la efectividad de los dos tratamientos sea diferente. Sería conveniente llevar a cabo un estudio más amplio. Otra posibilidad para realizar un contraste de dos proporciones usando muestras emparejadas, que no requiere de ninguna hipótesis sobre los tamaños de las muestras, es usar de manera adecuada la función binom.test. Para explicar este método, consideremos la tabla siguiente, donde ahora damos las probabilidades poblacionales de las cuatro combinaciones de resultados: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Característica 1}\\ \\\\ \\mbox{Característica 2} &amp;\\quad \\ \\!\\mbox{Sí}\\qquad\\quad\\, \\mbox{No}\\quad \\\\\\hline \\mbox{Sí} &amp; \\quad \\ p_{11} \\qquad\\quad p_{01}\\quad \\\\ \\mbox{No} &amp; \\quad \\ p_{10} \\qquad\\quad p_{00}\\quad \\end{array} \\] De esta manera \\(p_1=p_{11}+p_{10}\\) y \\(p_2=p_{11}+p_{01}\\). Entonces, \\(p_1=p_2\\) es equivalente a \\(p_{10}=p_{01}\\) y cualquier hipótesis alternativa se traduce en la misma desigualdad, pero para \\(p_{10}\\) y \\(p_{01}\\): \\(p_1\\neq p_2\\) es equivalente a \\(p_{10}\\neq p_{01}\\); \\(p_1&lt; p_2\\) es equivalente a \\(p_{10}&lt; p_{01}\\); y \\(p_1&gt; p_2\\) es equivalente a \\(p_{10}&gt; p_{01}\\). Por lo tanto podemos traducir el contraste sobre \\(p_1\\) y \\(p_2\\) al mismo contraste sobre \\(p_{10}\\) y \\(p_{01}\\). La gracia ahora está en que si la hipótesis nula \\(p_{10}=p_{01}\\) es cierta, entonces, en el total de casos discordantes, el número de sujetos en los que la característica 1 da Sí y la característica 2 da No sigue una ley binomial con \\(p=0.5\\). Por lo tanto, podemos efectuar el contraste usando un test binomial exacto tomando como muestra los casos discordantes de nuestra muestra, de tamaño \\(b+c\\), como éxitos los sujetos que han dado Sí en la característica 1 y No en la característica 2, de tamaño \\(c\\), con proporción a contrastar \\(p=0.5\\) y con hipótesis alternativa la que corresponda. La ventaja de este test es que su validez no requiere de ninguna hipótesis sobre los tamaños de las muestras. El inconveniente es que el intervalo de confianza que nos dará será para \\(p_{10}/(p_{10}+p_{01})\\), y no permite obtener un intervalo de confianza para la diferencia o el cociente de las probabilidades \\(p_1\\) y \\(p_2\\) de interés. Ejemplo 5.20 Usemos el test binomial para llevar a cabo el contraste bilateral del Ejemplo 5.19. Habíamos obtenido 30+48=78 casos discordantes, de los que 48 eran casos en los que el tratamiento A había dado Sí y el tratamiento B había dado No. binom.test(48, 78, p=0.5) ## ## Exact binomial test ## ## data: 48 and 78 ## number of successes = 48, number of trials = 78, p-value = 0.0535 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.498331 0.723398 ## sample estimates: ## probability of success ## 0.615385 Obtenemos de nuevo un p-valor en la zona de penumbra, ligeramente superior a 0.05. Ejemplo 5.21 Para determinar si un test casero de VIH basado en un frotis bucal da más positivos (que seguramente serán falsos positivos) que el test de VIH de referencia, basado en una analítica de sangre que detecta la presencia del virus, se tomó una muestra aleatoria de 241 individuos en situación de riesgo, y a todos se les realizaron ambos tests. Los resultados se resumen en la tabla siguiente: \\[ \\begin{array}{r|c} &amp; \\ \\mbox{Test estándar}\\ \\\\ \\mbox{Test casero} &amp;\\quad \\ \\mbox{ Positivo}\\qquad\\quad \\quad \\mbox{Negativo}\\quad \\\\\\hline \\mbox{Positivo} &amp; \\quad\\ \\ 72 \\qquad\\qquad\\qquad\\ 10 \\quad \\\\ \\mbox{Negativo} &amp; \\quad\\quad 2 \\qquad\\qquad\\quad\\quad \\ 157 \\quad \\end{array} \\] Si llamamos \\(p_{c}\\) a la probabilidad de que el test casero dé positivo y \\(p_{e}\\) a la probabilidad de que el test estándar dé positivo, queremos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:p_{e}=p_{c}\\\\ H_{1}:p_{e}&lt; p_{c} \\end{array}\\right. \\] Como el número de casos discordantes es pequeño (10+2=12) y el test es unilateral, usaremos el test binomial. binom.test(2, 12, alternative=&quot;less&quot;, p=0.5) ## ## Exact binomial test ## ## data: 2 and 12 ## number of successes = 2, number of trials = 12, p-value = 0.0193 ## alternative hypothesis: true probability of success is less than 0.5 ## 95 percent confidence interval: ## 0.000000 0.438105 ## sample estimates: ## probability of success ## 0.166667 Obtenemos evidencia significativa de que, efectivamente, el test casero da positivo en más ocasiones que el de referencia. 5.4 Cálculo de la potencia de un contraste Recordemos que la potencia de un contraste de hipótesis es la probabilidad de no cometer un error de tipo II, es decir, la probabilidad de aceptar la hipótesis alternativa si es verdadera. Usualmente, la probabilidad de cometer un error de tipo II se denota por \\(\\beta\\), y por lo tanto la potencia es \\(1-\\beta\\). La potencia de un contraste está relacionada con lo que se llama la magnitud del efecto. En un contraste, el efecto es la diferencia entre el valor estimado del parámetro a partir de la muestra usada y el valor que se da a dicho parámetro como hipótesis nula: por ejemplo, en el contraste de una media, la diferencia entre la media muestral \\(\\overline{x}\\) y el valor contrastado \\(\\mu_0\\); o, en el contraste de dos medias, la diferencia entre las dos medias muestrales. Se rechaza entonces la hipótesis nula si el efecto observado es tan grande que es muy improbable cuando la hipótesis nula es verdadera. Pero recordad que, en realidad, no se tiene en cuenta si el efecto observado ha sido grande o no por si mismo, solo si es estadísticamente significativo, es decir, si es improbable cuando la hipótesis nula es verdadera. Entonces, sin entrar en detalle, digamos que la magnitud del efecto es una medida estadística específica del tamaño del efecto observado respecto de su valor esperado. La fórmula para calcular la magnitud del efecto depende del contraste y del estadístico usado. Para cada tipo de test se han consensuado unos valores de la magnitud del efecto considerados como “pequeño”, “medio” y “grande”. Estos valores se obtienen con R con la función cohen.ES del paquete pwr aplicada al tipo de test (entrado en el parámetro test: por ejemplo, test=&quot;t&quot; para un test t usando t.test, o test=&quot;p&quot; para un test aproximado de proporciones usando prop.test) y el tipo de magnitud esperada (especificando en el parámetro size si esperamos que sea &quot;small&quot;, &quot;medium&quot; o &quot;large&quot;). A modo de ejemplo, la magnitud de efecto que se considera pequeña en un test t es: library(pwr) cohen.ES(test=&quot;t&quot;,size=&quot;small&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = t ## size = small ## effect.size = 0.2 y la magnitud de efecto que se considera media en un test aproximado de proporciones es: cohen.ES(test=&quot;p&quot;,size=&quot;medium&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = p ## size = medium ## effect.size = 0.5 Si se desea solo el valor de la magnitud del efecto, para poderlo entrar en otras funciones, se obtiene con el sufijo $effect.size: cohen.ES(test=&quot;p&quot;,size=&quot;medium&quot;)$effect.size ## [1] 0.5 Así pues, en un contraste de hipótesis intervienen cuatro cantidades fundamentales: el tamaño de la muestra, \\(n\\); el nivel de significación, \\(\\alpha\\); la potencia, \\(1-\\beta\\); y la magnitud del efecto. El tamaño de la muestra y el nivel de significación están bajo el control del investigador; sin embargo, la potencia del contraste y la magnitud del efecto afectan al contraste de forma más indirecta y su control escapa al investigador. Por ejemplo, si incrementamos el tamaño de la muestra, la potencia aumenta, pero el aumento preciso depende de la magnitud del efecto esperada. De hecho, las cuatro cantidades anteriores no son independientes, sino que, a partir de tres cualesquiera de ellas, se puede calcular la cuarta. Las funciones del paquete pwr permiten realizar estos cálculos para los contrastes de medias y proporciones. Las funciones de dicho paquete que por ahora nos interesan en este sentido son las siguientes: pwr.t.test, para utilizar en tests t de una media, de dos medias usando muestras emparejadas o de dos medias usando muestras independientes del mismo tamaño. pwr.t2n.test, para utilizar en tests t de dos medias usando muestras independientes de distinto tamaño. pwr.p.test, para utilizar en contrastes aproximados de una proporción. pwr.2p.test, para utilizar en contrastes aproximados de dos proporciones usando muestras independientes del mismo tamaño. pwr.2p2n.test, para utilizar en contrastes aproximados de dos proporciones usando muestras de distinto tamaño. Estas funciones tienen los parámetros básicos siguientes: n: el tamaño de la muestra (o de las muestras cuando son del mismo tamaño). n1 y n2: los tamaños de las dos muestras en pwr.2p2n.test y pwr.t2n.test. d (en las dos primeras) o h (en las tres últimas): la magnitud del efecto. sig.level: el nivel de significación. power: la potencia. type (en la primera): el tipo de muestras usado, siendo sus posibles valores &quot;one.sample&quot; (para contrastes de una muestra), &quot;two.sample&quot; (para contrastes de dos muestras independientes), o &quot;paired&quot; (para contrastes de dos muestras emparejadas). alternative: el tipo de hipótesis alternativa, con sus valores usuales. Si, en una cualquiera de estas funciones se especifican todos los parámetros n (o n1 y n2), d (o h), sig.level y power menos uno, la función da el valor del parámetro que falta. Veamos algunos ejemplos de uso. Ejemplo 5.22 Queremos calcular la potencia del contraste llevado a cabo en el Ejemplo 5.2. Se trataba de un contraste bilateral de una media usando un test t, por lo que utilizaremos la función pwr.t.test. Los parámetros que le entraremos son: n, el tamaño de la muestra; en este ejemplo, \\(n=25\\). d, la magnitud del efecto. Para tests t de una media e hipótesis nula \\(H_0: \\mu = \\mu_0\\), la magnitud del efecto se calcula con la fórmula \\[ d=\\frac{\\overline{x}-\\mu_0}{\\widetilde{s}_x}. \\] En nuestro ejemplo, \\(d=\\frac{|2.8048-2|}{0.68064}= 1.1824\\). sig.level, el nivel de significación; en este ejemplo, \\(\\alpha=0.05\\). Además como es un contraste bliateral de una media, especificaremos type=&quot;one.sample&quot; y alternative=&quot;two.sided&quot; (esto último en realidad no hace falta: como siempre, este es su valor por defecto). x=c(2.2,2.66,2.74,3.41,2.46,2.96,3.34,2.16,2.46,2.71,2.04, 3.74,3.24,3.92,2.38,2.82,2.2,2.42,2.82,2.84,4.22,3.64,1.77, 3.44,1.53) mag.ef=abs(mean(x)-2)/sd(x) #Magnitud del efecto pwr.t.test(n=25, d=mag.ef, sig.level=0.05, type=&quot;one.sample&quot;, alternative=&quot;two.sided&quot;) ## ## One-sample t test power calculation ## ## n = 25 ## d = 1.18241 ## sig.level = 0.05 ## power = 0.999893 ## alternative = two.sided Obtenemos que la potencia del test es prácticamente 1. Si estuviéramos diseñando el experimento y quisiéramos calcular el tamaño mínimo de una muestra para tener un nivel de significación del 5% y potencia del 99%, suponiendo a priori que la magnitud del efecto esperado va a ser grande (y que por lo tanto detectar que la hipótesis alternativa es verdadera va a ser fácil), primero calcularíamos cuánto vale una magnitud del efecto grande: cohen.ES(test=&quot;t&quot;,size=&quot;large&quot;)$effect.size ## [1] 0.8 y a continuación la usaríamos en la función pwr.t.test: pwr.t.test(d=0.8, sig.level=0.05,power=0.99, type=&quot;one.sample&quot;) ## ## One-sample t test power calculation ## ## n = 30.7143 ## d = 0.8 ## sig.level = 0.05 ## power = 0.99 ## alternative = two.sided Bastarían 31 observaciones para tener la potencia deseada. Si en cambio esperáramos una magnitud del efecto pequeña: pwr.t.test(d=cohen.ES(test=&quot;t&quot;,size=&quot;small&quot;)$effect.size, sig.level=0.05, power=0.99, type=&quot;one.sample&quot;) ## ## One-sample t test power calculation ## ## n = 461.238 ## d = 0.2 ## sig.level = 0.05 ## power = 0.99 ## alternative = two.sided En este caso necesitaríamos 462 observaciones. Ejemplo 5.23 Vamos a calcular la potencia del contraste \\[ \\left\\{ \\begin{array}{l} H_0:p_v=p_n\\\\ H_1:p_v&lt;p_n \\end{array} \\right. \\] del Ejemplo 5.17. En este caso, usamos la función pwr.2p.test, ya que usamos dos muestras del mismo tamaño, y le entramos los parámetros siguientes: n, el tamaño de las muestras; en este ejemplo, \\(n=60\\). h, la magnitud del efecto. Para calcularla,7 usamos la función ES.h del mismo paquete pwr y que se aplica a las proporciones muestrales de éxitos de las dos muestras: en este ejemplo, \\(\\widehat{p}_v=0.67\\) y \\(\\widehat{p}_n =0.8\\). mag.ef=ES.h(0.67,0.8) mag.ef ## [1] -0.296584 sig.level, el nivel de significación, 0.05. pwr.2p.test(h=mag.ef, n=60, sig.level=0.05,alternative=&quot;less&quot;) ## ## Difference of proportion power calculation for binomial distribution (arcsine transformation) ## ## h = -0.296584 ## n = 60 ## sig.level = 0.05 ## power = 0.491864 ## alternative = less ## ## NOTE: same sample sizes Hemos obtenido una potencia de, aproximadamente, un 49%. Si estuviéramos diseñando el experimento y quisiéramos calcular el tamaño de las muestras necesario para tener una potencia del 90% al nivel de significación del 5% y esperando una magnitud del efecto pequeña (porque esperamos una mejora con las nuevas trampas, pero solo pequeña), entraríamos: cohen.ES(test=&quot;p&quot;,size=&quot;small&quot;)$effect.size ## [1] 0.2 pwr.2p.test(h=-0.2, sig.level=0.05, power=0.9, alternative=&quot;less&quot;) ## ## Difference of proportion power calculation for binomial distribution (arcsine transformation) ## ## h = -0.2 ## n = 428.192 ## sig.level = 0.05 ## power = 0.9 ## alternative = less ## ## NOTE: same sample sizes Tendríamos que usar dos muestras de 429 cucarachas cada una. Observad que en pwr.2p.test hemos entrado en h la magnitud del efecto en negativo: esto es debido a que usamos alternative=&quot;less&quot; y por lo tanto esperamos que la primera proporción sea menor que la segunda. Ejemplo 5.24 En el contraste \\[ \\left\\{\\begin{array}{l} H_{0}:\\mu_n=\\mu_f\\\\ H_{1}:\\mu_n&gt; \\mu_f \\end{array}\\right. \\] del Ejemplo 5.6, ¿qué tamaño de la muestra de mujeres fumadoras tendríamos que tomar si usáramos una muestra de 100 no fumadoras, quisiéramos una potencia del 90% y un nivel de significación del 5% y esperáramos una magnitud del efecto media? Como es un contraste de dos medias independientes y los tamaños de las muestras pueden ser diferentes, usaremos la función pwr.t2n.test: pwr.t2n.test(n1=100, d=cohen.ES(test=&quot;t&quot;,size=&quot;medium&quot;)$effect.size, sig.level=0.05, power=0.9, alternative=&quot;greater&quot;) ## ## t test power calculation ## ## n1 = 100 ## n2 = 52.8251 ## d = 0.5 ## sig.level = 0.05 ## power = 0.9 ## alternative = greater Bastaría estudiar 53 madres fumadoras. 5.5 Guía rápida Excepto en las que decimos lo contrario, todas las funciones para realizar contrastes que damos a continuación admiten los parámetros alternative, que sirve para especificar el tipo de contraste (unilateral en un sentido u otro o bilateral), y conf.level, que sirve para indicar el nivel de confianza \\(1-\\alpha\\). Sus valores por defecto son contraste bilateral y nivel de confianza 0.95. t.test realiza tests t para contrastar una o dos medias (tanto usando muestras independientes como emparejadas). Aparte de alternative y conf.level, sus parámetros principales son: mu para especificar el valor de la media que queremos contrastar en un test de una media. paired para indicar si en un contraste de dos medias usamos muestras independientes o emparejadas. var.equal para indicar en un contraste de dos medias usando muestras independientes si las varianzas poblacionales son iguales o diferentes. SIGN.test del paquete BSDA, realiza un test de signos para contrastar una mediana. Dispone del parámetro md para entrar la mediana a contrastar. wilcox.test, para realizar tests de Wilcoxon y de Mann-Whitney para contrastar una o dos medianas (tanto usando muestras independientes como emparejadas). Sus parámetros son los mismos que los de t.test (salvo var.equal, que en estos tests no tiene sentido). sigma.test, para realizar tests \\(\\chi^2\\) para contrastar una varianza (o una desviación típica). Dispone de los parámetros sigma y sigmasq para indicar, respectivamente, la desviación típica o la varianza a contrastar. var.test, para realizar tests F para contrastar dos varianzas (o dos desviaciones típicas). fligner.test, para realizar tests no paramétricos de Fligner-Killeen para contrastar dos varianzas (o dos desviaciones típicas). No dispone de los parámetros alternative (solo sirve para contastes bilaterales) ni conf.level (no calcula intervalos de confianza). binom.test, para realizar tests binomiales exactos para contrastar una proporción. Dispone del parámetro p para indicar la proporción a contrastar. prop.test, para realizar tests aproximados para contrastar una proporción o dos proporciones de poblaciones usando muestras independientes. También dispone del parámetro p para indicar la proporción a contrastar en un contraste de una proporción. fisher.test, para realizar tests exactos de Fisher para contrastar dos proporciones usando muestras independientes. mcnemar.test, para realizar tests bilaterales de McNemar para contrastar dos proporciones usando muestras emparejadas. No dispone de los parámetros alternative ni conf.level. cohen.ES del paquete pwr, da los valores aceptados por convenio como “pequeño”, “mediano” y “grande” para diferentes tests. pwr.t.test del paquete pwr, relaciona el tamaño de la(s) muestra(s), el nivel de significación, la potencia y la magnitud del efecto (en el sentido de que si se entran tres de estos valores se obtiene el cuarto) en tests t de una media, de dos medias usando muestras emparejadas o de dos medias usando muestras independientes del mismo tamaño. Sus parámetros, son n: el tamaño de la muestra o de las muestras. sig.level: el nivel de significación. power: la potencia. d: la magnitud del efecto type: el tipo de muestras (una muestra, dos muestras emparejadas, dos muestras independientes). alternative: el tipo de hipótesis alternativa. pwr.t2n.testdel paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en tests t de dos medias usando muestras independientes de distinto tamaño. Sus parámetros son n1 y n2: los tamaños de las dos muestras. sig.level, power, d y alternative como en pwr.t.test. pwr.p.test del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de una proporción. Sus parámetros son n, sig.level, power y alternative como en pwr.t.test. h: la magnitud del efecto pwr.2p.test del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de dos proporciones usando muestras independientes del mismo tamaño. Sus parámetros son los mismos que los de pwr.p.test. pwr.2p2n.test, del paquete pwr, relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de dos proporciones usando muestras de distinto tamaño. Sus parámetros son n1 y n2: los tamaños de las dos muestras. sig.level, power y alternative como en pwr.p.test. 5.6 Ejercicios Modelo de test (1) Tenemos una muestra de una población normal formada por los números 2,5,3,5,6,6,7,2. Usando la función t.test, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste \\(H_0: \\mu=4\\) contra \\(H_1:\\mu \\neq 4\\) y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco. (2) Tenemos dos muestras de poblaciones normales, \\(x_1\\): 2,5,3,5,6,6,7,2 y \\(x_2\\): 3,2,5,4,2,2,4,5,1,6,2. Usando la función t.test, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste \\(H_0: \\mu_1=\\mu_2\\) contra \\(H_1:\\mu_1&gt;\\mu_2\\) suponiendo que las varianzas son diferentes y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.1. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco. (3) Tenemos dos muestras de poblaciones normales, \\(x_1\\): 2,5,3,5,6,6,7,2 y \\(x_2\\): 3,2,10,9,2,2,4,5,1,10,2. Usando la función var.test, calculad los extremos inferior y superior de un intervalo de confianza del 95% para \\(\\sigma_1^2/\\sigma_2^2\\) (redondeados a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI, sin acento, o NO) si en el contraste \\(H_0: \\sigma_1=\\sigma_2\\) contra \\(H_1:\\sigma_1 \\neq \\sigma_2\\) podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las tres respuestas en este orden, separadas por un único espacio en blanco. (4) Tenemos dos variables aleatorias de Bernoulli de proporciones poblacionales \\(p_1\\) y \\(p_2\\), respectivamente. En una muestra de 100 observaciones de la primera hemos obtenido 20 éxitos, y en una muestra de 150 observaciones de la segunda, hemos obtenido 40 éxitos. Usando la función prop.test, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste \\(H_0: p_1=p_2\\) contra \\(H_1:p_1&lt;p_2\\) y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco. Respuestas al test (1) 0.487 NO (2) 0.083 SI (3) 0.078 1.465 NO (4) 0.145 NO En realidad, se sabe que si las dos muestras provienen de poblaciones normales y son del mismo tamaño, el test t tiende a dar la misma conclusión tanto si se supone que las dos varianzas son iguales como si se supone que son diferentes (véase C. A. Markowski y E. P. Markowski, “Conditions for the Effectiveness of a Preliminary Test of Variance,” The American Statistician 44 (1990), pp. 322-326). Por lo tanto, si en este caso supiéramos que estas longitudes tienen distribuciones normales, bastaría realizar uno de los dos tests.↩ Las podéis encontrar en el dataframe sleep de la instalación básica de R, aunque no lo vamos a usar.↩ En realidad, la hipótesis nula de este test, la que no rechazamos, es que “Es igual de probable que un hijo de madre de 20 años pese más que un hijo de madre de 30 años que al revés”, pero no vamos a entrar en este nivel de precisión. Ambas hipótesis nulas quieren representar lo que realmente nos interesa confirmar o desmentir: “Los hijos de madres de 20 años pesan lo mismo que los de madres de 30 años”.↩ Véanse: E. S. Pearson, “The analysis of variance in cases of non-normal variation,” Biometrika 23 (1931), pp. 114-133; G. E. P. Box, “Non-normality and tests on variances,” Biometrika 40 (1953), pp. 318-335.↩ Por si a alguien le interesa, la fórmula para esta magnitud del efecto es \\[ h=2\\left(\\arcsin\\big(\\sqrt{\\widehat{p}_1}\\,\\big)-\\arcsin\\big(\\sqrt{\\widehat{p}_2}\\,\\big)\\right), \\] siendo \\(\\widehat{p}_1\\) y \\(\\widehat{p}_2\\) las proporciones muestrales de éxitos de las dos muestras.↩ "],
["chap-bondad.html", "Lección 6 Contrastes de bondad de ajuste 6.1 Pruebas gráficas: Q-Q-plots 6.2 El test \\(\\chi^2\\) de Pearson 6.3 El test \\(\\chi^2\\) para distribuciones continuas 6.4 El test de Kolgomorov-Smirnov 6.5 Tests de normalidad 6.6 Guía rápida 6.7 Ejercicios", " Lección 6 Contrastes de bondad de ajuste Una de las condiciones habituales que requerimos sobre una muestra, por ejemplo, al razonar sobre la distribución de sus estadísticos o al realizar contrastes de hipótesis, es que la población de la que la hemos extraído siga una determinada distribución. En la Lección ?? de la primera parte del curso comprobábamos gráficamente el ajuste de una muestra a una distribución normal mediante histogramas y dibujando las curvas de densidad muestral y de densidad de la normal. En esta lección presentamos algunas instrucciones que implementan tests de bondad de ajuste,8 técnicas cuantitativas que permiten contrastar si los datos de una muestra provienen o no de una determinada distribución de probabilidad. Los tests de bondad de ajuste tienen el mismo significado que los contrastes de parámetros estudiados en la Lección 5. Se contrasta una hipótesis nula \\(H_0\\): La muestra proviene de una población con distribución \\(X\\) contra la hipótesis alternativa \\(H_1\\): La muestra no proviene de una población con distribución \\(X\\) Si el resultado del test permite rechazar la hipótesis nula, es porque hemos obtenido evidencia significativa de que la muestra no proviene de una población con distribución \\(X\\), es decir, porque sería muy raro que siguiera esta distribución. Pero que el test no permita rechazar la hipótesis nula no nos da evidencia de que la muestra sí que provenga de una población con distribución \\(X\\): simplemente nos dice que no lo podemos rechazar. Naturalmente, a efectos prácticos, estamos dispuestos a aceptar “por defecto” la hipótesis nula si no la podemos rechazar, pero la mayoría de las veces, si no disponemos de algún argumento teórico que respalde esta conclusión, se trata de un abuso de lenguaje. Los pasos habituales para contrastar la bondad del ajuste de una muestra a una distribución son los siguientes: Fijar la familia de distribuciones teóricas a la que queremos ajustar los datos. Esta familia estará parametrizada por uno o varios parámetros. Recordemos los ejemplos más comunes: Si la familia es la Bernoulli, el parámetro es \\(p\\): la probabilidad poblacional de éxito. Si la familia es la Poisson, el parámetro es \\(\\lambda\\): la esperanza. Si la familia es la binomial, los parámetros son \\(n\\) y \\(p\\): el tamaño de las muestras y la probabilidad de éxito, respectivamente. Si la familia es la normal, los parámetros son \\(\\mu\\) y \\(\\sigma\\): la esperanza y la desviación típica, respectivamente. Si la familia es la \\(\\chi^2\\), el parámetro es el número de grados de libertad. Si la familia es la t de Student, el parámetro es de nuevo el número de grados de libertad. Otras familias de distribuciones tienen parámetros de localización (location), escala (scale) o forma (shape), por lo que no nos ha de extrañar si R nos pide que asignemos parámetros con estos nombres. Si el diseño del experimento no fija sus valores, tendremos que estimar a partir de la muestra los valores de los parámetros que mejor se ajusten a nuestros datos. Ya hemos tratado la estimación de parámetros en la Lección ??. Determinar qué tipo de contraste vamos a utilizar. En esta lección veremos dos tipos básicos de contrastes generales: El test \\(\\chi^2\\) de Pearson. Este test es válido tanto para variables discretas como para continuas, pero solo se puede aplicar a conjuntos grandes de datos (por fijar una cota concreta, de 30 o más elementos). Además, si el espacio muestral, es decir, el conjunto de resultados posibles, es infinito, es necesario agrupar estos resultados en un número finito de clases. El test de Kolgomorov-Smirnov. Este test solo es válido para variables continuas, y compara la función de distribución acumulada muestral con la teórica. No requiere que la muestra sea grande, pero en cambio, en principio, no admite que los datos de la muestra se puedan repetir.9 Por desgracia, las repeticiones suelen ser habituales si la muestra es grande y la precisión de los datos es baja o la variabilidad de la población muestreada es pequeña. Aparte, determinados tipos de distribuciones tienen sus contrastes de bondad de ajustes específicos. Este es el caso especialmente de la normal, para la que explicaremos algunos tests que permiten contrastar si una muestra proviene de alguna distribución normal. Realizar el contraste y redactar las conclusiones. Es conveniente apoyar los resultados del contraste con gráficos. En esta lección explicaremos los gráficos cuantil-cuantil, o Q-Q-plots, que sirven para visualizar el ajuste de unos datos a una distribución conocida y son una buena alternativa a los histogramas con curvas de densidad. 6.1 Pruebas gráficas: Q-Q-plots Para comparar la distribución de una muestra con una distribución poblacional teórica se pueden realizar diversas pruebas gráficas. En la Lección ?? de la primera parte del curso usábamos para ello histogramas con densidades estimadas y teóricas. En esta sección explicamos otro tipo de gráficos que pueden usarse con el mismo fin, los gráficos cuantil-cuantil, o, para abreviar, Q-Q-plots. Este tipo de gráficos compara los cuantiles observados de la muestra con los cuantiles teóricos de la distribución teórica. Figura 6.1: Q-Q-plot básico de la muestra del Ejemplo 6.1 contra una t de Student con 4 grados de libertad. La Figura 6.1 muestra un Q-Q-plot. Cada punto corresponde a un cuantil: grosso modo, hay un punto para cada \\(k/n\\)-cuantil, siendo \\(n\\) la longitud de la muestra y \\(k=1,\\ldots,n\\). Para cada uno de estos cuantiles, el punto correspondiente tiene abscisa el cuantil de la distribución teórica (en este caso, una t de Student con 4 grados de libertad) y ordenada el cuantil de la muestra. Por lo tanto, si el ajuste es bueno, para cada \\(k/n\\), el cuantil muestral y el cuantil teórico han de ser parecidos, de manera que los puntos del gráfico (les llamaremos Q-Q-puntos, para abreviar) han de estar cerca de la diagonal \\(y=x\\), que hemos añadido al gráfico. En general, se considera que un Q-Q-plot muestra un buen ajuste cuando no se observa una tendencia marcada de desviación respecto de la diagonal. Sin embargo, a menudo los Q-Q-plots son difíciles de interpretar, y es conveniente combinarlos con algún contraste de bondad de ajuste. Hay varias maneras de producir Q-Q-plots con R. Aquí solo explicaremos una: la función qqPlot del paquete car. Su sintaxis básica es qqPlot(x, distribution=..., parámetros, id=FALSE, ...) donde: x es el vector con la muestra. El parámetro distribution se ha de igualar al nombre de la familia de distribuciones entre comillas, y puede tomar como valor cualquier familia de distribuciones de la que R sepa calcular la densidad y los cuantiles: esto incluye las distribuciones que hemos estudiado hasta el momento: &quot;norm&quot;, &quot;binom&quot;, &quot;poisson&quot;, &quot;t&quot;, etc. A continuación, se tienen que entrar los parámetros de la distribución, igualando su nombre habitual (mean para la media, sd para la desviación típica, df para los grados de libertad, etc.) a su valor. En algunos casos, si no se especifican los parámetros, qqPlot toma sus valores por defecto: por ejemplo, si queremos realizar un Q-Q-plot contra una normal y no especificamos los valores de la media y la desviación típica de la distribución teórica, qqPlot los toma iguales a 0 y 1, respectivamente. Por defecto, el gráfico obtenido con la función qqPlot identifica los dos Q-Q-puntos con ordenadas más extremas. Para omitirlos, usad el parámetro id=FALSE. Otros parámetros a tener en cuenta: qqPlot añade por defecto una rejilla al gráfico, que podéis eliminar con grid=FALSE. qqPlot añade por defecto una línea recta que une los Q-Q-puntos correspondientes al primer y tercer cuartil: se la llama recta cuartil-cuartil. Un buen ajuste de los Q-Q-puntos a esta recta significa que la muestra se ajusta a la distribución teórica, pero posiblemente con parámetros diferentes a los especificados. Os recomendamos mantenerla, pero si queréis eliminarla por ejemplo para substituirla por la diagonal \\(y=x\\), podéis usar el parámetro line=&quot;none&quot;. qqPlot también añade dos curvas discontinuas que abrazan una “región de confianza del 95%” para el Q-Q-plot. Sin entrar en detalles, esta región contendría todos los Q-Q-puntos en un 95% de las ocasiones que tomáramos una muestra de la distribución teórica del mismo tamaño que la nuestra. Por lo tanto, si todos los Q-Q-puntos caen dentro de esta franja, no hay evidencia para rechazar que la muestra provenga de la distribución teórica. Esta franja de confianza es muy útil para interpretar el Q-Q-plot, pero la podéis eliminar con envelope=FALSE. Se pueden usar los parámetros usuales de plot para poner nombres a los ejes, título, modificar el estilo de los puntos, etc., y otros parámetros específicos para modificar el aspecto del gráfico. Por ejemplo, col.lines sirve para especificar el color de las líneas que añade. Consultad la Ayuda de la función. Ejemplo 6.1 Consideremos la siguiente muestra: muestra=c(0.27,0.81,-0.73,-0.96,1.33,0.91,-1.70,0.24,-0.19,0.29,1.41,0.13,-0.06, -0.85,-0.59,-3.62,-1.02,2.36,0.34,-0.31,0.81,-0.88,0.27,0.52,1.05,0.20,0.76,0.25, -1.43,3.71,-0.78,0.39,-1.01,1.53,-0.72,1.22,0.56,-1.17,-0.65,-0.33,-0.07,0.31, -0.74,0.36,-1.72,-1.21,-0.05,-1.17,0.28,1.30,0.89,1.45,0.13,-1.12,3.13,-1.21, -0.90,-0.31,-1.05,0.89,-1.06,0.21,-0.50,-0.36,-0.29,-0.19,-1.71,0.09,0.21,0.55, -1.42,0.19,-0.62,2.46,-0.17,-0.63,0.77,0.94,0.55,0.35,-4.47,1.71,0.07,-0.57, -1.43,-0.85,1.06,0.82,0.19,-1.08,0.30,-0.87,0.77,1.23,-0.04,0.66,-0.87,-0.86, -1.06,0.10) Queremos comprobar gráficamente si sigue una distribución t de Student de 4 grados de libertad. Vamos a usar la función qqPlot con sus parámetros por defecto: library(car) qqPlot(muestra, distribution=&quot;t&quot;, df=4, id=FALSE) Como todos los Q-Q-puntos están dentro de la región de confianza del 95%, podemos aceptar que la muestra proviene de una t de Student. El Q-Q-plot básico de la Figura 6.1 se ha obtenido con el código siguiente: qqPlot(muestra, distribution=&quot;t&quot;, df=4, envelope=FALSE, xlab=&quot;Cuantiles de t&quot;, ylab=&quot;Cuantiles de la muestra&quot;, line=&quot;none&quot;, pch=20, grid=FALSE, id=FALSE) abline(0,1, col=&quot;red&quot;, lwd=1.5) Veamos otro ejemplo. Ejemplo 6.2 Consideremos el data frame iris que contiene información sobre medidas relacionadas con las flores de una muestra de iris de tres especies. Vamos a producir un Q-Q-plot que ilustre si las longitudes de los sépalos de las plantas iris recogidas en esta tabla de datos siguen una distribución normal. A un Q-Q-plot que compara una muestra con una distribución normal se le suele llamar, para abreviar, un normal-plot. En primer lugar, estimamos los parámetros máximo verosímiles de la distribución normal que podría haber generado nuestra muestra: library(MASS) iris.sl=iris$Sepal.Length mu=fitdistr(iris.sl,&quot;normal&quot;)$estimate[1] sigma=fitdistr(iris.sl,&quot;normal&quot;)$estimate[2] round(c(mu,sigma),3) ## mean sd ## 5.843 0.825 y ahora generamos el Q-Q-plot usando estos parámetros qqPlot(iris.sl, distribution=&quot;norm&quot;, mean=mu, sd=sigma,xlab=&quot;Cuantiles de la normal&quot;,id=FALSE, ylab=&quot;Cuantiles de la muestra&quot;,main=&quot;Normal-plot de longitudes de sépalos de flores iris&quot;) Vemos cómo los primeros puntos salen de la región de confianza del 95%. Esto significa que los datos están más desplazados hacia la izquierda de la media que lo que se esperaría en una muestra aleatoria de una normal. El boxplot siguiente muestra este desplazamiento. boxplot(iris.sl, main=&quot;Boxplot de longitudes de sépalos de flores iris&quot;) Interpretamos el Q-Q-plot anterior como evidencia de que estas longitudes no siguen una distribución normal. Más adelante usaremos tests de normalidad específicos para contrastar la normalidad de estos datos. 6.2 El test \\(\\chi^2\\) de Pearson El test \\(\\chi^2\\) de Pearson contrasta si una muestra ha sido generada o no con una cierta distribución, cuantificando si sus valores aparecen con una frecuencia cercana a la que sería de esperar si la muestra siguiera esa distribución. Esto se lleva a cabo por medio del estadístico de contraste \\[ X^2=\\sum_{i=1}^k\\frac{(\\mbox{frec. observada}_i-\\mbox{frec. esperada}_i)^2}{\\mbox{frec. esperada}_i} \\] donde k es el número de clases e i es el índice de las clases, de manera que “frec. observadai” y “frec. esperadai” denotan, respectivamente, la frecuencia observada de la clase i-ésima y su frecuencia esperada bajo la distribución que contrastamos. Si se satisfacen una serie de condiciones, este estadístico sigue aproximadamente una ley \\(\\chi^2\\) con un número de grados de libertad igual al número de clases menos uno y menos el número de parámetros de la distribución teórica que hayamos estimado. Las condiciones que se han de satisfacer son: La muestra ha de ser grande, digamos que de tamaño como mínimo 30; Si los posibles valores son infinitos, hay que agruparlos en un número finito k de clases que cubran todos los posibles valores (recordad que en la Lección ?? de la primera parte del curso ya explicamos cómo agrupar variables aleatorias continuas con la función cut); Las frecuencias esperadas de las clases en las que hemos agrupado el espacio muestral han de ser todas, o al menos una gran mayoría, mayores o iguales que 5. La instrucción básica en R para realizar un test \\(\\chi^2\\) es chisq.test. Su sintaxis básica es chisq.test(x, p=..., rescale.p=..., simulate.p.value=...) donde: x es el vector (o la tabla, calculada con table) de frecuencias absolutas observadas de las clases en la muestra. p es el vector de probabilidades teóricas de las clases para la distribución que queremos contrastar. Si no lo especificamos, se entiende que la probabilidad es la misma para todas las clases. Obviamente, estas probabilidades se tienen que especificar en el mismo orden que las frecuencias de x y, como son las probabilidades de todos los resultados posibles, en principio tienen que sumar 1; esta condición se puede relajar con el siguiente parámetro. rescale.p es un parámetro lógico que, si se iguala a TRUE, indica que los valores de p no son probabilidades, sino solo proporcionales a las probabilidades; esto hace que R tome como probabilidades teóricas los valores de p partidos por su suma, para que sumen 1. Por defecto vale FALSE, es decir, se supone que el vector que se entra como p son probabilidades y por lo tanto debe sumar 1, y si esto no pasa se genera un mensaje de error indicándolo. Igualarlo a TRUE puede ser útil, porque nos permite especificar las probabilidades mediante las frecuencias esperadas o mediante porcentajes. Pero también es peligroso, porque si nos hemos equivocado y hemos entrado un vector en p que no corresponda a una probabilidad, R no nos avisará. simulate.p.value es un parámetro lógico que indica a la función si debe optar por una simulación para el cálculo del p-valor del contraste. Por defecto vale FALSE, en cuyo caso este p-valor no se simula sino que se calcula mediante la distribución \\(\\chi^2\\) correspondiente. Si se especifica como TRUE, R realiza una serie de replicaciones aleatorias de la situación teórica: por defecto, 2000, pero su número se puede especificar mediante el parámetro B. Es decir, genera un conjunto de vectores aleatorios de frecuencias con la distribución que queremos contrastar, cada uno de suma total la de x. A continuación, calcula la proporción de estas repeticiones en las que el estadístico de contraste es mayor o igual que el obtenido para x, y éste es el p-valor que da. Cuando no se satisfacen las condiciones para que \\(X^2\\) siga aproximadamente una distribución \\(\\chi^2\\), estimar el p-valor mediante simulaciones es una buena alternativa. Veamos un primer ejemplo sencillo. Ejemplo 6.3 Tenemos un dado, y queremos contrastar si está equilibrado o trucado. Lo hemos lanzado 40 veces y hemos obtenido los resultados siguientes: Resultados Frecuencias 1 8 2 4 3 6 4 3 5 7 6 12 Si el dado está equilibrado, la probabilidad de cada resultado es 1/6 y por lo tanto la frecuencia esperada de cada resultado es 40/6=6.667. Como la muestra tiene más de 30 elementos y las frecuencias esperadas son todas mayores que 5, podemos realizar de manera segura un test \\(\\chi^2\\). Por lo tanto, entraremos estas frecuencias en un vector y le aplicaremos la función chisq.test. Como contrastamos si todas las clases tienen la misma probabilidad, no hace falta especificar el valor del parámetro p. freqs=c(8,4,6,3,7,12) chisq.test(freqs) ## ## Chi-squared test for given probabilities ## ## data: freqs ## X-squared = 7.7, df = 5, p-value = 0.174 Observemos la estructura del resultado de un chisq.test. Nos da el valor del estadístico \\(X^2\\) (X-squared), el p-valor del contraste (p-value), y los grados de libertad de la distribución \\(\\chi^2\\) que ha usado para calcularlo (df). En este caso, el p-valor es 0.174, y por lo tanto no podemos rechazar que el dado esté equilibrado. Queremos remarcar que, como R no sabe si hemos estimado o no parámetros, el número de grados de libertad que usa chisq.test es simplemente el número de clases menos 1. Si no hemos estimado parámetros para calcular las probabilidades teóricas, ya va bien, pero si lo hemos hecho y por lo tanto el número de grados de libertad no es el adecuado, tendremos que calcular el p-valor correcto a partir del valor del estadístico. Veremos varios ejemplos más adelante. El resultado de un chisq.test es una list, de la que podemos extraer directamente la información que deseemos con los sufijos adecuados. En concreto, podemos obtener el valor del estadístico \\(X^2\\) con el sufijo $statistic, los grados de libertad con el sufijo $parameter y el p-valor con el sufijo $p.value. chisq.test(freqs)$statistic ## X-squared ## 7.7 chisq.test(freqs)$parameter ## df ## 5 chisq.test(freqs)$p.value ## [1] 0.173563 Imaginemos ahora que, en vez de lanzar el dado 40 veces, lo lanzamos 20 veces, y obtenemos los resultados siguientes: Resultados Frecuencias 1 4 2 2 3 3 4 2 5 3 6 6 ¿Hay evidencia de que el dado esté trucado? Ahora la muestra no es grande y las frecuencias esperadas son todas 20/6=3.333, menores que 5. Por tanto, el p-valor del test \\(\\chi^2\\) que se obtiene usando una distribución \\(\\chi^2_5\\) no tiene por qué tener ningún significado. En una situación como ésta es cuando conviene usar el parámetro simulate.p.value. Vamos a pedir a R que simule 5000 veces el experimento de lanzar 20 veces un dado equilibrado, y que calcule como p-valor la proporción de simulaciones en las que el estadístico \\(X^2\\) haya dado un valor mayor o igual que el que se obtiene con nuestra muestra. freqs2=c(4,2,3,2,3,6) chisq.test(freqs2, simulate.p.value=TRUE, B=5000) ## ## Chi-squared test for given probabilities with simulated p-value ## (based on 5000 replicates) ## ## data: freqs2 ## X-squared = 3.4, df = NA, p-value = 0.7 Resulta que en un 70% de las simulaciones el valor de \\(X^2\\) ha sido mayor o igual que el de nuestra muestra, 3.4. Por lo tanto, no hay evidencia de que el dado esté trucado. Como este p-valor se basa en simulaciones, en cada aplicación del test el p-valor puede dar resultados diferentes, pero en general la conclusión es robusta si se toma un número suficiente de simulaciones. chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.70126 chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.717656 chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.708858 Como vemos, los p-valores son todos similares. Por curiosidad, ¿qué p-valor da el test \\(\\chi^2\\) usando la distribución de \\(\\chi^2_5\\)? chisq.test(freqs2)$p.value ## Warning in chisq.test(freqs2): Chi-squared approximation may be incorrect ## [1] 0.63857 El p-valor no es muy diferente y la conclusión en este caso sería la misma, pero fijaos en el mensaje de advertencia: para la muestra dada, la aproximación de \\(X^2\\) mediante una \\(\\chi^2_5\\) no tiene por qué ser correcta. Ejemplo 6.4 Vamos a estudiar las frecuencias de los nucleótidos en una cadena de ADN, y contrastar si aparecen los cuatro con la misma probabilidad o no. En este caso, el espacio muestral son los cuatro nucleótidos: adenina (A), citosina (C), guanina (G) y timina (T). Identificaremos una cadena de ADN con un vector de letras a, c, g y t. Si llamamos \\(p_a\\), \\(p_c\\), \\(p_g\\) y \\(p_t\\) a las probabilidades de aparición de estas letras, el contraste que queremos realizar es \\[ \\left\\{\\begin{array}{l} H_0 : p_a=p_c=p_g=p_t=0.25\\\\ H_1: \\mbox{Algunos nucleótidos son más probables que otros} \\end{array} \\right. \\] Vamos a analizar cadenas de ADN “de verdad”, extraídas de la base de datos GenBank. Para ello, utilizaremos el paquete ape, que incorpora una función read.GenBank que permite leer secuencias de genes incluidas en esta base de datos y convertirlas en vectores de letras a, c, g y t. En concreto, si la aplicamos al número de acceso (accession number) de una secuencia (entrado entre comillas, ya que es una palabra) y usamos el parámetro as.character=TRUE, nos devuelve dicha secuencia como un vector de letras junto con otra información sobre la secuencia. En este ejemplo, nos vamos a interesar por los exones que codifican las tres partes de la mioglobina humana, que es una proteína relativamente pequeña constituida por una sola cadena polipeptídica de 153 aminoácidos. Sus números de acceso son M10090.1, M14602.1 y M14603.1. El código siguiente lee estos tres exones y los guarda en tres objetos. library(ape) myoglobin.exon1=read.GenBank(&quot;M10090.1&quot;, as.character=TRUE) myoglobin.exon2=read.GenBank(&quot;M14602.1&quot;, as.character=TRUE) myoglobin.exon3=read.GenBank(&quot;M14603.1&quot;, as.character=TRUE) Consultemos cómo son los objetos donde hemos guardado estas secuencias: str(myoglobin.exon1) ## List of 1 ## $ M10090.1: chr [1:2552] &quot;g&quot; &quot;t&quot; &quot;a&quot; &quot;c&quot; ... ## - attr(*, &quot;species&quot;)= chr &quot;Homo_sapiens&quot; Vemos que cada uno de ellos es en realidad una list formada por un solo objeto, el vector de bases, y un atributo. Vamos a extraer los vectores, para poder trabajar con ellos. La manera más sencilla es añadiendo a cada list el sufijo [[1]]: myoglobin.exon1[[1]][1:10] ## [1] &quot;g&quot; &quot;t&quot; &quot;a&quot; &quot;c&quot; &quot;t&quot; &quot;g&quot; &quot;t&quot; &quot;a&quot; &quot;t&quot; &quot;t&quot; Así pues, vamos a quedarnos solo con las cadenas de los tres exones. myoglobin.exon1.nuc=myoglobin.exon1[[1]] myoglobin.exon2.nuc=myoglobin.exon2[[1]] myoglobin.exon3.nuc=myoglobin.exon3[[1]] Nos preguntamos si en alguna de estas tres secuencias las cuatro bases aparecen de manera equiprobable. Para responder esta pregunta, calculamos las frecuencias de las letras en cada secuencia (con table) y aplicamos el test \\(\\chi^2\\) a los resultados. Puesto que miramos si todos los resultados aparecen con la misma probabilidad, no hace falta especificar el vector p de probabilidades. Empecemos con el primer exón. table(myoglobin.exon1.nuc) ## myoglobin.exon1.nuc ## a c g t ## 709 450 798 595 chisq.test(table(myoglobin.exon1.nuc)) ## ## Chi-squared test for given probabilities ## ## data: table(myoglobin.exon1.nuc) ## X-squared = 106.3, df = 3, p-value &lt;2e-16 El p-valor es prácticamente 0, podemos rechazar que las cuatro bases aparezcan con la misma probabilidad: las diferencias entre las frecuencias de los cuatro aminoácidos son lo suficientemente grandes como para hacer inverosímil que se hayan generado con la misma probabilidad. Veamos los otros dos exones. table(myoglobin.exon2.nuc) ## myoglobin.exon2.nuc ## a c g t ## 686 621 780 584 chisq.test(table(myoglobin.exon2.nuc)) ## ## Chi-squared test for given probabilities ## ## data: table(myoglobin.exon2.nuc) ## X-squared = 33.15, df = 3, p-value = 3e-07 table(myoglobin.exon3.nuc) ## myoglobin.exon3.nuc ## a c g t ## 323 382 441 320 chisq.test(table(myoglobin.exon3.nuc)) ## ## Chi-squared test for given probabilities ## ## data: table(myoglobin.exon3.nuc) ## X-squared = 26.86, df = 3, p-value = 6.29e-06 En los tres contrastes tenemos que rechazar la hipótesis nula: en ninguna de las tres cadenas es creíble que las bases aparezcan con la misma probabilidad. Ejemplo 6.5 Siguiendo con el ejemplo anterior, vamos a contrastar ahora si las bases del exón 1 siguen una distribución en la que A y G aparecen un 50% de veces más que C y T. Usaremos p=c(1.5,1,1.5,1) para especificar estas proporciones. chisq.test(table(myoglobin.exon1.nuc),p=c(1.5,1,1.5,1)) ## Error in chisq.test(table(myoglobin.exon1.nuc), p = c(1.5, 1, 1.5, 1)): probabilities must sum to 1. ¡Vaya! Nos habíamos olvidado de especificar rescale.p=TRUE, para poder entrar como p un vector proporcional a las probabilidades. chisq.test(table(myoglobin.exon1.nuc),p=c(1.5,1,1.5,1),rescale.p=TRUE)$p.value ## [1] 6.72027e-06 De nuevo, tenemos que rechazar la hipótesis nula. Ejemplo 6.6 Vamos a realizar otro experimento con los datos de los ejemplos anteriores. Construiremos una cadena formada por los tres exones, y vamos a comparar si la frecuencia de bases en cada exón es similar a la de la cadena total, que hará de distribución teórica. Definimos la cadena completa: myoglobin.todos.nuc=c(myoglobin.exon1.nuc,myoglobin.exon2.nuc,myoglobin.exon3.nuc) Calculemos la tabla de frecuencias relativas de las bases en la cadena completa probs.tot=prop.table(table(myoglobin.todos.nuc)) round(probs.tot,3) ## myoglobin.todos.nuc ## a c g t ## 0.257 0.217 0.302 0.224 Vamos a usar esta tabla como parámetro p de la función chisq.test: chisq.test(table(myoglobin.exon1.nuc),p=probs.tot)$p.value ## [1] 9.78458e-06 chisq.test(table(myoglobin.exon2.nuc),p=probs.tot)$p.value ## [1] 0.253398 chisq.test(table(myoglobin.exon3.nuc),p=probs.tot)$p.value ## [1] 0.000132497 Los exones 1 y 3 dan unos p-valores muy pequeños, y por lo tanto podemos rechazar que en estos exones las bases aparezcan con la misma probabilidad que en la cadena total. En cambio, el exón 2 da un \\(p\\)-valor de 0.253, por lo que para este exón no podemos rechazar la hipótesis nula y podemos concluir que las bases aparecen en él con las mismas frecuencias relativas que en la cadena total. Podemos ilustrar estas conclusiones gráficamente mediante un diagrama de barras conjunto. barplot(rbind(prop.table(table(myoglobin.todos.nuc)), prop.table(table(myoglobin.exon1.nuc)), prop.table(table(myoglobin.exon2.nuc)), prop.table(table(myoglobin.exon3.nuc))), beside=TRUE, legend=c(&quot;Totales&quot;,&quot;Exon1&quot;,&quot;Exon2&quot;,&quot;Exon3&quot;), args.legend=list(x=&quot;topleft&quot;,cex=0.8),col=2:5,ylim=c(0,0.45), main=&quot;Frecuencias relativas de las bases en los tres exones\\n de la mioglobina humana y en la cadena total&quot;) El gráfico que obtenemos confirma que las frecuencias relativas de las bases en el exón 2 se parecen bastante a las globales. Ejemplo 6.7 Ahora vamos a llevar a cabo el experimento siguiente. Queremos contrastar si la aparición de pares “cg” en la mioglobina humana es aleatoria, en el sentido de que se debe simplemente a las apariciones al azar de sus dos bases, o si por el contrario hay algún otro mecanismo que los produce. Para ello, tomaremos la secuencia completa de la mioglobina humana, que tenemos almacenada en myoglobin.todos.nuc, y repetiremos 100 veces el proceso siguiente: extraemos una muestra aleatoria simple de 20 posiciones de la secuencia y contamos cuántas de ellas contienen el par de bases “cg”. Luego contrastaremos si la muestra así obtenida proviene de una distribución binomial con la probabilidad de aparición de “cg” como si las dos bases fueran independientes. Fijamos la semilla de aleatoriedad para que se pueda reproducir el experimento.10 El código, que luego explicamos, y el resultado son los siguientes: cg.in.sample=function(x,S){#x un vector, S vector de índices length(which(x[S]==&quot;c&quot; &amp; x[S+1]==&quot;g&quot;)) } set.seed(1660) muestra=replicate(100, cg.in.sample(myoglobin.todos.nuc, sample(1:(length(myoglobin.todos.nuc)-1), 20,replace=TRUE))) muestra ## [1] 2 1 1 0 0 0 0 2 1 0 2 0 0 0 2 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 2 0 ## [36] 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 ## [71] 1 0 0 0 1 0 0 0 0 2 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 La función cg.in.sample que hemos definido toma un vector x y un vector de índices S y cuenta el número de índices s en los que x[s] es c y x[s+1] es g. Entonces, con el replicate, hemos repetido 100 veces el proceso de extraer una muestra aleatoria simple de 20 índices de myoglobin.todos.nuc (excluyendo el último, para que sean posiciones donde empieza un par de letras) y aplicar la función cg.in.sample a myoglobin.todos.nuc y a este vector de índices. Queremos determinar si esta muestra sigue una distribución binomial. En concreto, vamos a plantear tres casos de esta pregunta: Con probabilidad de aparición de la pareja “cg” 0.25·0.25=0.0625, que correspondería al hecho de que las dos bases aparecieran de manera equiprobable e independiente. Con probabilidad de aparición de la pareja “cg” el producto de las frecuencias relativas de las bases en la secuencia global, que correspondería al hecho de que las dos bases aparecieran de manera independiente, pero no equiprobable sino con sus probabilidades dentro de la secuencia de la mioglobina. Estimando el valor “real” de \\(p\\), lo que correspondería al hecho de que su probabilidad de aparición no tuviera nada que ver con las probabilidades individuales de sus dos bases. Empezamos con el primer caso. Calculemos las frecuencias con las que aparecen los diferentes resultados en la muestra, y las frecuencias esperadas con las que deberían aparecer si siguieran una distribución B(20,0.0625). table(muestra) ## muestra ## 0 1 2 ## 68 26 6 round(dbinom(0:20,20,0.0625)*100,2) #Frecuencias teóricas ## [1] 27.51 36.67 23.23 9.29 2.63 0.56 0.09 0.01 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Las frecuencias teóricas a partir de 4 son inferiores a 5 (recordad que la primera frecuencia corresponde al 0), y además su suma no llega a 5. Por lo tanto, vamos a agrupar en una sola clase los resultados mayores o iguales que 3. La nueva tabla de frecuencias esperadas es: round(c(dbinom(0:2,20,0.0625),1-pbinom(2,20,0.0625))*100,2) ## [1] 27.51 36.67 23.23 12.59 Ahora tenemos dos opciones: o bien tomamos como resultados posibles “0”, “1”, “2” y “3 o más”, en cuyo caso contaríamos que hemos observado 0 veces este último resultado en nuestra muestra, o bien tomamos como resultados posibles “0”, “1”, y “2 o más”, que se corresponde con los valores observados. Como norma general, es recomendable usar el mayor número de clases posible. Por consiguiente, vamos a optar por la primera estrategia: 4 clases y no 3. freq.obs=c(table(muestra),0) prob.teor=c(dbinom(0:2,20,0.0625),1-pbinom(2,20,0.0625)) chisq.test(freq.obs,p=prob.teor)$p.value ## [1] 5.62801e-19 El p-valor es prácticamente 0, por lo que podemos concluir que la muestra no sigue una distribución B(20,0.0625): las apariciones de los pares “cg” no se explican por la aparición de sus dos bases de manera independiente y equiprobable. ¿Hubiera variado la conclusión si hubiéramos optado por solo considerar tres clases? freq.obs=table(muestra) prob.teor=c(dbinom(0:1,20,0.0625),1-pbinom(1,20,0.0625)) chisq.test(freq.obs,p=prob.teor)$p.value ## [1] 9.75924e-20 El p-valor es prácticamente el mismo, la conclusión es la misma. Pasemos al segundo caso de nuestro problema. Vamos a calcular la frecuencia relativa de “c” y “g” en la secuencia completa de la mioglobina humana y tomaremos como probabilidad \\(p\\) el producto de ambas frecuencias relativas. prop.table(table(myoglobin.todos.nuc)) ## myoglobin.todos.nuc ## a c g t ## 0.256840 0.217222 0.301839 0.224099 p=prod(prop.table(table(myoglobin.todos.nuc))[2:3]) p ## [1] 0.0655661 Calculemos ahora las frecuencias esperadas tomando esta \\(p\\): round(dbinom(0:20,20,p)*100,2) ## [1] 25.76 36.15 24.10 10.15 3.03 0.68 0.12 0.02 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Vamos a tener que agrupar de nuevo en una sola clase los resultados mayores o iguales que 3. freq.obs=c(table(muestra),0) prob.teor=c(dbinom(0:2,20,p),1-pbinom(2,20,p)) chisq.test(freq.obs,p=prob.teor)$p.value ## [1] 1.81625e-21 Obtenemos de nuevo un valor prácticamente 0: las apariciones de los pares “cg” no se explican por la aparición de sus dos bases de manera independiente. Finalmente, vamos a estimar el parámetro \\(p\\). La binomial es otra de las distribuciones no cubiertas por fitdistr, por lo que tendremos que apelar a lo que sabemos de teoría para hacerlo. Como el valor esperado de una variable aleatoria \\(X\\sim B(n,p)\\) es \\(np\\), estimaremos \\(p\\) mediante \\(\\overline{X}/n\\). De hecho, éste es el estimador máximo verosímil de \\(p\\) cuando \\(n\\) es conocida. p.estim=mean(muestra)/20 p.estim ## [1] 0.019 Repetimos el proceso: calculemos las frecuencias teóricas round(dbinom(0:20,20,p.estim)*100,2) ## [1] 68.14 26.39 4.86 0.56 0.05 0.00 0.00 0.00 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 En este caso, hemos de agrupar los resultados en “0”, “1” y “2 o más”, para que las frecuencias teóricas sean mayores que 5. Coincide con los diferentes valores observados en la muestra. freq.obs=table(muestra) prob.teor=c(dbinom(0:1,20,p.estim),1-pbinom(1,20,p.estim)) chisq.test(freq.obs,p=prob.teor) ## ## Chi-squared test for given probabilities ## ## data: freq.obs ## X-squared = 0.05747, df = 2, p-value = 0.972 ¡Cuidado! Este p-valor no es el correcto. Hemos estimado un parámetro, pero R no lo sabe. Por lo tanto tenemos que bajar en 1 los grados de libertad y calcular el p-valor a mano, mediante \\[ P(\\chi_1^2\\geq X^2)=1-P(\\chi_1^2\\leq 0.057) \\] 1-pchisq(chisq.test(freq.obs,p=prob.teor)$statistic,1) ## X-squared ## 0.810537 El p-valor es grande. Por lo tanto, no podemos rechazar la hipótesis nula de que las apariciones de “cg” en muestras aleatorias de 20 posiciones sigan una ley binomial de parámetro \\(p=0.019\\). Que es, por otro lado, lo que debería pasar si las “cg” estuvieran repartidas de manera aleatoria en la secuencia original, por lo que no podemos rechazar esto último. La conclusión es, por lo tanto, que aceptamos que las “cg” aparecen distribuidas de manera aleatoria en la secuencia de la mioglobina humana, pero las “c” y las “g” que las forman no aparecen de manera independiente. 6.3 El test \\(\\chi^2\\) para distribuciones continuas El procedimiento de contraste de bondad de ajuste mediante el test \\(\\chi^2\\) para variables continuas tiene la particularidad de que es necesario un paso preliminar que consiste en definir los intervalos de clase para los que realizaremos el conteo de las frecuencias observadas. El proceso es similar al que estudiamos en la Lección ?? de la primera parte del curso para dibujar histogramas. Por lo tanto, necesitaremos definir unos intervalos de clase para el conteo de frecuencias absolutas observadas, y con las funciones cut y table obtendremos las frecuencias observadas de estas clases en la muestra de la variable continua. Para obtener los intervalos podemos seguir dos estrategias razonables: reutilizar los generados por la función hist, o dividir el rango de la variable en un número prefijado \\(k\\) de intervalos de amplitud fija. Vamos a ver en detalle un ejemplo de cada tipo. Ejemplo 6.8 Vamos a contrastar si las longitudes de los sépalos de las plantas iris recogidas en la tabla de datos iris siguen una distribución normal. Recordaréis que el Q-Q-plot de estas longitudes que obteníamos en el Ejemplo 6.2 mostraba evidencia de que no la siguen. Primero vamos a estimar de nuevo la media y la desviación típica de la distribución de estas longitudes. iris.sl=iris$Sepal.Length mu=fitdistr(iris.sl,&quot;normal&quot;)$estimate[1] sigma=fitdistr(iris.sl,&quot;normal&quot;)$estimate[2] round(c(mu,sigma),3) ## mean sd ## 5.843 0.825 En este ejemplo, usaremos los intervalos en los que la función hist agrupa por defecto estos datos. h=hist(iris.sl, plot=FALSE) h$breaks ## [1] 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 Ahora se nos presenta el problema de que los intervalos que definen estos puntos de corte (breaks) no cubren toda la recta real, que es el espacio muestral de una variable aleatoria normal. Así que tenemos que reemplazar los extremos de este vector de breaks por los límites del espacio muestral de la variable, que en este caso son \\(-\\infty\\) e \\(\\infty\\). breaks2=h$breaks breaks2[1]=-Inf #Cambiamos el primer elemento por -Infinito breaks2[length(breaks2)]=Inf #Cambiamos el último elemento por Infinito breaks2 ## [1] -Inf 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Inf Ahora podemos calcular las frecuencias de la muestra en los intervalos definidos por estos puntos de corte: freq.obs=table(cut(iris.sl,breaks=breaks2)) freq.obs ## ## (-Inf,4.5] (4.5,5] (5,5.5] (5.5,6] (6,6.5] (6.5,7] ## 5 27 27 30 31 18 ## (7,7.5] (7.5, Inf] ## 6 6 Ahora calcularemos las probabilidades teóricas. Para cada intervalo \\((x,y]\\) en los que hemos cortado la recta real, tenemos que calcular \\(P(x &lt; X\\leq y)= P(X\\leq y)-P( X\\leq x)\\), para lo que usaremos expresiones de la forma pnorm(y,mu,sigma)-pnorm(x,mu,sigma). Definimos dos vectores que nos den los extremos izquierdos y derechos de cada intervalo. extremo.izq=breaks2[-length(breaks2)] extremo.der=breaks2[-1] extremo.izq ## [1] -Inf 4.5 5.0 5.5 6.0 6.5 7.0 7.5 extremo.der ## [1] 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Inf Ahora podemos calcular las probabilidades teóricas y las frecuencias esperadas de todos los intervalos de golpe. La probabilidades teóricas: probs.teor=pnorm(extremo.der,mu,sigma)-pnorm(extremo.izq,mu,sigma) probs.teor ## [1] 0.0517955 0.1016307 0.1852753 0.2365772 0.2116091 0.1325812 0.0581747 ## [8] 0.0223563 Las frecuencias esperadas: freq.esp=probs.teor*length(iris.sl) round(freq.esp,3) ## [1] 7.769 15.245 27.791 35.487 31.741 19.887 8.726 3.353 La frecuencia esperada de la última clase es inferior a 5, así que vamos a fundirla con la penúltima y así la clase resultante tendrá una frecuencia esperada superior a 5. k=length(probs.teor) probs.teor2=c(probs.teor[1:(k-2)],sum(probs.teor[(k-1):k])) #Nuevas probabilidades teóricas freq.obs2=c(freq.obs[1:(k-2)],sum(freq.obs[(k-1):k])) #Nuevas frecuencias observadas chisq.test(freq.obs2,p=probs.teor2) ## ## Chi-squared test for given probabilities ## ## data: freq.obs2 ## X-squared = 11.12, df = 6, p-value = 0.0847 Recordemos que el p-valor obtenido no es el correcto: como hemos estimado dos parámetros, lo tenemos que calcular con una \\(\\chi^2\\) con 4 grados de libertad (dos menos de los que ha usado chisq.test): test.iris=chisq.test(freq.obs2,p=probs.teor2) 1-pchisq(test.iris$statistic,test.iris$parameter-2) ## X-squared ## 0.0252518 El p-valor es inferior a 0.05, por tanto obtenemos evidencia de que la muestra no proviene de una población normal, es decir, de que las longitudes de los sépalos de las flores iris no siguen una ley normal. Ejemplo 6.9 Vamos a repetir el estudio del ejemplo anterior, pero ahora calculando a mano los intervalos. En general, el número de intervalos debe ser suficiente para cubrir toda la forma de la distribución, pero tampoco conviene que haya muchos para evitar frecuencias esperadas pequeñas que obliguen a agrupar intervalos. Para una distribución normal se recomienda tomar entre 5 y 15 intervalos. Otra posibilidad es decidir el número de intervalos con alguna de las reglas explicadas en la Lección ?? de la primera parte del curso. En nuestro ejemplo, vamos a usar 10 intervalos. Para calcularlos, tomamos el máximo y el mínimo de las observaciones, los restamos y dividimos por el número de intervalos (y, si fuera necesario, redondearíamos adecuadamente). Ampl=(max(iris.sl)-min(iris.sl))/10 Ampl ## [1] 0.36 Los extremos de los intervalos en los que dividimos la muestra forman la secuencia que empieza en el mínimo y va sumando la amplitud hasta definir los \\(k=10\\) intervalos. Luego hay que adecuar los dos extremos para que cubran el dominio de la densidad de la distribución teórica, en nuestro caso la recta real. breaks=min(iris.sl)+Ampl*(0:10) breaks ## [1] 4.30 4.66 5.02 5.38 5.74 6.10 6.46 6.82 7.18 7.54 7.90 breaks2=breaks breaks2[1]=-Inf breaks2[length(breaks2)]=Inf breaks2 ## [1] -Inf 4.66 5.02 5.38 5.74 6.10 6.46 6.82 7.18 7.54 Inf Calculemos, como en el ejemplo anterior, las frecuencias observadas, las probabilidades teóricas y las frecuencias esperadas. frec.obs=table(cut(iris.sl,breaks=breaks2)) frec.obs ## ## (-Inf,4.66] (4.66,5.02] (5.02,5.38] (5.38,5.74] (5.74,6.1] (6.1,6.46] ## 9 23 14 27 22 20 ## (6.46,6.82] (6.82,7.18] (7.18,7.54] (7.54, Inf] ## 18 6 5 6 extremo.izq=breaks2[-length(breaks2)] extremo.der=breaks2[-1] prob.teor=pnorm(extremo.der,mu,sigma)-pnorm(extremo.izq,mu,sigma) frec.esp=round(prob.teor*length(iris.sl),2) frec.esp ## [1] 11.37 12.51 19.20 24.44 25.79 22.56 16.37 9.85 4.91 2.99 Agruparemos las frecuencias de los dos últimos intervalos y aplicaremos el test \\(\\chi^2\\) con el número adecuado de grados de libertad: frec.obs2=c(frec.obs[1:8], sum(frec.obs[9:10])) prob.teor2=c(prob.teor[1:8], sum(prob.teor[9:10])) test.iris.2=chisq.test(frec.obs2,p=prob.teor2) 1-pchisq(test.iris.2$statistic, test.iris.2$parameter-2) ## X-squared ## 0.0227734 El p-valor es de nuevo inferior a 0.05: volvemos a obtener evidencia significativa de que la muestra no proviene de una población normal. 6.4 El test de Kolgomorov-Smirnov El test de Kolgomorov-Smirnov (K-S) es un test genérico para contrastar la bondad de ajuste a distribuciones continuas. Se puede usar con muestras pequeñas (se suele recomendar 5 elementos como el tamaño mínimo para que el resultado sea significativo), pero la muestra no puede contener valores repetidos: si los contiene, la distribución del estadístico de contraste bajo la hipótesis nula no es la que predice la teoría sino que solo se aproxima a ella, y por lo tanto los p-valores que se obtienen son aproximados. Hay que tener en cuenta que el test K-S realiza un contraste en el que la hipótesis nula es que la muestra proviene de una distribución continua completamente especificada. Es decir, no sirve para contrastar si la muestra proviene, pongamos, de “alguna” distribución normal, sino solo para contrastar si proviene de una distribución normal con una media y una desviación típica concretas. Así pues, si queremos contrastar que la muestra proviene de alguna distribución de una familia concreta y estimamos sus parámetros a partir de la muestra, el test K-S solo nos permite rechazar o no la hipótesis de que la muestra proviene de la distribución de esa familia con exactamente esos parámetros. Por lo tanto, si el resultado es rechazar la hipótesis nula, esto no excluye que la muestra provenga de una distribución de la misma familia con otros parámetros. En la próxima sección veremos algunos tests que permiten contrastar, en general, si una muestra proviene de alguna distribución normal. La función básica para realizar el test K-S es ks.test. Su sintaxis básica para una muestra es ks.test(x, y, parámetros) donde: x es la muestra de una variable continua. y puede ser un segundo vector, y entonces se contrasta si ambos vectores han sido generados por la misma distribución continua, o el nombre de la función de distribución (empezando con p) que queremos contrastar, entre comillas; por ejemplo &quot;pnorm&quot; para la distribución normal. Los parámetros de la función de distribución si se ha especificado una; por ejemplo mean=0, sd=1 para una distribución normal estándar. Ejemplo 6.10 Efectuemos el test de Kolmogorov-Smirnov para contrastar si las longitudes de sépalos de flores iris siguen una distribución normal de media y desviación típica sus estimaciones máximo verosímiles a partir la muestra iris.sl. Recordemos que tenemos guardados de los dos últimos ejemplos los valores de estas estimaciones en las variables mu y sigma: round(c(mu,sigma),3) ## mean sd ## 5.843 0.825 ks.test(iris.sl, &quot;pnorm&quot;, mean=mu, sd=sigma) ## Warning in ks.test(iris.sl, &quot;pnorm&quot;, mean = mu, sd = sigma): ties should ## not be present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: iris.sl ## D = 0.08945, p-value = 0.181 ## alternative hypothesis: two-sided Obtenemos un p-valor de 0.181, que no nos permite rechazar la hipótesis de que siguen una ley N(5.843, 0.825). Pero R nos avisa de que hay empates. ¿Hay muchos? Vamos a calcular su frecuencia. La función unique aplicada a un vector nos da el vector de sus elementos sin repeticiones. De esta manera podemos saber cuántos elementos diferentes hay en un vector, y por consiguiente también cuántas repeticiones. length(unique(iris.sl)) ## [1] 35 1-length(unique(iris.sl))/length(iris.sl) ## [1] 0.766667 Por tanto, el vector (de 150 entradas) de longitudes de sépalos solo tiene 35 valores diferentes. El resto, un 76.67%, son valores repetidos. Hay muchos empates, y el resultado de este test en este caso es poco fiable Como hemos comentado, el test K-S también se puede usar para contrastar si dos muestras se han obtenido de poblaciones con la misma distribución continua. Para hacerlo, se ha de aplicar la función ks.test a las dos muestras. Ejemplo 6.11 La tabla de datos Salaries del paquete car contiene información sobre los sueldos de 397 profesores de una universidad norteamericana en el curso 2008-09. Démosle un vistazo. library(car) str(Salaries) ## &#39;data.frame&#39;: 397 obs. of 6 variables: ## $ rank : Factor w/ 3 levels &quot;AsstProf&quot;,&quot;AssocProf&quot;,..: 3 3 1 3 3 2 3 3 3 3 ... ## $ discipline : Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ yrs.since.phd: int 19 20 4 45 40 6 30 45 21 18 ... ## $ yrs.service : int 18 16 3 39 41 6 23 45 20 18 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 1 ... ## $ salary : int 139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ... La variable sex nos da el sexo del profesor y la variable salary su sueldo anual en dólares. Queremos contrastar si los sueldos de hombres y mujeres siguen la misma distribución. Para ello, vamos a suponer que provienen de distribuciones continuas y usaremos el test K-S. Primero miraremos si hay muchos empates. sal.female=Salaries[Salaries$sex==&quot;Female&quot;,]$salary #Salarios de mujeres sal.male=Salaries[Salaries$sex==&quot;Male&quot;,]$salary #Salarios de hombres 1-length(unique(sal.female))/length(sal.female) #Proporción de salarios de mujeres repetidos ## [1] 0.0512821 1-length(unique(sal.male))/length(sal.male) #Proporción de salarios de hombres repetidos ## [1] 0.0502793 1-length(unique(Salaries$salary))/length(Salaries$salary) #Proporción global de salarios repetidos ## [1] 0.0654912 Las repeticiones en cada lista significan alrededor del 5% de los datos, y en total un 6.5%. No son muchas, así que vamos a arriesgarnos con el test K-S. ks.test(sal.male,sal.female) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: sal.male and sal.female ## D = 0.2472, p-value = 0.0271 ## alternative hypothesis: two-sided El p-valor pequeño nos permite rechazar que los salarios de hombres y mujeres sigan la misma distribución. Pero no nos paremos aquí. Si dibujamos un boxplot de los salarios según el sexo, observaremos que los sueldos de los hombres tienen mayor mediana y variabilidad que los de las mujeres, incluyendo algunos valores atípicos grandes (¿el rector y otros altos cargos académicos?). boxplot(salary~sex, data=Salaries, main=&quot;&quot;) Si cancelamos este efecto, estandarizando las muestras, ¿siguen saliendo distribuciones diferentes? ks.test(scale(sal.male),scale(sal.female)) ## Warning in ks.test(scale(sal.male), scale(sal.female)): p-value will be ## approximate in the presence of ties ## ## Two-sample Kolmogorov-Smirnov test ## ## data: scale(sal.male) and scale(sal.female) ## D = 0.139, p-value = 0.505 ## alternative hypothesis: two-sided Al estandarizar, ya no tenemos evidencia de que provengan de distribuciones diferentes. Es decir, podemos aceptar que sus valores tipificados siguen la misma distribución. 6.5 Tests de normalidad Existen algunos tests específicos de normalidad que permiten contrastar si una muestra proviene de alguna distribución normal. El más conocido es el test de normalidad de Kolmogorov-Smirnov-Lilliefors (K-S-L). Se trata de una variante del test K-S, y se puede realizar aplicando a la muestra la función lillie.test del paquete nortest. Vamos a usar el test K-S-L para contrastar si las longitudes de los sépalos de las iris siguen una ley normal. library(nortest) iris.sl=iris$Sepal.Length lillie.test(iris.sl) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: iris.sl ## D = 0.08865, p-value = 0.00579 El p-valor es muy pequeño, y nos permite rechazar que la muestra provenga de una población normal. La ventaja del test K-S-L es que es muy conocido, ya que es una variante del K-S (incluso usa el mismo estadístico), pero tiene un inconveniente: aunque es muy sensible a las diferencias entre la muestra y la distribución teórica alrededor de sus valores medios, le cuesta detectar diferencias prominentes en un extremo u otro de la distribución. Esto afecta su potencia. Por ejemplo, sabemos que una t de Student se parece bastante a una normal estándar, pero su densidad es algo más aplanada y hace que en los dos extremos esté por encima de la de la normal. Al test K-S-L le cuesta detectar esta discrepancia, como podemos ver en el siguiente ejemplo: set.seed(100) x=rt(50,3) #Una muestra de una t de Student con 3 g.l. lillie.test(x) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: x ## D = 0.1033, p-value = 0.201 Este inconveniente del test K-S-L lo resuelve el test de normalidad de Anderson-Darling (A-D). Para realizarlo podemos usar la función ad.test del paquete nortest. Encontraréis los detalles del estadístico que usa en la Ayuda de la función. ad.test(iris.sl) ## ## Anderson-Darling normality test ## ## data: iris.sl ## A = 0.8892, p-value = 0.0225 De nuevo obtenemos un p-valor muy pequeño. Veamos ahora que este test sí que detecta que la muestra anterior de una t de Student con 3 grados de libertad no proviene de una normal: set.seed(100) x=rt(50,3) ad.test(x) ## ## Anderson-Darling normality test ## ## data: x ## A = 1.166, p-value = 0.00433 Un inconveniente común a los tests K-S-L y A-D es que, si bien pueden usarse con muestras pequeñas (pongamos de más de 5 elementos), se comportan mal con muestras grandes, de varios miles de elementos. En muestras de este tamaño, cualquier pequeña divergencia de la normalidad se magnifica y en estos dos tests aumenta la probabilidad de errores de tipo I. Un test que resuelve este problema es el de Shapiro-Wilk (S-W), implementado en la función shapiro.test de la instalación básica de R. Este test es importante, porque un experimento reciente ha mostrado evidencia significativa de que su potencia es mayor que la de los tests anteriores.11 De nuevo, los detalles del estadístico que usa los encontraréis en la Ayuda de la función. shapiro.test(iris.sl) ## ## Shapiro-Wilk normality test ## ## data: iris.sl ## W = 0.9761, p-value = 0.0102 set.seed(100) x=rt(50,3) shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.8949, p-value = 0.000329 Un último inconveniente que afecta a todos los tests explicados hasta ahora es el de los empates. Sus estadísticos tienen las distribuciones que se usan para calcular los p-valores cuando la muestra no tiene datos repetidos, y por lo tanto, si hay muchos, el p-valor puede no tener ningún significado. De los tres, el menos sensible a repeticiones es el S-W, pero si hay muchas es conveniente usar un test que no sea sensible a ellas, como por ejemplo el test omnibus de D’Agostino-Pearson. Este test se encuentra implementado en la función dagoTest del paquete fBasics, y lo que hace es cuantificar lo diferentes que son la asimetría y la curtosis de la muestra (dos parámetros estadísticos relacionados con la forma de la gráfica de la función de densidad muestral) respecto de los esperados en una distribución normal, y resume esta discrepancia en un p-valor con el significado usual. library(fBasics) dagoTest(iris.sl) ## ## Title: ## D&#39;Agostino Normality Test ## ## Test Results: ## STATISTIC: ## Chi2 | Omnibus: 5.7356 ## Z3 | Skewness: 1.5963 ## Z4 | Kurtosis: -1.7853 ## P VALUE: ## Omnibus Test: 0.05682 ## Skewness Test: 0.1104 ## Kurtosis Test: 0.07421 ## ## Description: ## Tue Feb 26 17:24:42 2019 by user: El p-valor relevante es el del “Omnibus test”, en este caso 0.0568 cae en la zona de penumbra. Queremos hacer una última advertencia en esta sección. Aunque los tests que hemos explicado se pueden aplicar a muestras pequeñas, es muy difícil rechazar la normalidad de una muestra muy pequeña. Por ejemplo, una muestra de 10 valores escogidos con distribución uniforme entre 0 y 5 pasa holgadamente todos los tests de normalidad (salvo el de D’Agostino-Pearson, que requiere una muestra de al menos 20 elementos): set.seed(100) x=runif(10,0,5) lillie.test(x) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: x ## D = 0.1459, p-value = 0.79 ad.test(x) ## ## Anderson-Darling normality test ## ## data: x ## A = 0.1663, p-value = 0.912 shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.9803, p-value = 0.967 dagoTest(x) ## Error in .omnibus.test(x): sample size must be at least 20 6.6 Guía rápida qqPlot del paquete car, sirve para dibujar un Q-Q-plot de una muestra contra una distribución teórica. Sus parámetros principales son: distribution: el nombre de la familia de distribuciones, entre comillas. Los parámetros de la distribución: mean para la media, sd para la desviación típica, df para los grados de libertad, etc. Los parámetros usuales de plot. chisq.test sirve para realizar tests \\(\\chi^2\\) de bondad de ajuste. Sus parámetros principales son: p: el vector de probabilidades teóricas. rescale.p: igualado a TRUE, indica que los valores de p no son probabilidades, sino sólo proporcionales a las probabilidades. simulate.p.value: igualado a TRUE, R calcula el p-valor mediante simulaciones. B: en este último caso, permite especificar el número de simulaciones. ks.test realiza el test de Kolmogorov-Smirnov. Tiene dos tipos de uso: ks.test(x,y): contrasta si los vectores x e y han sido generados por la misma distribución continua. ks.test(x, &quot;distribución&quot;, parámetros): contrasta si el vector x ha sido generado por la distribución especificada, que se ha de indicar con el nombre de la función de distribución de R (la que empieza con p). lillie.test del paquete nortest, realiza el test de normalidad de Kolmogorov-Smirnov-Lilliefors. ad.test del paquete nortest, realiza el test de normalidad de Anderson-Darling. shapiro.test, realiza el test de normalidad de Shapiro-Wilk. dagoTest del paquete fBasics, realiza el test ómnibus de D’Agostino-Pearson. 6.7 Ejercicios Modelo de test (1) Un determinado experimento tiene cinco resultados posibles: A, B, C, D, E. Lo repetimos un cierto número de veces y obtenemos 65 veces el resultado A, 95 veces el resultado B, 87 veces el resultado C, 70 veces el resultado D y 193 veces el resultado E. Realizad un test \\(\\chi^2\\) para contrastar si los resultados A, B, C y D tienen la misma probabilidad y E tiene el doble de probabilidad que cada uno de los otros resultados. Dad el p-valor del contraste (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI, en mayúsculas y sin acento, o NO) si tendríamos que rechazar la hipótesis nula con un nivel de significación \\(\\alpha=0.05\\). Tenéis que dar las respuestas en este orden y separadas por un único espacio en blanco. (2) Queremos contrastar si una determinada variable sigue una distribución de Poisson. Hemos efectuado algunas observaciones y hemos obtenido 10 veces el resultado 0, 32 veces el resultado 1, 18 veces el resultado 2, 19 veces el resultado 3 y 6 veces el resultado 4. Tenéis que calcular el estimador máximo verosímil del parámetro \\(\\lambda\\) de una variable de Poisson que haya generado estas observaciones (redondeado a 3 cifras decimales, y sin ceros innecesarios a la derecha), calcular el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del test \\(\\chi^2\\) para determinar si la muestra sigue alguna distribución de Poisson habiendo estimado como su parámetro \\(\\lambda\\) este valor redondeado, y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.1\\), tendríamos que rechazar la hipótesis nula de que esta muestra proviene de una variable aleatoria de Poisson. Dad las tres respuestas en este orden y separadas por un único espacio en blanco. (3) Queremos contrastar si una cierta variable sigue una distribución normal. Hemos efectuado 150 observaciones y hemos obtenido 9 veces un valor dentro de ]0,3], 27 veces un valor dentro de ]3,6], 51 veces un valor dentro de ]6,9], 46 veces un valor dentro de ]9,12] y 17 veces un valor dentro de ]12,15]. Tenéis que: calcular los estimadores máximo verosímiles del parámetro \\(\\mu\\) y del parámetro \\(\\sigma\\) de una variable normal que haya generado estas observaciones (ambos redondeados a 2 cifras decimales y sin ceros innecesarios a la derecha): calcular el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del test \\(\\chi^2\\) para contrastar si la muestra sigue alguna distribución normal, empleando estos valores estimados redondeados de los parámetros que habéis dado para especificar la distribución teórica; y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que esta muestra proviene de una variable aleatoria normal. Dad las cuatro respuestas en este orden y separadas por un único espacio en blanco. (4) Generad una muestra aleatoria x de 25 valores de una distribución \\(\\chi^2\\) con 10 grados de libertad, fijando antes set.seed(2014), y aplicad el test de Kolmogorov-Smirnov para contrastar si x proviene de una distribución N(10,3.16). Dad el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que esta muestra proviene de esta distribución. Tenéis que dar las dos respuestas en este orden y separadas por un único espacio en blanco. (5) Queremos contrastar si la muestra siguiente sigue una distribución normal: 4.6, 0.97, 0.3, 1.11, 2.16, 15.52, 1.13, 0.17, 0.64, 2.00. Dad el p-valor (redondeado a 3 cifras decimales y sin ceros innecesarios a la derecha) del test de Kolmogorov-Smirnov-Lilliefors para esta muestra y decid (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que esta muestra sigue una distribución normal. Tenéis que dar las dos respuestas en este orden y separadas por un único espacio en blanco. (6) Generad una muestra aleatoria x de 15 valores de una distribución normal con \\(\\mu=2\\) y \\(\\sigma=0.8\\) fijando antes set.seed(2014), y una muestra aleatoria y de 25 valores de una distribución exponencial de parámetro \\(1/\\lambda=0.5\\) fijando antes set.seed(1007). Aplicad el test de Kolmogorov-Smirnov para contrastar si x e y provienen de una misma distribución continua. Tenéis que dar el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.05\\), tendríamos que rechazar la hipótesis nula de que estas dos muestras provienen de la misma distribución continua. Dad las dos respuestas en este orden y separadas por un único espacio en blanco. Respuestas al test (1) 0.02 SI (2) 1.753 0.064 SI (3) 8.2 3.18 0.692 NO (4) 0.313 NO (5) 0.001 SI (6) 0.117 NO En inglés se utiliza la abreviatura GOF, de goodness of fit.↩ A las repeticiones se las suele llamar empates, ties en inglés, porque la función de distribución acumulada muestral ordena los datos y las repeticiones producen empates en las posiciones de valores sucesivos.↩ Lo confesamos, hemos elegido la semilla de aleatoriedad no porque seamos fans de Scarlatti sino para que la muestra obtenida solo contenga ceros, unos y doses, lo que, como veréis, motivará una pequeña discusión sobre qué clases tomar.↩ Véase N. M. Razali, Y. B. Wah, “Power comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests.” S. Stat. Model. Anal. 2 (2011), pp. 21–33.↩ "],
["chap-indep.html", "Lección 7 Contrastes de independencia y homogeneidad 7.1 Tablas de contingencia 7.2 Contraste de independencia 7.3 Contraste de homogeneidad 7.4 Potencia de un contraste \\(\\chi^2\\) 7.5 Guía rápida 7.6 Ejercicios", " Lección 7 Contrastes de independencia y homogeneidad El test \\(\\chi^2\\) explicado en la lección anterior permite contrastar, en situaciones adecuadas, si una muestra proviene de una determinada distribución y por lo tanto también si una muestra sigue la misma distribución que otra muestra. Como, en última instancia, la independencia de dos variables cualitativas se puede describir en términos de igualdades de probabilidades, el test \\(\\chi^2\\) también nos permitirá contrastar si dos variables cualitativas son independientes, tanto en el sentido de que las probabilidades conjuntas sean el producto de las probabilidades marginales (con un contraste de independencia) como en el sentido de que las distribuciones condicionadas de una respecto de los valores de la otra sean todas iguales (con un contraste de homogeneidad). Aunque, como veremos, los contrastes de independencia y homogeneidad son idénticos desde el punto de vista matemático e incluso utilizan el mismo estadístico \\(\\chi^2\\) y la misma definición de p-valor, provienen de diseños experimentales diferentes: En un contraste de independencia se toma una muestra transversal de la población, es decir, se selecciona al azar una cierta cantidad de individuos de la población, se observan las dos variables sobre cada uno de ellos, y se contrasta si las probabilidades conjuntas son iguales al producto de las probabilidades marginales de cada variable. Formalmente, si \\(X\\) e \\(Y\\) son las dos variables, se contrasta si para cada par de posibles valores \\(x\\) de \\(X\\) e \\(y\\) de \\(Y\\) se tiene que \\[ P(X=x,Y=y)=P(X=x)\\cdot P(Y=y) \\] o si por el contrario hay algún par de valores \\(x\\), \\(y\\) para los que esta igualdad sea falsa. En un contraste de homogeneidad se escoge una de las variables y para cada uno de sus posibles valores se toma una muestra aleatoria, de tamaño prefijado, de individuos con ese valor para esa variable; su unión forma una muestra estratificada en el sentido de la Sección 2.1. A continuación, se observa sobre cada uno de estos individuos la otra variable. En esta situación contrastamos si la distribución de probabilidades de la segunda variable es la misma en los diferentes estratos definidos por los niveles de la primera variable. Formalmente, sean \\(Y\\) la variable que usamos en primer lugar para clasificar los individuos de la población y tomar una muestra de cada clase, con posibles valores \\(y_1,\\ldots,y_k\\), y \\(X\\) la variable que medimos en segundo lugar sobre los individuos escogidos. Entonces, se contrasta si, para cada posible valor \\(x\\) de \\(X\\), \\[ P(X=x|Y=y_1)=P(X=x|Y=y_2)=\\cdots=P(X=x|Y=y_k) \\] o si por el contrario existen \\(x\\), \\(y_i\\), \\(y_j\\) tales que \\(P(X=x|Y=y_i)\\neq P(X=x|Y=y_j)\\). En ambos contrastes, la hipótesis nula es que las variables son independientes, bajo una u otra formulación matemática, y la hipótesis alternativa es que son dependientes (o hay asociación entre ellas). La hipótesis nula se rechaza si se obtiene evidencia que hace inverosímiles las igualdades de probabilidades que se contrastan. Para ilustrar esta lección, hemos generado una muestra aleatoria de cadenas formadas por las bases “a”, “c”, “g” y “t”. En concreto, hemos generado cadenas de longitud 100 de tres tipos: A, B y C. Estos tipos se distinguen por los vectores de probabilidades que han determinado las frecuencias de las cuatro bases en las secuencias. Queremos investigar si hay relación entre el tipo (A, B o C) de una cadena, y la base de frecuencia máxima en ella. Los datos y el método de generación se encuentran en el repositorio de GitHub https://github.com/biocom-uib/Experimento-Cadenas. Este directorio contiene: El fichero LeemeGeneracionDatos.Rmd, que contiene y explica el código de generación de las muestras. El html resultante de compilar este fichero R Markdown se puede leer en https://biocom-uib.github.io/Experimento-Cadenas/. El fichero MuestraTotalBases.txt, que contiene una tabla de datos de 10000 observaciones de las dos variables siguientes sobre cadenas: el tipo, que es un factor con los niveles A, B y C, y max.frec, que es otro factor que indica qué base tiene mayor frecuencia en la cadena. Este fichero es de formato texto, con una primera fila con el nombre de las variables y sus columnas separadas por comas. Los ficheros MuestraTipoAbases.txt, MuestraTipoBbases.txt y MuestraTipoCbases.txt, que contienen las subtablas de datos de la anterior formadas solo por las cadenas de tipo A, de tipo B y de tipo C, respectivamente. Los url de estos ficheros para poderlos cargar en una seisón de R se pueden consultar en el Readme del repositorio. El bloque de código siguientes carga en un data frame la tabla global MuestraTotalBases.txt y comprueba que no ha habido problemas. Como su url empieza con https, hay que usar la construcción especial para cargarlo que ya explicamos en la Lección ?? de la primera parte. library(RCurl) poblacion=read.csv(text=getURL(&quot;https://raw.githubusercontent.com/biocom-uib/Experimento-Cadenas/master/MuestraTotalBases.txt&quot;)) str(poblacion) ## &#39;data.frame&#39;: 10000 obs. of 2 variables: ## $ tipo : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 2 1 1 3 1 1 1 3 2 2 ... ## $ max.frec: Factor w/ 4 levels &quot;a&quot;,&quot;c&quot;,&quot;g&quot;,&quot;t&quot;: 3 1 2 4 4 1 1 3 2 2 ... head(poblacion) ## tipo max.frec ## 1 B g ## 2 A a ## 3 A c ## 4 C t ## 5 A t ## 6 A a A continuación extraemos de este data frame tres subtablas, una para cada tipo de cadena: poblacionA=subset(poblacion,tipo==&quot;A&quot;) poblacionB=subset(poblacion,tipo==&quot;B&quot;) poblacionC=subset(poblacion,tipo==&quot;C&quot;) También podríamos haber definido estos data frames a partr de los ficheros MuestraTipoAbases.txt, MuestraTipoBbases.txt y MuestraTipoCbases.txt de manera similar a como hemos cargado el fichero MuestraTotalBases.txt en el data frame poblacion. 7.1 Tablas de contingencia Ya estudiamos en la Lección ?? de la primera parte las tablas de contingencia. En esta sección vamos a repasar y ampliar algunas de las funciones de R para el manejo de esta clase de tablas. La tabla de contingencia de frecuencias absolutas conjuntas de las dos variables del data frame poblacion se calcula de la manera siguiente: tabla=table(poblacion$tipo,poblacion$max.frec) tabla ## ## a c g t ## A 1027 1003 998 998 ## B 1259 1297 209 232 ## C 245 602 1453 677 Su tabla de frecuencias relativas conjuntas en el total de la muestra, en términos de proporciones (tantos por uno), es: prop.table(tabla) ## ## a c g t ## A 0.1027 0.1003 0.0998 0.0998 ## B 0.1259 0.1297 0.0209 0.0232 ## C 0.0245 0.0602 0.1453 0.0677 Para añadir las distribuciones marginales de la tabla de contingencia (o márgenes de la tabla), se añade una nueva fila con las sumas de cada columna y una nueva columna con las sumas de cada fila. Con R, esto se puede llevar a cabo fácilmente con la función addmargins. Su sintaxis básica es addmargins(tabla, margin=..., FUN=...) donde: tabla es una table. margin es un parámetro que puede tomar los valores siguientes: 1 si queremos una nueva fila con las marginales de cada columna. 2 si queremos una nueva columna con las marginales de cada fila. c(1,2), que es el valor por defecto para tablas de contingencia bidimensionales (y por lo tanto no hace falta especificarlo), si queremos las marginales por filas y por columnas.12 FUN es la función que se aplica a las filas o columnas para obtener el valor marginal. Por defecto es la suma, que es la función que nos interesa en esta lección, y por tanto tampoco hace falta especificarlo. El resultado es otro objeto de la clase table al que se le han añadido una o varias filas o columnas. Éstas contiene los márgenes resultantes de aplicar la función indicada por FUN. La etiqueta de las nuevas filas o columnas es la función que se aplica. Por ejemplo, para obtener las tablas marginales completas de frecuencias absolutas y relativas en nuestro ejemplo, haríamos: addmargins(tabla) ## ## a c g t Sum ## A 1027 1003 998 998 4026 ## B 1259 1297 209 232 2997 ## C 245 602 1453 677 2977 ## Sum 2531 2902 2660 1907 10000 addmargins(prop.table(tabla)) ## ## a c g t Sum ## A 0.1027 0.1003 0.0998 0.0998 0.4026 ## B 0.1259 0.1297 0.0209 0.0232 0.2997 ## C 0.0245 0.0602 0.1453 0.0677 0.2977 ## Sum 0.2531 0.2902 0.2660 0.1907 1.0000 También podemos calcular la tabla de proporciones por filas y con su marginal por filas comprobar que efectivamente la suma de cada fila es 1: addmargins(prop.table(tabla,margin=1),margin=2) ## ## a c g t Sum ## A 0.2550919 0.2491307 0.2478887 0.2478887 1.0000000 ## B 0.4200868 0.4327661 0.0697364 0.0774107 1.0000000 ## C 0.0822976 0.2022170 0.4880752 0.2274101 1.0000000 Y viceversa, podemos calcular la tabla de proporciones por columnas y con su marginal por columnas comprobar que efectivamente la suma de cada columna es 1: addmargins(prop.table(tabla,margin=2),margin=1) ## ## a c g t ## A 0.4057685 0.3456237 0.3751880 0.5233351 ## B 0.4974318 0.4469331 0.0785714 0.1216571 ## C 0.0967997 0.2074431 0.5462406 0.3550079 ## Sum 1.0000000 1.0000000 1.0000000 1.0000000 Observad que el significado de margin en addmargins es diferente de, por ejemplo, en prop.table o en apply: en estas dos últimas instrucciones indica la dimensión en la que calculamos las proporciones o aplicamos la función, mientras que en addmargins indica la dimensión en la que añadimos el margen, que, por lo tanto, se calcula aplicando la función en la otra dimensión. Si sólo nos interesa la fila o la columna de marginales, podemos usar las instrucciones colSums y rowSums, que suman una tabla por columnas y por filas, respectivamente. Por ejemplo, para obtener los vectores de marginales por columnas y por filas, respectivamente, podríamos entrar: colSums(tabla) ## a c g t ## 2531 2902 2660 1907 rowSums(tabla) ## A B C ## 4026 2997 2977 También podemos obtener estos márgenes extrayendo los márgenes de la tabla con márgenes, obtenida aplicando addmargins a la tabla original, por medio de las instrucciones usuales para extraer filas y columnas. addmargins(tabla)[&quot;Sum&quot;,-dim(addmargins(tabla))[2]] ## a c g t ## 2531 2902 2660 1907 addmargins(tabla)[-dim(addmargins(tabla))[1],&quot;Sum&quot;] ## A B C ## 4026 2997 2977 Observad, por ejemplo, la construcción addmargins(tabla)[&quot;Sum&quot;,-dim(addmargins(tabla))[2]] Con addmargins(tabla)[&quot;Sum&quot;,] obtendríamos la fila Sum de la tabla con las marginales, incluyendo la última entrada, correspondiente a la columna Sum. Por lo tanto, hay que eliminarla: como dim(addmargins(tabla))[2] es el número de columnas de la tabla addmargins(tabla), es decir, la longitud del vector addmargins(tabla)[&quot;Sum&quot;,], la última entrada (correspondiente a la última columna) se puede eliminar especificando -dim(addmargins(tabla))[2] en las columnas al extraer la fila Sum. 7.2 Contraste de independencia El contraste de independencia para tablas de contingencia bidimensionales consiste en decidir si las dos variables de la tabla tienen distribuciones independientes, es decir, si la distribución de probabilidades conjunta es igual al producto de las probabilidades marginales. En nuestro ejemplo, queremos decidir si podemos aceptar que las variables tipo y max.frec son independientes o si por el contrario hay evidencia de que la distribución de las bases de máxima frecuencia depende del tipo de cadena. Vamos a extraer una muestra aleatoria simple de la población y observar los valores de las dos variables. En concreto, seleccionaremos una muestra transversal de 150 filas, al azar y con reposición, de entre las 10000 filas del data frame poblacion. El código es el siguiente (fijamos la semilla de aleatoriedad para que sea reproducible): set.seed(42) n=150 indices.muestra=sample(1:10000, size=n, replace=TRUE) muestra.test.indep= poblacion[indices.muestra, ] #Las filas que forman la muestra Ahora calculamos la tabla de contingencia con sus marginales. tabla.ind=table(muestra.test.indep$tipo, muestra.test.indep$max.frec) tabla.ind ## ## a c g t ## A 15 10 16 17 ## B 19 18 0 4 ## C 3 10 25 13 tabla.ind.marg=addmargins(tabla.ind) tabla.ind.marg ## ## a c g t Sum ## A 15 10 16 17 58 ## B 19 18 0 4 41 ## C 3 10 25 13 51 ## Sum 37 38 41 34 150 Extraemos sus dos márgenes. Las frecuencias marginales de las filas: frec.abs.tipo=tabla.ind.marg[-dim(tabla.ind.marg)[1],&quot;Sum&quot;] frec.abs.tipo ## A B C ## 58 41 51 Las frecuencias marginales de las columnas: frec.abs.max.frec=tabla.ind.marg[&quot;Sum&quot;,-dim(tabla.ind.marg)[2]] frec.abs.max.frec ## a c g t ## 37 38 41 34 El test de independencia usa las frecuencias absolutas esperadas bajo la hipótesis nula de independencia, que se obtienen, para cada celda (i,j), multiplicando la frecuencia marginal de la fila i por la de la columna j y dividiendo por el tamaño de la muestra. En nuestro ejemplo estas frecuencias esperadas son \\[ \\begin{array}{l|cccc} &amp; \\mbox{a} &amp; \\mbox{c} &amp; \\mbox{g} &amp; \\mbox{t} \\\\ \\hline \\mbox{A} &amp; 58\\cdot 37/150 &amp; 58\\cdot 38/150 &amp; 58\\cdot 41/150 &amp; 58\\cdot 34/150 \\\\ \\mbox{B} &amp; 41\\cdot 37/150 &amp; 41\\cdot 38/150 &amp; 41\\cdot 41/150 &amp; 41\\cdot 34/150 \\\\ \\mbox{C} &amp; 51\\cdot 37/150 &amp; 51\\cdot 38/150 &amp; 51\\cdot 41/150 &amp; 51\\cdot 34/150 \\end{array} \\] y podemos obtenerlas fácilmente mediante un producto de matrices: \\[ \\frac{1}{150}\\cdot\\left(\\begin{array}{c} 58 \\\\ 41 \\\\ 51\\end{array}\\right)\\cdot \\big( 37 ,38 , 41 , 34\\big). \\] Por lo tanto, con R obtenemos esta tabla de frecuencias esperadas de la manera siguiente: frec.esperadas=frec.abs.tipo%*%t(frec.abs.max.frec)/n frec.esperadas ## a c g t ## [1,] 14.3067 14.6933 15.8533 13.14667 ## [2,] 10.1133 10.3867 11.2067 9.29333 ## [3,] 12.5800 12.9200 13.9400 11.56000 Aunque vayamos a realizar el test de independencia con una función de R, es necesario comprobar que todas estas frecuencias esperadas (o al menos la gran mayoría) son mayores o iguales que 5, por lo que no podemos evitar este cálculo. En este caso vemos que se cumple esta condición. Si queremos realizar el test \\(\\chi^2\\) de independencia a mano, podemos calcular el estadístico de forma directa con chi2.estadistico=sum((tabla.ind-frec.esperadas)^2/frec.esperadas) chi2.estadistico ## [1] 47.1842 y el p-valor del contraste, con p.valor=1-pchisq(chi2.estadistico,df=(4-1)*(3-1)) p.valor ## [1] 1.71932e-08 Para realizar el test \\(\\chi^2\\) de independencia con R, es suficiente aplicar la función chisq.test a la tabla de contingencia de frecuencias absolutas: chisq.test(tabla.ind) ## ## Pearson&#39;s Chi-squared test ## ## data: tabla.ind ## X-squared = 47.18, df = 6, p-value = 1.72e-08 Como el p-valor es muy pequeño, podemos rechazar la hipótesis de que las variables objeto de estudio sean independientes: hemos obtenido evidencia estadísticamente significativa de que la distribución de las bases de frecuencia máxima sí que depende del tipo de cadena. Si algunas frecuencias absolutas esperadas son inferiores a 5, la aproximación del p-valor por una distribución \\(\\chi^2\\) podría no ser adecuada. En este caso, al ser las variables cualitativas, no podemos recurrir al agrupamiento de valores consecutivos, puesto que no tienen orden. Si se da esta situación, lo mejor es recurrir a simular el p-valor usando el parámetro simulate.p.value=TRUE. Por ejemplo, consideremos la situación siguiente: set.seed(300) n2=100 indices.muestra2=sample(1:10000,size=n2,replace=TRUE) muestra.test.indep2= poblacion[indices.muestra2,] tabla.ind2=table(muestra.test.indep2$tipo,muestra.test.indep2$max.frec) tabla.ind2 ## ## a c g t ## A 11 12 10 9 ## B 10 18 1 1 ## C 4 7 13 4 Si aplicamos a esta tabla la función chisq.test, obtenemos: chisq.test(tabla.ind2) ## Warning in chisq.test(tabla.ind2): Chi-squared approximation may be ## incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: tabla.ind2 ## X-squared = 23.63, df = 6, p-value = 0.00061 ¡Vaya! Veamos la tabla de frecuencias esperadas: frec.abs.tipo2=rowSums(tabla.ind2) frec.abs.max.frec2=colSums(tabla.ind2) frec.esperadas2=frec.abs.tipo2%*%t(frec.abs.max.frec2)/n2 frec.esperadas2 ## a c g t ## [1,] 10.5 15.54 10.08 5.88 ## [2,] 7.5 11.10 7.20 4.20 ## [3,] 7.0 10.36 6.72 3.92 Hay frecuencias esperadas inferiores a 5. Por lo tanto, lo recomendable es calcular el p-valor del test \\(\\chi^2\\) de independencia mediante simulaciones. Pero ahora tenemos que ir con cuidado en una cosa: hemos fijado la semilla de aleatoriedad para obtener una muestra de cadenas con frecuencias esperadas inferiores a 5. Lo recomendable es reiniciar esta semilla a un valor aleatorio con set.seed(NULL). set.seed(NULL) chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.0009998 chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.0009998 chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.00079984 chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value ## [1] 0.00079984 El p-valor es sistemáticamente pequeño, lo que nos permite rechazar la hipótesis de que las variables son independientes. 7.3 Contraste de homogeneidad Como ya hemos dicho, la diferencia entre el contraste de homogeneidad y el de independencia está en el diseño del experimento: en cada contraste se selecciona la muestra de una manera diferente. En nuestro caso, para contrastar si la distribución de probabilidad de la base de mayor frecuencia es la misma para cada tipo de cadena o no, lo que vamos a hacer es tomar una muestra aleatoria de cadenas de cada tipo de 50 elementos cada una, juntarlas en una sola muestra estratificada, y aplicar el test \\(\\chi^2\\) a esta muestra. El código siguiente realiza el muestreo en cada subpoblación de tipo y guarda la muestra total en el data frame muestra.test.homo. Fijamos de nuevo la semilla de aleatoriedad (otra), para que el test sea reproducible. set.seed(100) Generamos los vectores de índices de las muestras: n3=50 indices.muestraA=sample(1:dim(poblacionA)[1],size=n3,replace=TRUE) indices.muestraB=sample(1:dim(poblacionB)[1],size=n3,replace=TRUE) indices.muestraC=sample(1:dim(poblacionC)[1],size=n3,replace=TRUE) Finalmente, tomamos las filas de cada muestra y las combinamos en un data frame: muestraA.50=poblacionA[indices.muestraA,] muestraB.50=poblacionB[indices.muestraB,] muestraC.50=poblacionC[indices.muestraC,] muestra.test.homo=rbind(muestraA.50,muestraB.50,muestraC.50) str(muestra.test.homo) ## &#39;data.frame&#39;: 150 obs. of 2 variables: ## $ tipo : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ max.frec: Factor w/ 4 levels &quot;a&quot;,&quot;c&quot;,&quot;g&quot;,&quot;t&quot;: 4 2 2 4 3 2 1 1 3 4 ... Calculamos la tabla de contingencia de la muestra: tabla.homo=table(muestra.test.homo$tipo,muestra.test.homo$max.frec) tabla.homo ## ## a c g t ## A 10 16 16 8 ## B 21 21 6 2 ## C 5 12 23 10 Añadimos los márgenes: addmargins(tabla.homo) ## ## a c g t Sum ## A 10 16 16 8 50 ## B 21 21 6 2 50 ## C 5 12 23 10 50 ## Sum 36 49 45 20 150 Confirmamos que hemos tomado 50 cadenas de cada grupo. Ahora calculamos las frecuencias esperadas bajo la hipótesis nula, para comprobar si son todas mayores o iguales que 5: frec.abs.tipo=rowSums(tabla.homo) frec.abs.tipo ## A B C ## 50 50 50 frec.abs.max.frec=colSums(tabla.homo) frec.abs.max.frec ## a c g t ## 36 49 45 20 frec.esperadas=frec.abs.tipo%*%t(frec.abs.max.frec)/sum(frec.abs.tipo) frec.esperadas ## a c g t ## [1,] 12 16.3333 15 6.66667 ## [2,] 12 16.3333 15 6.66667 ## [3,] 12 16.3333 15 6.66667 Todas las frecuencias son mayores o iguales que 5, así que aplicamos la función chisq.test sin simular el p-valor: chisq.test(tabla.homo) ## ## Pearson&#39;s Chi-squared test ## ## data: tabla.homo ## X-squared = 28.59, df = 6, p-value = 7.27e-05 El p-valor es muy pequeño, por lo que podemos rechazar que las distribuciones de los valores de las bases de máxima frecuencia sean la misma para cada valor de la variable tipo. En definitiva, el tipo de cadena afecta a la distribución de la base de mayor frecuencia. Es la misma conclusión a la que habíamos llegado con el test de independencia, solo que ahora hemos realizado un tipo de experimento diferente. 7.4 Potencia de un contraste \\(\\chi^2\\) La potencia de un contraste \\(\\chi^2\\), tanto de bondad de ajuste como de independencia o de homogeneidad, se puede calcular de manera similar a cómo lo hacíamos en otros tipos de contrastes de uno y dos parámetros. La instrucción para llevarlo a cabo es pwr.chisq.test del paquete pwr. Su sintaxis básica es pwr.chisq.test(N=..., df=..., sig.level=..., w=..., power=...) donde: N es el tamaño de la muestra. df es el número de grados de libertad del estadístico.13 sig.level es el nivel de significación \\(\\alpha\\). w es la magnitud del efecto, que en este tipo de tests se define como \\(\\sqrt{X^2/N}\\), siendo \\(X^2\\) el estadístico de contraste y \\(N\\) el tamaño de la muestra. power es la potencia \\(1-\\beta\\). Si se especifican todos estos parámetros menos uno, la función da el valor del parámetro que falta. Normalmente, querremos saber la potencia de un contraste a posteriori o el tamaño de la muestra necesario para tener la potencia deseada para una magnitud del efecto esperada concreta. Veamos algunos ejemplos de uso. Ejemplo 7.1 Vamos a calcular la potencia del contraste del Ejemplo 6.3. En ese ejemplo, el tamaño de la muestra fue \\(N=40\\), el número de grados de libertad fue 5 y obtuvimos que \\(X^2=7.7\\), por lo que la magnitud del efecto fue \\(w=\\sqrt{7.7/40}\\). Tomaremos el nivel de significación usual, \\(\\alpha=0.05\\). library(pwr) pwr.chisq.test(N=40, df=5, sig.level=0.05, w=sqrt(7.7/40)) ## ## Chi squared power calculation ## ## w = 0.438748 ## N = 40 ## df = 5 ## sig.level = 0.05 ## power = 0.545751 ## ## NOTE: N is the number of observations La potencia del contraste ha sido de, aproximadamente, un 55%. Ejemplo 7.2 Vamos a calcular la potencia del contraste de normalidad de las longitudes de los sépalos de flores iris del Ejemplo 6.8. En ese ejemplo el tamaño de muestra fue \\(N=150\\); como usamos 7 clases, pero estimamos 2 parámetros, el número de grados de libertad fue 4; obtuvimos que \\(X^2=11.0637\\), por lo que \\(w=\\sqrt{11.0637/150}\\); y ahora, por variar, tomaremos \\(\\alpha=0.1\\). pwr.chisq.test(N=150, df=4, sig.level=0.1, w=sqrt(11.0637/15)) ## ## Chi squared power calculation ## ## w = 0.858825 ## N = 150 ## df = 4 ## sig.level = 0.1 ## power = 1 ## ## NOTE: N is the number of observations La potencia da 1: la probabilidad de que aceptáramos que la muestra seguía una distribución normal si no fuera verdad es prácticamente 0. Ejemplo 7.3 En el contraste de homogeneidad de la Sección 7.3 hemos tomado tres muestras de 50 individuos cada una, en total 150 individuos. El estadístico de contraste ha valido \\(X^2=29.7373\\), por lo que la magnitud del efecto en ese test ha sido de \\(w=\\sqrt{29.7373/150}=0.445\\), entre mediana y grande según la función cohen.ES, que nos da las magnitudes del efecto que por convención se entienden como pequeñas, medianas o grandes para los diferentes tests considerados en el paquete pwr: cohen.ES(test=&quot;chisq&quot;, size=&quot;medium&quot;)$effect.size ## [1] 0.3 cohen.ES(test=&quot;chisq&quot;, size=&quot;large&quot;)$effect.size ## [1] 0.5 ¿De qué tamaño deberíamos haber tomado las muestras para garantizar una potencia del 90%, suponiendo que esperásemos una magnitud del efecto mediana y tomásemos un nivel de significación \\(\\alpha=0.05\\)? pwr.chisq.test(df=6, sig.level=0.05, w=0.3, power=0.9) ## ## Chi squared power calculation ## ## w = 0.3 ## N = 193.543 ## df = 6 ## sig.level = 0.05 ## power = 0.9 ## ## NOTE: N is the number of observations Hubiéramos necesitado como mínimo un total de unos 194 individuos: si queríamos tomar las tres muestras del mismo tamaño, esto significa tres muestras de como mínimo 65 individuos cada una. 7.5 Guía rápida table calcula tablas de contingencia de frecuencias absolutas. prop.table calcula tablas de contingencia de frecuencias relativas. addmargins sirve para añadir a una table una fila o una columna obtenidas aplicando una función a todas las columnas o a todas las filas de la tabla, respectivamente. Sus parámetros principales son: margin: igualado a 1, se aplica la función por columnas, añadiendo una nueva fila; igualado a 2, se aplica la función por filas, añadiendo una nueva columna; igualado a c(1,2), que es su valor por defecto, hace ambas cosas. FUN: la función que se aplica a las filas o columnas; su valor por defecto es sum. colSums calcula un vector con las sumas de las columnas de una matriz o una tabla. rowSums calcula un vector con las sumas de las filas de una matriz o una tabla. chisq.test sirve para realizar tests \\(\\chi^2\\) de independencia y homogeneidad. El resultado es una list formada, entre otros, por los objetos siguientes: statistic (el valor del estadístico \\(X^2\\)), parameter (los grados de libertad) y p.value (el p-valor). Sus parámetros principales en el contexto de esta lección son: simulate.p.value: igualado a TRUE, calcula el p-valor mediante simulaciones. B: en este último caso, permite especificar el número de simulaciones. pwr.chisq.test del paquete pwr, sirve para calcular uno de los parámetros siguientes a partir de los otros cuatro: N: el tamaño de la muestra. df: el número de grados de libertad del contraste. sig.level: el nivel de significación \\(\\alpha\\). power: la potencia \\(1-\\beta\\). w: la magnitud del efecto. 7.6 Ejercicios Modelo de test (1) Hemos observado dos variables cualitativas en una muestra de una población. Cada variable tiene 3 niveles. La tabla de contingencia resultante ha sido la siguiente: \\[ \\begin{array}{c|ccc} &amp;X&amp;Y&amp;Z\\cr\\hline A &amp; 2 &amp; 17 &amp; 11\\cr B &amp; 8 &amp; 10 &amp; 25\\cr C &amp; 3 &amp; 14 &amp; 5 \\end{array} \\] ¿Es verdad que, si estas variables aleatorias fueran independientes, las frecuencias esperadas de cada combinación de niveles, uno de cada variable, serían todas \\(\\geq 5\\)? Tenéis que contestar SI, en mayúsculas y sin acento, o NO. (2) Hemos observado dos variables cualitativas en una muestra de una población. Una variable tiene 4 niveles y la otra 3. La tabla de contingencia resultante ha sido la siguiente: \\[ \\begin{array}{c|cccc} &amp;A&amp;B&amp;C &amp; D\\cr\\hline X &amp; 50 &amp; 19&amp;17 &amp; 21\\cr Y &amp;69 &amp; 47 &amp; 56 &amp; 37 \\cr Z &amp;33 &amp; 23 &amp; 18 &amp; 21 \\end{array} \\] Emplead la función chisq.test para contrastar si estas dos variables son independientes o no. Tenéis que dar el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decir (contestando SI o NO) si, con un nivel de significación \\(\\alpha=0.1\\), podríamos rechazar la hipótesis nula de que estas dos variables son independientes. Dad las dos respuestas en este orden y separadas por un único espacio en blanco. (3) Hemos realizado un test \\(\\chi^2\\) de independencia sobre una muestra de 200 individuos, con un nivel de significación de 0.1. Las variables objeto de estudio tenían 5 y 6 niveles, respectivamente. El estadístico de contraste ha valido \\(16.56\\). ¿Cuál es el p-valor del contraste? ¿Cuál es la potencia del contraste realizado? Tenéis que dar ambos valores en este orden, redondeados a 3 cifras decimales sin ceros innecesarios a la derecha, y separados por un único espacio en blanco. Respuestas al test (1) NO (2) 0.128 NO (3) 0.681 0.777 El valor por defecto de margin es el vector de todas las dimensiones de la tabla. Hay que recordar que, aunque ahora sólo tratamos con tablas bidimensionales, con table se pueden especificar tablas de contingencia de un número arbitrario de dimensiones.↩ El número de clases menos 1 y menos el número de parámetros estimados en un test de bondad de ajuste, el número de niveles de una variable menos 1 por el número de niveles de la otra variable menos 1 en un test de independencia o de homogeneidad.↩ "],
["introduccion-a-la-estadistica-descriptiva-multidimensional.html", "Lección 8 Introducción a la estadística descriptiva multidimensional 8.1 Matrices de datos cuantitativos 8.2 Transformaciones lineales 8.3 Covarianzas y correlaciones 8.4 Correlación de Spearman 8.5 Contrastes de correlación 8.6 Un ejemplo 8.7 Representación gráfica de datos multidimensionales 8.8 Guía rápida 8.9 Ejercicios", " Lección 8 Introducción a la estadística descriptiva multidimensional En general, los datos que se recogen en experimentos son multidimensionales: medimos varias variables aleatorias sobre una misma muestra de individuos, y organizamos esta información en tablas de datos en las que las filas representan los individuos observados y cada columna corresponde a una variable diferente. En las lecciones finales de la primera parte ya aparecieron datos cualitativos y ordinales multidimensionales, para los que calculamos y representamos gráficamente sus frecuencias globales y marginales; en esta lección estudiamos algunos estadísticos específicos para resumir y representar la relación existente entre diversas variables cuantitativas. 8.1 Matrices de datos cuantitativos Supongamos que hemos medido los valores de \\(p\\) variables aleatorias \\(X_1,\\ldots,X_p\\) sobre un conjunto de \\(n\\) individuos u objetos. Es decir, tenemos \\(n\\) observaciones de \\(p\\) variables. En cada observación, los valores que toman estas variables forman un vector que será una realización del vector aleatorio \\(\\underline{X}=(X_1,X_2,\\ldots,X_p)\\). Para trabajar con estas observaciones, las dispondremos en una tabla de datos donde cada fila corresponde a un individuo y cada columna, a una variable. En R, lo más conveniente es definir esta tabla en forma de data frame, pero, por conveniencia de lenguaje, en el texto de esta lección la representaremos como una matriz \\[ {X}=\\begin{pmatrix} x_{1 1} &amp; x_{1 2} &amp;\\ldots &amp; x_{1 p}\\\\ x_{2 1} &amp; x_{2 2} &amp;\\ldots &amp; x_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ x_{n 1} &amp; x_{n 2} &amp;\\ldots &amp; x_{n p} \\end{pmatrix}. \\] Utilizaremos las notaciones siguientes: Denotaremos la \\(i\\)-ésima fila de \\(X\\) por \\[ {x}_{i\\bullet}=(x_{i 1}, x_{i 2}, \\ldots, x_{i p}). \\] Este vector está compuesto por las observaciones de las \\(p\\) variables sobre el \\(i\\)-ésimo individuo. Denotaremos la \\(j\\)-ésima columna de \\(X\\) por \\[ x_{\\bullet j}=\\begin{pmatrix}x_{1 j} \\\\ x_{2 j}\\\\ \\vdots \\\\ x_{n j} \\end{pmatrix}. \\] Esta columna está formada por todos los valores de la \\(j\\)-ésima variable, es decir, es una muestra de \\(X_j\\). Observad que, en cada caso, la bolita \\(\\bullet\\) en el subíndice representa el índice “variable” de los elementos del vector o de la columna. De esta manera, podremos expresar la matriz de datos \\(X\\) tanto por filas (individuos) como por columnas (muestras de variables): \\[ {X}=\\begin{pmatrix}{x}_{1\\bullet}\\\\x_{2\\bullet}\\\\\\vdots \\\\ {x}_{n\\bullet}\\end{pmatrix}=({x}_{\\bullet1}, {x}_{\\bullet 2}, \\ldots, {x}_{\\bullet p}). \\] Con estas notaciones, podemos generalizar al caso multidimensional los estadísticos de una variable cuantitativa, definiéndolos como los vectores que se obtienen aplicando el estadístico concreto a cada columna de la tabla de datos. Así: El vector de medias de \\(X\\) es el vector formado por las medias aritméticas de sus columnas: \\[ \\overline{X}=(\\overline{{{x}}}_{\\bullet1}, \\overline{{x}}_{\\bullet 2}, \\ldots, \\overline{{x}}_{\\bullet p}), \\] donde, para cada \\(j=1, \\ldots, p\\), \\[ \\overline{{x}}_{\\bullet j}=\\frac{1}{n}\\sum\\limits_{i=1}^n x_{i j}. \\] Observemos que \\[ \\begin{array}{rl} \\overline{X} &amp; \\displaystyle = (\\overline{{{x}}}_{\\bullet1}, \\overline{x}_{\\bullet 2},\\ldots,\\overline{x}_{\\bullet p}) = \\frac{1}{n} \\Big(\\sum_{i=1}^n x_{i 1}, \\sum_{i=1}^n x_{i 2},\\ldots, \\sum_{i=1}^n x_{i p}\\Big)\\\\[1ex] &amp; \\displaystyle =\\frac{1}{n} \\sum_{i=1}^n (x_{i 1}, x_{i 2},\\ldots,x_{i p} ) = \\frac{1}{n} \\sum_{i=1}^n {{x}_{i\\bullet}} \\end{array} \\] Es decir, el vector de medias de \\(X\\) es la media aritmética de sus vectores fila. El vector de varianzas de \\(X\\) es el vector formado por las varianzas de sus columnas: \\[ s^2_{X}=(s^2_{1}, s^2_2, \\ldots, s^2_p), \\] donde \\[ s_j^2=\\frac{1}{n}\\sum_{i=1}^n {(x_{ij}-\\overline{{x}}_{\\bullet j})^2}. \\] El vector de varianzas muestrales de \\(X\\) está formado por las varianzas muestrales de sus columnas: \\[ \\widetilde{s}^2_{X}=(\\widetilde{s}^2_{1}, \\widetilde{s}^2_2, \\ldots, \\widetilde{s}^2_p), \\] donde \\[ \\widetilde{s}_j^2=\\frac{1}{n-1}\\sum_{i=1}^n {(x_{ij}-\\overline{{x}}_{\\bullet j})^2}=\\frac{n}{n-1}s_j^2. \\] Los vectores de desviaciones típicas \\(s_{X}\\) y de desviaciones típicas muestrales \\(\\widetilde{s}_{X}\\) de \\(X\\) son los formados por las desviaciones típicas y las desviaciones típicas muestrales de sus columnas, respectivamente: \\[ \\begin{array}{l} s_{X}=(s_{1}, s_2, \\ldots, s_p)=(+\\sqrt{\\vphantom{s_p^2}{s}^2_{1}}, +\\sqrt{\\vphantom{s_p^2}{s}^2_2}, \\ldots, +\\sqrt{s_p^2})\\\\[1ex] \\widetilde{s}_{X}=(\\widetilde{s}_{1}, \\widetilde{s}_2, \\ldots, \\widetilde{s}_p)=(+\\sqrt{\\vphantom{s_p^2}\\widetilde{s}^2_{1}}, +\\sqrt{\\vphantom{s_p^2}\\widetilde{s}^2_2}, \\ldots, +\\sqrt{\\widetilde{s}^2_p}) \\end{array} \\] Como en el caso unidimensional, \\(\\overline{X}\\) es un estimador insesgado de la esperanza \\(E(\\underline{X})=\\boldsymbol\\mu\\) del vector aleatorio \\(\\underline{X}\\) del cual \\(X\\) es una muestra. Por lo que refiere a \\({s}^2_{X}\\) y \\(\\widetilde{s}^2_{X}\\), ambas son estimadores del vector de varianzas de \\(\\underline{X}\\): \\(\\widetilde{s}^2_{X}\\) es insesgado y, cuando todas las variables aleatorias del vector son normales, \\({s}^2_{X}\\) es el máximo verosímil. Estos vectores de estadísticos se pueden calcular con R aplicando la función correspondiente al estadístico a todas las columnas de la tabla de datos. La manera más sencilla de hacerlo en un solo paso es usando la función sapply, si tenemos guardada la tabla como un data frame, o apply con MARGIN=2, si la tenemos guardada en forma de matriz. Ejemplo 8.1 Consideremos la tabla de datos \\[ X=\\begin{pmatrix} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{pmatrix} \\] formada por 4 observaciones de 3 variables; por lo tanto, \\(n=4\\) y \\(p=3\\). Vamos a guardarla en un data frame y a calcular sus estadísticos. X=data.frame(V1=c(1,1,2,3),V2=c(-1,0,3,0),V3=c(3,3,0,1)) X ## V1 V2 V3 ## 1 1 -1 3 ## 2 1 0 3 ## 3 2 3 0 ## 4 3 0 1 Su vector de medias es: sapply(X, mean) ## V1 V2 V3 ## 1.75 0.50 1.75 Su vector de varianzas muestrales es: sapply(X, var) ## V1 V2 V3 ## 0.916667 3.000000 2.250000 Su vector de desviaciones típicas muestrales es: sapply(X, sd) ## V1 V2 V3 ## 0.957427 1.732051 1.500000 Su vector de varianzas es: var_ver=function(x){var(x)*(length(x)-1)/length(x)} #Varianza &quot;verdadera&quot; sapply(X, var_ver) ## V1 V2 V3 ## 0.6875 2.2500 1.6875 Su vector de desviaciones típicas es: sd_ver=function(x){sqrt(var_ver(x))} #Desv. típica &quot;verdadera&quot; sapply(X, sd_ver) ## V1 V2 V3 ## 0.829156 1.500000 1.299038 Nota. De ahora en adelante, supondremos que todos los vectores de datos cuantitativos que aparezcan en lo que queda de lección, incluidas las columnas de tablas de datos, son no constantes y, por lo tanto, tienen desviación típica no nula. 8.2 Transformaciones lineales A veces es conveniente aplicar una transformación lineal a una tabla de datos \\(X\\), sumando a cada columna un valor y luego multiplicando cada columna resultante por otro valor. Los dos ejemplos más comunes de trasformación lineal son el centrado y la tipificación de datos. Para centrar una matriz de datos \\(X\\), se resta a cada columna su media aritmética: \\[ \\widetilde{X}= \\begin{pmatrix} x_{1 1}- \\overline{x}_{\\bullet 1}&amp; x_{1 2}- \\overline{x}_{\\bullet 2} &amp;\\ldots &amp; x_{1 p}- \\overline{x}_{\\bullet p}\\\\ x_{2 1} - \\overline{x}_{\\bullet 1}&amp; x_{2 2}- \\overline{x}_{\\bullet 2} &amp;\\ldots &amp; x_{2 p}- \\overline{x}_{\\bullet p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ x_{n 1} - \\overline{x}_{\\bullet 1}&amp; x_{n 2}- \\overline{x}_{\\bullet 2} &amp;\\ldots &amp; x_{n p}- \\overline{x}_{\\bullet p} \\end{pmatrix}. \\] Llamaremos a esta matriz la matriz de datos centrados de \\(X\\). Ejemplo 8.2 Consideremos de nuevo la matriz de datos del Ejemplo 8.1, \\[ {X}=\\begin{pmatrix} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{pmatrix} \\] Para centrarla, hemos de restar a cada columna su media. Ya hemos calculado estas medias hace un momento: sapply(X, mean) ## V1 V2 V3 ## 1.75 0.50 1.75 Por lo tanto, su matriz de datos centrados es \\[ \\widetilde{X}=\\begin{pmatrix} 1-1.75&amp;-1-0.5&amp;3-1.75\\\\ 1-1.75&amp;0-0.5&amp;3-1.75\\\\ 2-1.75&amp;3-0.5&amp;0-1.75 \\\\ 3-1.75&amp;0-0.5&amp;1-1.75 \\end{pmatrix}= \\begin{pmatrix} -0.75&amp;-1.5&amp;1.25\\\\ -0.75&amp;-0.5&amp;1.25\\\\ 0.25&amp;2.5&amp;-1.75\\\\ 1.25&amp;-0.5&amp;-0.75 \\end{pmatrix}. \\] Dado un vector de datos formado por una muestra de una variable cuantitativa, su vector de datos tipificados es el vector que se obtiene restando a cada entrada la media aritmética del vector y dividiendo el resultado por su desviación típica. De esta manera, se obtiene un vector de datos de media aritmética 0 y varianza 1. Tipificar un vector de datos es conveniente cuando se quiere trabajar con estos datos sin que influyan ni su media ni las unidades en los que están medidos: al dividir por su desviación típica, los valores resultantes son adimensionales. Por lo tanto, tipificar las variables de una tabla de datos permite compararlas dejando de lado las diferencias que pueda haber entre sus valores medios o sus varianzas. La matriz tipificada de una matriz de datos \\(X\\) es la matriz \\(Z\\) que se obtiene tipificando cada columna; es decir, para tipificar una matriz de datos \\(X\\), restamos a cada columna su media y a continuación dividimos cada columna por la desviación típica de la columna original en \\(X\\) (que coincide con la desviación típica de la columna “centrada”, puesto que sumar o restar constantes no modifica la desviación típica): \\[ Z=\\begin{pmatrix} \\frac{x_{1 1}- \\overline{x}_{\\bullet 1}}{s_1}&amp; \\frac{x_{1 2}- \\overline{x}_{\\bullet 2}}{s_2} &amp;\\ldots &amp; \\frac{x_{1 p}- \\overline{x}_{\\bullet p}}{s_p}\\\\[2ex] \\frac{x_{2 1} - \\overline{x}_{\\bullet 1}}{s_1}&amp; \\frac{x_{2 2}- \\overline{x}_{\\bullet 2}}{s_2} &amp;\\ldots &amp; \\frac{x_{2 p}- \\overline{x}_{\\bullet p}}{s_p}\\\\[2ex] \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ \\frac{x_{n 1} - \\overline{x}_{\\bullet 1}}{s_1}&amp; \\frac{x_{n 2}- \\overline{x}_{\\bullet 2}}{s_2} &amp;\\ldots &amp; \\frac{x_{n p}- \\overline{x}_{\\bullet p}}{s_p} \\end{pmatrix}. \\] Ejemplo 8.3 Vamos a tipificar a mano la tabla de datos \\[ {X}=\\begin{pmatrix} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{pmatrix} \\] del Ejemplo 8.1. Ya la hemos centrado en el Ejemplo 8.2. Para tipificarla, tenemos que dividir cada columna de esta matriz centrada por la desviación típica de la columna correspondiente en la matriz original. Hemos calculado estas desviaciones típicas en el Ejemplo 8.1: sapply(X, sd_ver) ## V1 V2 V3 ## 0.829156 1.500000 1.299038 Dividiendo cada columna de la matriz centrada \\(\\widetilde{X}\\) por la correspondiente desviación típica obtenemos: \\[ \\begin{array}{rl} Z &amp; =\\begin{pmatrix} -0.75/0.829156&amp;-1.5/1.5&amp;1.25/1.299038\\\\ -0.75/0.829156&amp;-0.5/1.5&amp;1.25/1.299038\\\\ 0.25/0.829156&amp;2.5/1.5&amp;-1.75/1.299038\\\\ 1.25/0.829156&amp;-0.5/1.5&amp;-0.75/1.299038 \\end{pmatrix}\\\\ &amp; = \\begin{pmatrix} -0.9045340&amp;-1.0000000&amp;0.96225\\\\ -0.9045340&amp;-0.333333&amp;0.96225\\\\ 0.301511&amp;1.666667&amp;-1.347151\\\\ 1.507557&amp;-0.333333&amp;-0.57735 \\end{pmatrix} \\end{array} \\] La manera más sencilla de aplicar con R una transformación lineal a una tabla de datos \\(X\\), y en particular de centrarla o tipificarla, es usando la instrucción scale(X, center=..., scale=...) donde: X puede ser tanto una matriz como un data frame; el resultado será siempre una matriz. El valor del parámetro center es el vector que restamos a sus columnas, en el sentido de que cada entrada de este vector se restará a todas las entradas de la columna correspondiente. Su valor por defecto (que no es necesario especificar, aunque también se puede especificar con center=TRUE) es el vector \\(\\overline{X}\\) de medias de \\(X\\); para especificar que no se reste nada, podemos usarcenter=FALSE. El valor del parámetro scale es el vector por el que dividimos las columnas de \\(X\\):cada columna se divide por la entrada correspondiente de este vector.Su valor por defecto (de nuevo, se puede especificar igualando el parámetro a TRUE) es el vector \\(\\widetilde{s}_X\\) de desviaciones típicas muestrales; para especificar que no se divida por nada, podemos usar scale=FALSE. En particular, la instrucción scale(X) centra la tabla de datos \\(X\\) y divide sus columnas por sus desviaciones típicas muestrales; por lo tanto, no la tipifica según nuestra definición, ya que no las divide por sus desviaciones típicas “verdaderas”. Ejemplo 8.4 Recordemos la tabla de datos \\(X\\) del Ejemplo 8.1. X ## V1 V2 V3 ## 1 1 -1 3 ## 2 1 0 3 ## 3 2 3 0 ## 4 3 0 1 Su matriz centrada es: X_centrada=scale(X, center=TRUE, scale=FALSE) X_centrada ## V1 V2 V3 ## [1,] -0.75 -1.5 1.25 ## [2,] -0.75 -0.5 1.25 ## [3,] 0.25 2.5 -1.75 ## [4,] 1.25 -0.5 -0.75 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 Coincide con la matriz obtenida en el Ejemplo 8.2. Observad la estructura del resultado: en primer lugar nos da la matriz centrada, y a continuación nos dice que tiene un atributo llamado &quot;scaled:center&quot; cuyo valor es el vector usado para centrarla. Este atributo no interferirá para nada en las operaciones que realicéis con la matriz centrada, pero, si os molesta, recordad que se puede eliminar sustituyendo el resultado de centrar la matriz en los puntos suspensivos de la instrucción siguiente: attr(... , &quot;scaled:center&quot;)=NULL En nuestro ejemplo: attr(X_centrada, &quot;scaled:center&quot;)=NULL X_centrada ## V1 V2 V3 ## [1,] -0.75 -1.5 1.25 ## [2,] -0.75 -0.5 1.25 ## [3,] 0.25 2.5 -1.75 ## [4,] 1.25 -0.5 -0.75 Como ya hemos avisado, para tipificar esta tabla de datos no podemos hacer lo siguiente: X_tip=scale(X) X_tip ## V1 V2 V3 ## [1,] -0.783349 -0.866025 0.833333 ## [2,] -0.783349 -0.288675 0.833333 ## [3,] 0.261116 1.443376 -1.166667 ## [4,] 1.305582 -0.288675 -0.500000 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 ## attr(,&quot;scaled:scale&quot;) ## V1 V2 V3 ## 0.957427 1.732051 1.500000 Para hacerlo bien según la definición que hemos dado, tenemos dos opciones. Una es multiplicar la matriz anterior por \\(\\sqrt{n/(n-1)}\\), donde \\(n\\) es el número de filas de la tabla. (El motivo es que, como \\(\\widetilde{s}_X=\\sqrt{\\frac{n}{n-1}}\\cdot s_X\\), se tiene que \\(\\frac{1}{s_X}=\\sqrt{\\frac{n}{n-1}}\\cdot \\frac{1}{\\widetilde{s}_X}\\); por lo tanto, si queríamos dividir por \\(s_X\\) y scale(X) ha dividido por \\(\\widetilde{s}_X\\), basta multiplicar su resultado por \\(\\sqrt{\\frac{n}{n-1}}\\) para obtener el efecto deseado.) n=dim(X)[1] #Número de filas de X X_tip=scale(X)*sqrt(n/(n-1)) X_tip ## V1 V2 V3 ## [1,] -0.904534 -1.000000 0.96225 ## [2,] -0.904534 -0.333333 0.96225 ## [3,] 0.301511 1.666667 -1.34715 ## [4,] 1.507557 -0.333333 -0.57735 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 ## attr(,&quot;scaled:scale&quot;) ## V1 V2 V3 ## 0.957427 1.732051 1.500000 Ahora sí que coincide con la matriz obtenida “a mano” en el Ejemplo 8.3. Otra posibilidad es usar, como valor del parámetro scale, el vector \\(s_X\\) de desviaciones típicas de las columnas. X_tip1=scale(X, scale=sapply(X, sd_ver)) X_tip1 ## V1 V2 V3 ## [1,] -0.904534 -1.000000 0.96225 ## [2,] -0.904534 -0.333333 0.96225 ## [3,] 0.301511 1.666667 -1.34715 ## [4,] 1.507557 -0.333333 -0.57735 ## attr(,&quot;scaled:center&quot;) ## V1 V2 V3 ## 1.75 0.50 1.75 ## attr(,&quot;scaled:scale&quot;) ## V1 V2 V3 ## 0.829156 1.500000 1.299038 Observaréis que la matriz resultante es la misma, pero el atributo que indica el vector por el que hemos dividido las columnas es diferente: en este caso, es el de desviaciones típicas. Ahora, en ambos casos, podemos usar la función attr para eliminar los dos atributos, &quot;scaled:center&quot; y &quot;scaled:scale&quot;, que se han añadido a la matriz tipificada. Por ejemplo: attr(X_tip, &quot;scaled:center&quot;)=NULL attr(X_tip, &quot;scaled:scale&quot;)=NULL X_tip ## V1 V2 V3 ## [1,] -0.904534 -1.000000 0.96225 ## [2,] -0.904534 -0.333333 0.96225 ## [3,] 0.301511 1.666667 -1.34715 ## [4,] 1.507557 -0.333333 -0.57735 8.3 Covarianzas y correlaciones La covarianza entre dos variables es una medida de la tendencia que tienen ambas variables a variar conjuntamente. Cuando la covarianza es positiva, si una de las dos variables crece o decrece, la otra tiene el mismo comportamiento; en cambio, cuando la covarianza es negativa, esta tendencia se invierte: si una variable crece, la otra decrece y viceversa. Puesto que interpretar el valor de la covarianza más allá de su signo es difícil, se suele usar una versión “normalizada” de la misma, la correlación de Pearson, que mide de manera más precisa la relación lineal entre dos variables. La covarianza generaliza la varianza, en el sentido de que la varianza de una variable es su covarianza consigo misma. Y como en el caso de la varianza, definiremos dos versiones de la covarianza: la “verdadera” y la muestral. La diferencia estará de nuevo en el denominador. Formalmente, la covarianza de las variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) de una matriz de datos \\(X\\) es \\[ s_{i j}=\\frac{1}{n} \\sum_{k =1}^n\\big((x_{k i}-\\overline{{x}}_{\\bullet i})(x_{kj}-\\overline{{x}}_{\\bullet j})\\big)= \\frac{1}{n} \\Big(\\sum_{k =1}^n x_{k i} x_{k j}\\Big) - \\overline{{x}}_{\\bullet i} \\overline{{x}}_{\\bullet j}, \\] y su covarianza muestral es \\[ \\widetilde{s}_{ij} = \\frac{1}{n-1} \\sum_{k =1}^n\\big((x_{k i}-\\overline{{x}}_{\\bullet i})(x_{kj}-\\overline{{x}}_{\\bullet j})\\big)= \\frac{n}{n-1} s_{ij}. \\] El estadístico \\(\\tilde{s}_{ij}\\) es siempre un estimador insesgado de la covarianza \\(\\sigma_{i j}\\) de las variables aleatorias \\(X_i\\) y \\(X_j\\) de las que \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) son muestras, mientras que \\(s_{i j}\\) es su estimador máximo verosímil cuando la distribución conjunta de \\(X_i\\) y \\(X_j\\) es normal bivariante. Es inmediato comprobar a partir de sus definiciones que ambas covarianzas son simétricas, y que la covarianza de una variable consigo misma es su varianza: \\[ s_{i j}= s_{j i}, \\quad \\widetilde{s}_{i j}= \\widetilde{s}_{j i}, \\quad s_{i i}=s_{i}^2, \\quad \\widetilde{s}_{ii}=\\widetilde{s}_i^2. \\] Ejemplo 8.5 La covarianza de las dos primeras columnas de la matriz de datos \\[ X=\\begin{pmatrix} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{pmatrix} \\] del Ejemplo 8.1 se calcularía de la manera siguiente: \\[ s_{12}=\\frac{1}{4}(1\\cdot (-1)+1\\cdot 0+2\\cdot 3+3\\cdot 0)-1.75\\cdot 0.5= 1.25-0.875=0.375 \\] Su covarianza muestral se obtendría multiplicando por \\(4/3\\) este valor: \\[ \\widetilde{s}_{12} = \\frac{4}{3} s_{12}=0.5. \\] La covarianza muestral de dos vectores numéricos de la misma longitud \\(n\\) se puede calcular con R mediante la función cov. Para obtener su covarianza “verdadera”, hay que multiplicar el resultado de cov por \\((n-1)/n\\). Ejemplo 8.6 La covarianza muestral de las dos primeras columnas de la tabla de datos \\(X\\), que tenemos guardada en el data frame X, es: cov(X$V1, X$V2) ## [1] 0.5 y su covarianza “verdadera” es: n=dim(X)[1] ((n-1)/n)*cov(X$V1, X$V2) ## [1] 0.375 Queremos recalcar que, como en el caso de la varianza con var, R calcula con cov la versión muestral de la covarianza. Las matrices de covarianzas y de covarianzas muestrales de una tabla de datos \\(X\\) son, respectivamente, \\[ {S}= \\begin{pmatrix} s_{1 1} &amp; s_{1 2} &amp; \\ldots &amp; s_{1 p}\\\\ s_{2 1} &amp; s_{2 2} &amp; \\ldots &amp; s_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ s_{p 1} &amp; s_{p 2} &amp; \\ldots &amp; s_{p p} \\end{pmatrix},\\ \\widetilde{{S}}= \\begin{pmatrix} \\widetilde{s}_{1 1} &amp; \\widetilde{s}_{1 2} &amp; \\ldots &amp; \\widetilde{s}_{1 p}\\\\ \\widetilde{s}_{2 1} &amp; \\widetilde{s}_{2 2} &amp; \\ldots &amp; \\widetilde{s}_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\widetilde{s}_{p 1} &amp; \\widetilde{s}_{p 2} &amp; \\ldots &amp; \\widetilde{s}_{p p} \\end{pmatrix}, \\] donde cada \\(s_{i j}\\) y cada \\(\\widetilde{s}_{i j}\\) son, respectivamente, la covarianza y la covarianza muestral de las correspondientes columnas \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\). Estas matrices de covarianzas miden la tendencia a la variabilidad conjunta de los datos de \\(X\\) y, si \\(n\\) es el número de filas de \\(X\\), se tiene que \\[ S=\\frac{n-1}{n}\\widetilde{{S}}. \\] La matriz de covarianzas muestrales \\(\\widetilde{{S}}\\) es un estimador insesgado de la matriz de covarianzas \\(\\Sigma\\) del vector de variables aleatorias \\(\\underline{X}\\), y si este tiene distribución normal multivariante, \\(S\\) es un estimador máximo verosímil de \\(\\Sigma\\). Ambas matrices de covarianzas son simétricas, puesto que \\(s_{i j}=s_{j i}\\), y tienen todos sus valores propios \\(\\geq 0\\). La matriz de covarianzas muestrales de una tabla de datos se calcula aplicando la función cov al data frame o a la matriz que contenga dicha tabla. Para obtener su matriz de covarianzas “verdaderas”, es suficiente multiplicar el resultado de cov por \\((n-1)/n\\), donde \\(n\\) es el número de filas de la tabla de datos. Ejemplo 8.7 Continuemos con la matriz \\[ X=\\begin{pmatrix} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{pmatrix} \\] del Ejemplo 8.1. Su matriz de covarianzas muestrales es cov(X) ## V1 V2 V3 ## V1 0.916667 0.50000 -1.08333 ## V2 0.500000 3.00000 -2.16667 ## V3 -1.083333 -2.16667 2.25000 y su matriz de covarianzas es n=dim(X)[1] ((n-1)/n)*cov(X) ## V1 V2 V3 ## V1 0.6875 0.375 -0.8125 ## V2 0.3750 2.250 -1.6250 ## V3 -0.8125 -1.625 1.6875 Como la matriz de covarianzas es difícil de interpretar como medida de variabilidad de una tabla de datos, debido a que no es una única cantidad sino toda una matriz, interesa cuantificar esta variabilidad mediante un único índice. No hay consenso sobre este índice, y entre los que se han propuesto destacamos: La varianza total de \\(X\\): la suma de las varianzas de sus columnas. La varianza media de \\(X\\): la media de las varianzas de sus columnas, es decir, la varianza total partida por el número de columnas. La varianza generalizada de \\(X\\): el determinante de su matriz de covarianzas. La desviación típica generalizada de \\(X\\): la raíz cuadrada positiva de su varianza generalizada. Pasemos ahora a la correlación lineal de Pearson (o, de ahora en adelante, simplemente correlación de Pearson) de dos variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) de \\(X\\), que se define como \\[ r_{i j}=\\frac{s_{i j}}{s_i\\cdot s_j}. \\] Observad que \\[ \\frac{\\widetilde{s}_{i j}}{\\widetilde{s}_i\\cdot \\widetilde{s}_j}= \\frac{\\frac{n}{n-1}\\cdot {s}_{i j}}{\\sqrt{\\frac{n}{n-1}}\\cdot {s}_i \\cdot\\sqrt{\\frac{n}{n-1}}\\cdot{s}_j}= \\frac{s_{i j}}{s_i \\cdot s_j}=r_{i j}, \\] y, por lo tanto, esta correlación se puede calcular también a partir de las versiones muestrales de la covarianza y las desviaciones típicas por medio de la misma fórmula. El estadístico \\(r_{ij}\\) es un estimador máximo verosímil de la correlación de Pearson \\(\\rho_{i j}=Cor(X_i,X_j)\\) de las variables aleatorias \\(X_i\\) y \\(X_j\\) cuando su distribución conjunta es normal bivariante, y aunque es sesgado, su sesgo tiende a 0 cuando \\(n\\) tiende a \\(\\infty\\). Las propiedades más importantes de \\(r_{i,j}\\) son las siguientes: Es simétrica: \\(r_{i j}=r_{j i}\\). \\(-1\\leq r_{i j}\\leq 1\\). \\(r_{i i}=1\\). \\(r_{i j}\\) tiene el mismo signo que \\(s_{i j}\\). \\(r_{i j}=\\pm 1\\) si y, sólo si, existe una relación lineal perfecta entre las variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\): es decir, si, y sólo si, existen valores \\(a, b\\in \\mathbb{R}\\) tales que \\[ \\left(\\begin{array}{c} x_{1j}\\\\ \\vdots \\\\ x_{nj}\\end{array}\\right)= a\\cdot \\left(\\begin{array}{c} x_{1i}\\\\ \\vdots \\\\ x_{ni}\\end{array}\\right) +b. \\] La pendiente \\(a\\) de esta relación lineal tiene el mismo signo que \\(r_{i j}\\). El coeficiente de determinación \\(R^2\\) de la regresión lineal por mínimos cuadrados de \\({x}_{\\bullet j}\\) respecto de \\({x}_{\\bullet i}\\) es igual al cuadrado de su correlación de Pearson, \\(r_{i j}^2\\); por lo tanto, cuánto más se aproxime el valor absoluto de \\(r_{ij}\\) a 1, más se acercan las variables \\({x}_{\\bullet i}\\) y \\({x}_{\\bullet j}\\) a depender linealmente la una de la otra. Así pues, la correlación de Pearson entre dos variables viene a ser una covarianza “normalizada”, ya que, como vemos, su valor está entre -1 y 1, y mide la tendencia de las variables a estar relacionadas según una función lineal. En concreto, cuanto más se acerca dicha correlación a 1 (respectivamente, a -1), más se acerca una (cualquiera) de las variables a ser función lineal creciente (respectivamente, decreciente) de la otra. Con R, la correlación de Pearson de dos vectores se puede calcular aplicándoles la función cor. Ejemplo 8.8 En ejemplos anteriores hemos calculado la covarianza y las varianzas de las dos primeras columnas de la matriz de datos \\[ {X}=\\begin{pmatrix} 1&amp;-1&amp;3\\\\ 1&amp;0&amp;3\\\\ 2&amp;3&amp;0\\\\ 3&amp;0&amp;1 \\end{pmatrix} \\] Hemos obtenido los valores siguientes \\[ s_{12}=0.375,\\quad s_1=0.829156,\\quad s_2=1.5. \\] Por lo tanto, su correlación de Pearson es \\[ r_{1 2}=\\frac{0.375}{0.829156\\cdot 1.5}=0.301511. \\] Ahora vamos a calcularla con R, y aprovecharemos para confirmar su relación con el valor de \\(R^2\\) de la regresión lineal de la segunda columna respecto de la primera. Recordemos que esta tabla de datos sigue guardada en el data frame X X ## V1 V2 V3 ## 1 1 -1 3 ## 2 1 0 3 ## 3 2 3 0 ## 4 3 0 1 La correlación de sus dos primeras columnas es: cor(X$V1, X$V2) ## [1] 0.301511 que coincide con el valor obtenido “a mano”. Comprobemos ahora que su cuadrado es igual al valor de \\(R^2\\) de la regresión lineal: cor(X$V1, X$V2)^2 ## [1] 0.0909091 summary(lm(X$V2~X$V1))$r.squared ## [1] 0.0909091 La matriz de correlaciones de Pearson de \\(X\\) es \\[ {R}= \\begin{pmatrix} 1 &amp; r_{1 2} &amp; \\ldots &amp; r_{1 p}\\\\ r_{2 1} &amp; 1 &amp; \\ldots &amp; r_{2 p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ r_{p 1} &amp; r_{p 2} &amp; \\ldots &amp; 1 \\end{pmatrix} \\] donde cada \\(r_{i j}\\) es la correlación de Pearson de las columnas correspondientes de \\(X\\). Esta matriz de correlaciones tiene siempre determinante \\(|R|\\leq 1\\) y todos sus valores propios son \\(\\geq 0\\), y con R se puede calcular aplicando la misma instrucción cor a la tabla de datos, sea en forma de matriz o de data frame. Así, la matriz de correlaciones de nuestra tabla de datos \\(X\\) es: cor(X) ## V1 V2 V3 ## V1 1.000000 0.301511 -0.754337 ## V2 0.301511 1.000000 -0.833950 ## V3 -0.754337 -0.833950 1.000000 Se tiene el teorema siguiente, que se puede demostrar mediante un simple, aunque farragoso, cálculo algebraico: Teorema 8.1 La matriz de correlaciones de Pearson de \\(X\\) es igual a: La matriz de covarianzas de su matriz tipificada. La matriz de covarianzas muestrales de su matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales en vez de por las “verdaderas”. La importancia de este resultado es que, si la tabla de datos es muy grande, suele ser más eficiente calcular la matriz de covarianzas de su matriz tipificada que la matriz de correlaciones de Pearson de la tabla original. Observad, por otro lado, que las dos matrices de covarianzas mencionadas en el enunciado coinciden, puesto que la matriz tipificada se obtiene multiplicando por \\(\\sqrt{n/(n-1)}\\) la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Esto implica que la matriz de covarianzas muestrales de la matriz tipificada se obtiene multiplicando por \\(n/(n-1)\\) la matriz de covarianzas muestrales de la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Finalmente, la matriz de covarianzas se obtiene a partir de la de covarianzas muestrales multiplicándola por \\((n-1)/n\\). Entonces, los factores \\(n/(n-1)\\) y \\((n-1)/n\\) se compensan y resulta que la matriz de covarianzas de la matriz tipificada coincide con la matriz de covarianzas muestrales de la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Recordemos que si aplicamos la función scale a una tabla de datos \\(X\\), la tipifica dividiendo por las desviaciones típicas muestrales. Por lo tanto, otra manera de reformular el teorema anterior es decir que cor(X) da lo mismo que cov(scale(X)). Comprobemos esta igualdad para nuestra matriz de datos \\(X\\). cor(X) ## V1 V2 V3 ## V1 1.000000 0.301511 -0.754337 ## V2 0.301511 1.000000 -0.833950 ## V3 -0.754337 -0.833950 1.000000 cov(scale(X)) ## V1 V2 V3 ## V1 1.000000 0.301511 -0.754337 ## V2 0.301511 1.000000 -0.833950 ## V3 -0.754337 -0.833950 1.000000 Cuando se calcula la covarianza o la correlación de Pearson de dos vectores que contienen valores NA, lo usual es no tenerlos en cuenta: es decir, si un vector contiene un NA en una posición, se eliminan de los dos vectores sus entradas en dicha posición. De esta manera, se tomaría como covarianza de \\[ \\left(\\begin{array}{c} 1\\\\ 2\\\\ NA\\\\ 4\\\\ 6\\\\ 2\\end{array}\\right)\\mbox{ y } \\left(\\begin{array}{c} 2\\\\ 4\\\\ -3\\\\ 5\\\\ 7\\\\ NA \\end{array}\\right) \\] la de \\[ \\left(\\begin{array}{c}1\\\\ 2\\\\ 4\\\\ 6\\end{array}\\right)\\mbox{ y } \\left(\\begin{array}{c} 2\\\\ 4\\\\ 5\\\\ 7\\end{array}\\right). \\] Como ya nos pasaba con las funciones de estadística descriptiva univariante com mean o var, cuando aplicamos cov o cor a un par de vectores que contengan entradas NA, obtenemos por defecto NA. En las funciones univariantes usábamos na.rm=TRUE para pedir a R que obviara los NA, pero esta solución ahora no es posible, porque las posiciones de los NA también cuentan, y si los borramos tal cual se desmonta el emparejamiento de los datos. Así que, si se quiere que R calcule el valor de cov o cor sin tener en cuenta los NA, se ha de especificar añadiendo el parámetro use=&quot;complete.obs&quot;, que le indica que ha de usar las observaciones completas, es decir, las posiciones que no tienen NA en ninguno de los dos vectores. Veamos el efecto sobre los dos vectores anteriores. Llamémosles \\(x\\) e \\(y\\), y sean \\(x_1\\) e \\(y_1\\) los vectores que se obtienen eliminando las entradas que contienen un NA en alguno de los dos vectores. x=c(1,2,NA,4,6,2) y=c(2,4,-3,5,7,NA) x1=x[is.na(x)!=TRUE &amp; is.na(y)!=TRUE] y1=y[is.na(x)!=TRUE &amp; is.na(y)!=TRUE] x1 ## [1] 1 2 4 6 y1 ## [1] 2 4 5 7 Si calculamos la covarianza de \\(x\\) e \\(y\\) tal cual con la función cov, da NA: cov(x, y) ## [1] NA Usando use=&quot;complete.obs&quot;, obtenemos la covarianza de \\(x_1\\) e \\(y_1\\): cov(x, y, use=&quot;complete.obs&quot;) ## [1] 4.5 cov(x1, y1) ## [1] 4.5 Lo mismo sucede con la función cor: cor(x, y) ## [1] NA cor(x, y, use=&quot;complete.obs&quot;) ## [1] 0.974913 cor(x1, y1) ## [1] 0.974913 Al calcular las matrices de covarianzas o correlaciones de una tabla de datos que contenga valores NA, se suele seguir una de las dos estrategias siguientes, según lo que interese al usuario: Para cada par de columnas, se calcula su covarianza o su correlación con la estrategia explicada más arriba para dos vectores, obviando el hecho de que forman parte de una tabla de datos mayor; es decir, al efectuar el cálculo para cada par de columnas concreto, se eliminan de cada una de ellas sus entradas NA y aquellas en cuya fila la otra tiene un NA. Esta opción se especifica dentro de la función cov o cor con el parámetro use=&quot;pairwise.complete.obs&quot;. Antes de nada, se eliminan las filas de la tabla que contienen algún NA en alguna columna, dejando solo en la tabla las filas “completas”, las que no contienen ningún NA. Luego se calcula la matriz de covarianzas o de correlaciones de la tabla resultante. Esta opción se especifica con el parámetro use=&quot;complete.obs&quot;. Veamos un ejemplo. Consideremos la matriz de datos \\(Y\\) siguiente, cuyas dos primeras columnas son los vectores \\(x\\) e \\(y\\) anteriores: Y=cbind(c(1,2,NA,4,6,2), c(2,4,-3,5,7,NA), c(-2,1,0,2,NA,0)) Y ## [,1] [,2] [,3] ## [1,] 1 2 -2 ## [2,] 2 4 1 ## [3,] NA -3 0 ## [4,] 4 5 2 ## [5,] 6 7 NA ## [6,] 2 NA 0 Supongamos que queremos calcular su matriz de correlaciones de Pearson. Como todas las filas de \\(Y\\) tienen entradas NA, todas las correlaciones fuera de la diagonal dan NA (R sabe que la correlación de un columna consigo misma siempre es 1, y ya no la calcula): cor(Y) ## [,1] [,2] [,3] ## [1,] 1 NA NA ## [2,] NA 1 NA ## [3,] NA NA 1 Una opción es calcular las correlaciones de Pearson de cada par de variables eliminando sus valores NA pero sin tener en cuenta los posibles valores NA de la otra variable: cor(Y, use=&quot;pairwise.complete.obs&quot;) ## [,1] [,2] [,3] ## [1,] 1.000000 0.974913 0.891902 ## [2,] 0.974913 1.000000 0.438727 ## [3,] 0.891902 0.438727 1.000000 Observad que la entrada (1,2) de esta matriz es la correlación de los vectores \\(x\\) e \\(y\\) calculada con use=&quot;complete.obs&quot;. Calculemos ahora la matriz de correlaciones de Pearson de la matriz con filas completas: cor(Y, use=&quot;complete.obs&quot;) ## [,1] [,2] [,3] ## [1,] 1.000000 0.928571 0.891042 ## [2,] 0.928571 1.000000 0.995871 ## [3,] 0.891042 0.995871 1.000000 Veamos que efectivamente coincide con la matriz de correlaciones de Pearson de la matriz que se obtiene eliminando las filas que contienen algún NA. Esta matriz es: noNAs=is.na(Y[,1])!=TRUE &amp; is.na(Y[,2])!=TRUE &amp; is.na(Y[,3])!=TRUE Y1=Y[noNAs,] Y1 ## [,1] [,2] [,3] ## [1,] 1 2 -2 ## [2,] 2 4 1 ## [3,] 4 5 2 y su matriz de correlaciones de Pearson es: cor(Y1) ## [,1] [,2] [,3] ## [1,] 1.000000 0.928571 0.891042 ## [2,] 0.928571 1.000000 0.995871 ## [3,] 0.891042 0.995871 1.000000 8.4 Correlación de Spearman La correlación de Pearson mide específicamente la tendencia de dos variables cuantitativas continuas a depender linealmente una de otra. En circunstancias en las que no esperemos esta dependencia lineal, o en las que nuestras variables sean cuantitativas discretas o simplemente cualitativas, usar la correlación de Pearson para analizar la relación entre dos variables no es lo más adecuado. Entre las propuestas alternativas, la más popular es la correlación de Spearman. Este índice asigna a cada valor de cada vector su rango (su posición en el vector ordenado de menor a mayor, y en caso de empates la media de las posiciones que ocuparían todos los empates) y calcula la correlación de Pearson de estos rangos. Con R, la correlación de Spearman se calcula directamente con la función cor entrándole el parámetro method=&quot;spearman&quot;. (El valor por defecto del parámetro method es &quot;pearson&quot; y por eso no lo indicamos cuando calculamos la correlación de Pearson.) Ejemplo 8.9 Vamos a calcular la correlación de Spearman de las dos primeras columnas de la matriz de datos \\(X\\) que hemos venido usando en nuestros ejemplos. En la tabla siguiente calculamos los rangos de sus entradas: \\[ \\begin{array}{|c|c|c|c|} \\hline {x}_{\\bullet 1}&amp; rango &amp; {x}_{\\bullet 2}&amp; rango \\\\\\hline\\hline 1&amp; 1.5 &amp; -1&amp; 1 \\\\ 1&amp;1.5 &amp; 0 &amp; 2.5\\\\ 2&amp;3 &amp; 3&amp; 4 \\\\ 3&amp;4 &amp; 0&amp; 2.5\\\\\\hline \\end{array} \\] ¿Cómo hemos obtenido los rangos? Fijaos por ejemplo en la primera columna: los dos 1 ocuparían la posición 1 y 2, les asignamos a ambos como rango la media de estas posiciones, 1.5; el 2 ocuparía la posición 3 y el 3 ocuparía la posición 4, y estos son también sus rangos. Con R estos rangos se calculan con la función rank. Así, los rangos de los elementos de \\(x_{\\bullet 1}\\) son rank(X$V1) ## [1] 1.5 1.5 3.0 4.0 y los de los elementos de \\(x_{\\bullet 2}\\) son rank(X$V2) ## [1] 1.0 2.5 4.0 2.5 Por lo tanto, la correlación de Spearman de \\[ (1,1,2,3)\\mbox{ y }(-1,0,3,0) \\] es la correlación de Pearson de \\[ (1.5, 1.5, 3, 4)\\mbox{ y }(1, 2.5, 4, 2.5) \\] Veámoslo: cor(X$V1,X$V2,method=&quot;spearman&quot;) ## [1] 0.5 cor(rank(X$V1),rank(X$V2)) ## [1] 0.5 8.5 Contrastes de correlación Como ya hemos comentado, podemos usar la correlación de Pearson \\(r_{xy}\\) de dos vectores \\(x\\) e \\(y\\), formados por los valores de dos variables cuantitativas \\(X,Y\\) medidos sobre una misma muestra de individuos, para estimar la correlación \\(\\rho_{XY}\\) de estas variables poblacionales. Cuando además ambas variables aleatorias son normales, disponemos de una fórmula para calcular intervalos de confianza para la correlación poblacional y de un método para efectuar contrastes de hipótesis con hipótesis nula \\(H_0: \\rho_{XY}=0\\) (“no hay correlación entre \\(X\\) e \\(Y\\)”). No vamos a entrar en los detalles de las fórmulas ni de los teoremas en que se basan, pero es importante que recordéis que la función de R que lleva a cabo dichos contrastes “de correlación” es la función cor.test. En particular, esta función calcula el intervalo de confianza asociado a un contraste de estos: si el contraste es bilateral, es decir, con hipótesis alternativa \\(H_1: \\rho_{XY}\\neq 0\\), el intervalo que produce esta función es el intervalo de confianza usual para \\(\\rho_{XY}\\) con nivel de confianza correspondiente al nivel de significación del contraste. La sintaxis de cor.test es la misma que la del resto de funciones para realizar contrastes de hipótesis básicos: cor.test(x, y, alternative=..., conf.level=...) donde x e y son los dos vectores de datos, que también se pueden especificar mediante una fórmula. Estos dos vectores han de tener la misma longitud, puesto que se entiende que son mediciones sobre el mismo conjunto de individuos. El parámetro alternative puede tomar los tres valores usuales y su valor por defecto es, como siempre, &quot;two.sided&quot;, que corresponde al contraste bilateral, con hipótesis alternativa \\(H_1: \\rho_{XY}\\neq 0\\). Los valores alternative=&quot;greater&quot; y alternative=&quot;less&quot; permiten contrastar si \\(X\\) e \\(Y\\) tienen correlación positiva o negativa, respectivamente. Como en el resto de funciones de contrastes, el resultado es una list que, entre otros objetos, contiene: p.value: El p-valor del test. conf.int: Un intervalo de confianza del nivel de confianza especificado. estimate: El valor de la correlación de Pearson (calculado con use=&quot;complete.obs&quot; si algún vector contiene valores NA). Ejemplo 8.10 Queremos contrastar si hay correlación positiva entre el peso de una madre en el momento de la concepción del hijo y el peso de su hijo en el momento de nacer. Para ello vamos a usar la tabla de datos birthwt incluida en el paquete MASS que ya usamos en una lección anterior, que contiene información sobre recién nacidos y sus madres, y que en particular dispone de las variables bwt, que da el peso del recién nacido en gramos, y lwt, que da el peso de la madre en libras en el momento de su última menstruación. Vamos a suponer que ambos pesos siguen distribuciones normales. Si denotamos por \\(X\\) e \\(Y\\) las correspondientes variables poblacionales, queremos realizar el contraste \\[ \\left\\{\\begin{array}{l} H_0: \\rho_{XY}=0\\\\ H_1: \\rho_{XY}&gt;0 \\end{array}\\right. \\] Vamos a usar la función cor.test. library(MASS) cor.test(birthwt$bwt, birthwt$lwt, alternative=&quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: birthwt$bwt and birthwt$lwt ## t = 2.585, df = 187, p-value = 0.00525 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.0672064 1.0000000 ## sample estimates: ## cor ## 0.185733 El p-valor 0.005 nos da evidencia estadísticamente significativa de que, en efecto, hay una correlación positiva entre el peso de la madre y el peso del recién nacido. Lo podríamos haber obtenido directamente con cor.test(birthwt$bwt, birthwt$lwt, alternative=&quot;greater&quot;)$p.value ## [1] 0.00525209 El último valor, el 0.185733 bajo el cor, es la correlación de Pearson de los dos vectores de pesos, y el 95 percent confidence interval es el intervalo de confianza del 95% del contraste unilateral planteado y nos dice que tenemos un 95% de confianza en que la correlación entre el peso de la madre y el peso del recién nacido es superior a 0.067. Si hubiéramos querido calcular un intervalo de confianza del 95% para \\(\\rho_{XY}\\) que repartiera por igual a ambos lados el 5% de probabilidad de no contener su valor real, hubiéramos podido usar el intervalo de confianza del contraste bilateral: cor.test(birthwt$bwt, birthwt$lwt)$conf.int ## [1] 0.044174 0.319981 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 La potencia de un contraste de correlación se calcula con la función pwr.r.test del paquete pwr. En este caso, el tamaño del efecto es simplemente la correlación de Pearson, que se entra en la función mediante el parámetro r. Apliquémosla para calcular la potencia del contraste de correlación anterior: library(pwr) dim(birthwt) ## [1] 189 10 round(cor(birthwt$bwt,birthwt$lwt),4) ## [1] 0.1857 pwr.r.test(n=189,r=0.1857,sig.level=0.05,alternative=&quot;greater&quot;) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 189 ## r = 0.1857 ## sig.level = 0.05 ## power = 0.822373 ## alternative = greater La probabilidad de error de tipo II en este contraste era de un poco menos del 18%. Si quisiéramos realizar este contraste de correlación con una potencia del 90% suponiendo que la magnitud del efecto va ser pequeña, usaríamos primero cohen.ES con test=&quot;r&quot; para determinar qué magnitud del efecto se considera pequeña y a continuación pwr.r.test dejando sin especificar la n: cohen.ES(test=&quot;r&quot;,size=&quot;small&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = r ## size = small ## effect.size = 0.1 pwr.r.test(power=0.9,r=0.1,sig.level=0.05,alternative=&quot;greater&quot;) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 852.647 ## r = 0.1 ## sig.level = 0.05 ## power = 0.9 ## alternative = greater Hubiéramos necesitado datos de al menos 853 recién nacidos. 8.6 Un ejemplo Recordaréis el data frame iris, que tabulaba las longitudes y anchuras de los pétalos y los sépalos de una muestra de flores iris de tres especies. Vamos a extraer una subtabla con sus cuatro variables numéricas, que llamaremos iris_num, y calcularemos sus matrices de covarianzas y correlaciones. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... iris_num=iris[, 1:4] n=dim(iris_num)[1] #Número de filas Su matriz de covarianzas muestrales es: cov(iris_num) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.685694 -0.042434 1.274315 0.516271 ## Sepal.Width -0.042434 0.189979 -0.329656 -0.121639 ## Petal.Length 1.274315 -0.329656 3.116278 1.295609 ## Petal.Width 0.516271 -0.121639 1.295609 0.581006 Su matriz de covarianzas “verdaderas” es: cov(iris_num)*(n-1)/n ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.6811222 -0.0421511 1.265820 0.512829 ## Sepal.Width -0.0421511 0.1887129 -0.327459 -0.120828 ## Petal.Length 1.2658200 -0.3274587 3.095503 1.286972 ## Petal.Width 0.5128289 -0.1208284 1.286972 0.577133 Su matriz de correlaciones de Pearson es: cor(iris_num) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.000000 -0.117570 0.871754 0.817941 ## Sepal.Width -0.117570 1.000000 -0.428440 -0.366126 ## Petal.Length 0.871754 -0.428440 1.000000 0.962865 ## Petal.Width 0.817941 -0.366126 0.962865 1.000000 Observamos, por ejemplo, una gran correlación de Pearson positiva entre la longitud y la anchura de los pétalos, 0.963, lo que indica una estrecha relación lineal con pendiente positiva entre estas magnitudes. Valdría la pena, entonces, calcular la recta de regresión lineal de una de estas medidas en función de la otra. lm(Petal.Length~Petal.Width, data=iris_num) ## ## Call: ## lm(formula = Petal.Length ~ Petal.Width, data = iris_num) ## ## Coefficients: ## (Intercept) Petal.Width ## 1.08 2.23 En cambio, la correlación de Pearson entre la longitud y la anchura de los sépalos es -0.11757, muy cercana a cero, lo que es señal de que la variación conjunta de las longitudes y anchuras de los sépalos no tiene una tendencia clara. Vamos a ordenar ahora los pares de variables numéricas de iris en orden decreciente de su correlación en valor absoluto, para saber cuáles están más correlacionadas (en positivo o negativo). Para ello, en primer lugar creamos un data frame cuyas filas están formadas por pares diferentes de variables numéricas de iris, su correlación de Pearson y el valor absoluto de esta última, y a continuación ordenamos las filas de este data frame en orden decreciente de estos valores absolutos. Todo esto lo llevamos a cabo en el siguiente bloque de código, que luego explicamos: medidas=names(iris_num) n=length(medidas) #En este caso, n=4 indices=upper.tri(diag(n)) medida1=matrix(rep(medidas, times=n), nrow=n, byrow=FALSE)[indices] medida2=matrix(rep(medidas, times=n), nrow=n, byrow=TRUE)[indices] corrs=as.vector(cor(iris_num))[indices] corrs.abs=abs(corrs) corrs_df=data.frame(medida1, medida2, corrs, corrs.abs) corrs_df_sort=corrs_df[order(corrs_df$corrs.abs, decreasing=TRUE), ] corrs_df_sort ## medida1 medida2 corrs corrs.abs ## 6 Petal.Length Petal.Width 0.962865 0.962865 ## 2 Sepal.Length Petal.Length 0.871754 0.871754 ## 4 Sepal.Length Petal.Width 0.817941 0.817941 ## 3 Sepal.Width Petal.Length -0.428440 0.428440 ## 5 Sepal.Width Petal.Width -0.366126 0.366126 ## 1 Sepal.Length Sepal.Width -0.117570 0.117570 Vemos que el par de variables con mayor correlación de Pearson en valor absoluto son Petal.Length y Petal.Width, como ya habíamos observado, seguidos por Petal.Length y Sepal.Length. Vamos a explicar el código. La función upper.tri, aplicada a una matriz cuadrada \\(M\\), produce la matriz triangular superior de valores lógicos del mismo orden que \\(M\\), cuyas entradas \\((i,j)\\) con \\(i&lt;j\\) son todas TRUE y el resto todas FALSE. Existe una función similar, lower.tri, para producir matrices triangulares inferiores de valores lógicos. upper.tri(diag(4)) ## [,1] [,2] [,3] [,4] ## [1,] FALSE TRUE TRUE TRUE ## [2,] FALSE FALSE TRUE TRUE ## [3,] FALSE FALSE FALSE TRUE ## [4,] FALSE FALSE FALSE FALSE lower.tri(diag(4)) ## [,1] [,2] [,3] [,4] ## [1,] FALSE FALSE FALSE FALSE ## [2,] TRUE FALSE FALSE FALSE ## [3,] TRUE TRUE FALSE FALSE ## [4,] TRUE TRUE TRUE FALSE Ambas funciones disponen del parámetro diag que, igualado a TRUE, define también como TRUE las entradas de la diagonal principal. upper.tri(diag(4), diag=TRUE) ## [,1] [,2] [,3] [,4] ## [1,] TRUE TRUE TRUE TRUE ## [2,] FALSE TRUE TRUE TRUE ## [3,] FALSE FALSE TRUE TRUE ## [4,] FALSE FALSE FALSE TRUE Si \\(M\\) es una matriz y \\(L\\) es una matriz de valores lógicos del mismo orden, M[L] produce el vector construido de la manera siguiente: de cada columna, se queda sólo con las entradas de \\(M\\) cuya entrada correspondiente en \\(L\\) es TRUE, y a continuación concatena estas columnas, de izquierda a derecha, en un vector. Así, por ejemplo, tomemos la matriz \\(M\\) siguiente: M=matrix(1:16, nrow=4, byrow=T) M ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 El vector formado por las entradas de su triángulo superior, concatenadas por columnas, se obtiene de la manera siguiente: M[upper.tri(diag(4))] ## [1] 2 3 7 4 8 12 Ahora definimos las matrices siguientes, formadas por 4 copias (la primera por columnas, la segunda, por filas) del vector, al que hemos llamado medidas, de nombres de las variables numéricas de iris: matrix(rep(medidas, times=4), nrow=4, byrow=FALSE) ## [,1] [,2] [,3] [,4] ## [1,] &quot;Sepal.Length&quot; &quot;Sepal.Length&quot; &quot;Sepal.Length&quot; &quot;Sepal.Length&quot; ## [2,] &quot;Sepal.Width&quot; &quot;Sepal.Width&quot; &quot;Sepal.Width&quot; &quot;Sepal.Width&quot; ## [3,] &quot;Petal.Length&quot; &quot;Petal.Length&quot; &quot;Petal.Length&quot; &quot;Petal.Length&quot; ## [4,] &quot;Petal.Width&quot; &quot;Petal.Width&quot; &quot;Petal.Width&quot; &quot;Petal.Width&quot; matrix(rep(medidas, times=4), nrow=4, byrow=TRUE) ## [,1] [,2] [,3] [,4] ## [1,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [2,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [3,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [4,] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; Al aplicar estas matrices a la matriz de valores lógicos upper.tri(diag(4)) obtenemos los nombres de las variables correspondientes a las filas y las columnas del triángulo superior, respectivamente, y al aplicar la matriz de correlaciones a esta matriz de valores lógicos, obtenemos sus entradas en este triángulo; en los tres vectores, las entradas siguen el mismo orden. Esto nos permite construir el data frame corrs_df cuyas filas están formadas por pares diferentes de variables numéricas de iris, su correlación de Pearson (columna corrs) y, aplicando abs a esta última variable, dicha correlación en valor absoluto (columna corrs.abs). corrs_df ## medida1 medida2 corrs corrs.abs ## 1 Sepal.Length Sepal.Width -0.117570 0.117570 ## 2 Sepal.Length Petal.Length 0.871754 0.871754 ## 3 Sepal.Width Petal.Length -0.428440 0.428440 ## 4 Sepal.Length Petal.Width 0.817941 0.817941 ## 5 Sepal.Width Petal.Width -0.366126 0.366126 ## 6 Petal.Length Petal.Width 0.962865 0.962865 Finalmente, la función order ordena los valores del vector al que se aplica, en orden decreciente si se especifica el parámetro decreasing=TRUE. Cuando aplicamos un data frame a una de sus variables reordenada de esta manera, reordena sus filas según el orden de esta variable. En este caso hubiéramos conseguido lo mismo con la función sort, pero la función order se puede aplicar a más de una variable del data frame: esto permite ordenar las filas del data frame en el orden de la primera variable de manera que, en caso de empate, queden ordenadas por la segunda variable, y así sucesivamente. 8.7 Representación gráfica de datos multidimensionales La representación gráfica de tablas de datos multidimensionales tiene la dificultad de las dimensiones; para dos o tres variables es sencillo visualizar las relaciones entre las mismas, pero para más variables ya no nos bastan nuestras tres dimensiones espaciales y tenemos que usar algunos trucos, tales como representaciones gráficas conjuntas de pares de variables. La manera más sencilla de representar gráficamente una tabla de datos formada por dos variables numéricas es aplicando la función plot a la matriz de datos o al data frame. Esta función produce el diagrama de dispersión (scatter plot) de los datos: el gráfico de los puntos del plano definidos por las filas de la tabla. A modo de ejemplo, si extrajéramos de la tabla iris una subtabla conteniendo sólo las longitudes y anchuras de los pétalos y quisiéramos visualizar la relación entre estas dimensiones, podríamos dibujar su diagrama de dispersión con el código del bloque siguiente. El resultado es la Figura 8.1, que muestra una clara tendencia positiva: cuanto más largos son los pétalos, más anchos tienden a ser. Esto se corresponde con la correlación de Pearson de 0.963 que hemos obtenido en la sección anterior. iris.pet=iris[ ,c(&quot;Petal.Length&quot;,&quot;Petal.Width&quot;)] plot(iris.pet, pch=20, xlab=&quot;Largo&quot;, ylab=&quot;Ancho&quot;) Figura 8.1: Diagrama de dispersión de las longitudes y anchuras de los pétalos de las flores de la tabla iris. Para tablas de datos de tres columnas numéricas, podemos usar con un fin similar la instrucción scatterplot3d del paquete homónimo, que dibuja un diagrama de dispersión tridimensional. Como plot, se puede aplicar a un data frame o a una matriz; por ejemplo, para representar gráficamente las tres primeras variables numéricas de iris, podríamos usar el código siguiente y obtendríamos la Figura 8.2: library(scatterplot3d) scatterplot3d(iris[ , 1:3], pch=20) Figura 8.2: Diagrama de dispersión tridimensional de las tres primeras columnas de la tabla iris. Podéis consultar la Ayuda de la instrucción para saber cómo modificar su apariencia: cómo ponerle un título, poner nombres adecuados a los ejes, usar colores, cambiar el estilo del gráfico, etc. Una representación gráfica muy popular de las tablas de datos de tres o más columnas numéricas son las matrices formadas por los diagramas de dispersión de todos sus pares de columnas. Si la tabla de datos es un data frame, esta matriz de diagramas de dispersión se obtiene simplemente aplicando la función plot al data frame; por ejemplo, la instrucción plot(iris[ , 1:4]) produce el gráfico de la Figura 8.3. En este gráfico, los cuadrados en la diagonal indican a qué variables corresponden cada fila y cada columna, de manera que podamos identificar fácilmente qué variables compara cada diagrama de dispersión; así, en el diagrama de la primera fila y segunda columna de esta figura, las abscisas corresponden a anchuras de sépalos y las ordenadas a longitudes de sépalos. Observad que la nube de puntos no muestra una tendencia clara y en todo caso ligeramente negativa, lo que se corresponde con la correlación de Pearson entre estas variables de -0.118 que hemos obtenido en la sección anterior. Figura 8.3: Matriz de diagramas de dispersión de la tabla iris. Podemos usar los parámetros usuales de plot para mejorar el gráfico resultante; por ejemplo, podemos usar colores para distinguir las flores según su especie. Así, la instrucción siguiente produce el gráfico de la Figura 8.4. plot(iris[ , 1:4], col=iris$Species, pch=20, cex=0.7) Figura 8.4: Matriz de diagramas de dispersión de la tabla iris, con las especies distinguidas por colores. Para obtener la matriz de diagramas de dispersión de una tabla de datos multidimensional también se puede usar la función pairs: así, pairs(iris[, 1:4]) produce exactamente el mismo gráfico que plot(iris[, 1:4]). La ventaja principal de pairs es que se puede aplicar a una matriz para obtener la matriz de diagramas de dispersión de sus columnas, mientras que plot no. El paquete car incorpora una función que permite dibujar matrices de diagramas de dispersión enriquecidos con información descriptiva extra de las variables de la tabla de datos y que además facilita el control del gráfico resultante, por lo que os recomendamos su uso frente a las funciones básicas plot y pairs. Se trata de la función spm (abreviatura de scatterplotMatrix); por ejemplo, el código siguiente produce el gráfico la Figura 8.5. library(car) spm(iris[ , 1:4], var.labels=c(&quot;Long. Sep.&quot;,&quot;Ancho Sep.&quot;,&quot;Long. Pet.&quot;,&quot;Ancho Pet.&quot;)) Figura 8.5: Una matriz de diagramas de dispersión de la tabla iris producida con la función spm. Observad para empezar que hemos cambiado los nombres que identifican las variables en los cuadrados de la diagonal, con el parámetro var.labels, y que en dichos cuadrados aparecen además unas curvas: se trata de la curva de densidad estimada de la variable correspondiente de la que hablábamos en la Lección ?? de la primera parte del curso. La información gráfica contenida en estos cuadrados de la diagonal se puede modificar con el parámetro diagonal: podemos pedir, por ejemplo, que dibuje un histograma de cada variable (con diagonal=list(method =&quot;histogram&quot;)) o su boxplot (con diagonal=list(method=&quot;boxplot&quot;)) o un QQ-plot (con diagonal=list(method=&quot;qqplot&quot;)). Así, el código siguiente produce el gráfico la Figura 8.6. spm(iris[ , 1:4], var.labels=c(&quot;Long. Sep.&quot;,&quot;Ancho Sep.&quot;,&quot;Long. Pet.&quot;,&quot;Ancho Pet.&quot;), diagonal=list(method=&quot;boxplot&quot;), pch=20,cex=0.75) Figura 8.6: Matriz de diagramas de dispersión de la tabla iris con boxplots en la diagonal. Observad también que los diagramas de dispersión de la matriz producida con spm contienen algunas líneas. La línea recta es la recta de regresión por mínimos cuadrados y, sin entrar en detalle sobre su significado exacto, las curvas discontinuas representan la tendencia de los datos. Podéis eliminar la recta de regresión con regLine=FALSE (no os lo recomendamos) y las curvas discontinuas con smooth=FALSE; si las queréis mantener, consultad la Ayuda de la función para saber cómo cambiar su estilo, color, etc. A veces querremos agrupar los datos de las variables numéricas de una tabla de datos. Los motivos serán los mismos que cuando se trata de una sola variable: por ejemplo, si los datos son aproximaciones de valores reales, o si son muy heterogéneos. Cuando tenemos dos variables emparejadas agrupadas, se pueden representar gráficamente las frecuencias de sus pares de clases mediante un histograma bidimensional, que divide el conjunto de todos los pares de valores en rectángulos definidos por los pares de intervalos e indica sobre cada rectángulo su frecuencia absoluta, por ejemplo mediante colores o intensidades de gris (dibujar barras verticales sobre las regiones es una mala idea, las de delante pueden ocultar las de detrás). Hay muchos paquetes de R que ofrecen funciones para dibujar histogramas bidimensionales; aquí explicaremos la función hist2d del paquete gplots. Su sintaxis básica es hist2d(x,y, nbins=..., col=...) donde: x e y son los vectores de primeras y segundas coordenadas de los puntos. Si son las dos columnas de un data frame de dos variables, lo podemos entrar en su lugar. nbins sirve para indicar los números de clases: podemos igualarlo a un único valor, y tomará ese número de clases sobre cada vector, o a un vector de dos entradas que indiquen el número de clases de cada vector. col sirve para especificar los colores a usar. Por defecto, los rectángulos vacíos aparecen de color negro, y el resto se colorean con tonalidades de rojo, de manera que los tonos más cálidos indican frecuencias mayores. Además, podemos usar los parámetros usuales de plot para poner un título, etiquetar los ejes, etc. A modo de ejemplo, vamos a dibujar el histograma bidimensional de las longitudes y anchuras de los pétalos de las flores iris, agrupando ambas dimensiones en los números de clases que da la regla de Freedman-Diaconis (y que calcula la función nclass.FD): library(gplots) hist2d(iris$Petal.Length, iris$Petal.Width, nbins=c(nclass.FD(iris$Petal.Length),nclass.FD(iris$Petal.Width))) Obtenemos (junto con una serie de información en la consola que hemos omitido) la Figura 8.7, que podéis comparar con el diagrama de dispersión de los mismos datos de la Figura 8.1. Figura 8.7: Histograma bidimensional de longitudes y anchuras de pétalos de flores iris. En los histogramas bidimensionales con muchas regiones de diferentes frecuencias, es conveniente usar de manera adecuada los colores para representarlas. Una posibilidad es usar el paquete RColorBrewer, que permite elegir esquemas de colores bien diseñados. Las dos funciones básicas son: brewer.pal(n,&quot;paleta predefinida&quot;), que carga en un vector de colores (una paleta) una secuencia de \\(n\\) colores de la paleta predefinida en el paquete. Los nombres y contenidos de todas las paletas predefinidas que se pueden usar en esta función se obtienen, en la ventana de gráficos, ejecutando la instrucción display.brewer.all(). Por ejemplo, la paleta de colores de la Figura 8.8 se define con el código siguiente: brewer.pal(11,&quot;Spectral&quot;) Figura 8.8: Paleta brewer.pal(11,“Spectral”) colorRampPalette(brewer.pal(...))(m), produce una nueva paleta de \\(m\\) colores a partir del resultado de brewer.pal, interpolando nuevos colores. Luego se puede usar la función rev para invertir el orden de los colores, lo que es conveniente en los histogramas bidimensionales si queremos que las frecuencias bajas correspondan a tonos azules y las frecuencias altas a tonos rojos. Así, la paleta de colores que se define con rev(colorRampPalette(brewer.pal(11,&quot;Spectral&quot;))(50)) es la de la Figura 8.9. Figura 8.9: Paleta rev(colorRampPalette(brewer.pal(11,“Spectral”))(50)) Vamos a usar esta última paleta en un histograma bidimensional de la tabla de alturas de padres e hijos recogidas por Karl Pearson en 1903 y que tenemos guardada en el url https://raw.githubusercontent.com/AprendeR-UIB/Material/master/pearson.txt; el resultado es la Figura 8.10. library(RCurl) df_pearson=read.table(text=getURL(&quot;https://raw.githubusercontent.com/AprendeR-UIB/Material/master/pearson.txt&quot;),header=TRUE) hist2d(df_pearson, nbins=30, col=rev(colorRampPalette(brewer.pal(11,&quot;Spectral&quot;))(50))) Figura 8.10: Histograma bidimensional de las alturas de padres e hijos recogidas por Karl Pearson. Para terminar, veamos cómo producir un gráfico conjunto de un histograma bidimensional y los dos histogramas unidimensionales. Se trata de una modificación del gráfico similar explicado en http://www.everydayanalytics.ca/2014/09/5-ways-to-do-2d-histograms-in-r.html, el cual a su vez se inspira en un gráfico de la p. 62 de Computational Actuarial Science with R de Arthur Charpentier (Chapman and Hall/CRC, 2014). Considerad la función siguiente, cuyos parámetros son un data frame df de dos variables y un número n de clases, común para las dos variables: hist.doble=function(df,n){ par.anterior=par() h1=hist(df[,1], breaks=n, plot=F) h2=hist(df[,2], breaks=n, plot=F) m=max(h1$counts, h2$counts) par(mar=c(3,3,1,1)) layout(matrix(c(2,0,1,3), nrow=2, byrow=T), heights=c(1,3), widths=c(3,1)) hist2d(df, nbins=n, col=rev(colorRampPalette(brewer.pal(11,&quot;Spectral&quot;))(50))) par(mar=c(0,2,1,0)) barplot(h1$counts, axes=F, ylim=c(0, m), col=&quot;red&quot;) par(mar=c(2,0,0.5,1)) barplot(h2$counts, axes=F, xlim=c(0, m), col=&quot;red&quot;, horiz=T) par.anterior} Entonces, la instrucción hist.doble(df_pearson,25) produce la Figura 8.11. Figura 8.11: Histograma bidimensional con histogramas unidimensionales de las alturas de padres e hijos recogidas por Karl Pearson. Algunas explicaciones sobre el código, por si lo queréis modificar: Hemos “simulado” los histogramas mediante diagramas de barras de sus frecuencias absolutas, para poder dibujar horizontal el de la segunda variable. El parámetro axes=FALSE en los barplot indica que no dibuje sus ejes de coordenadas. La función par establece los parámetros generales básicos de los gráficos. Como con esta función los modificamos, guardamos los parámetros anteriores en par.anterior y al final los restauramos. El parámetro mar de la función par sirve para especificar, por este orden, los márgenes inferior, izquierdo, superior y derecho de la próxima figura, en números de líneas. La instrucción layout divide la figura a producir en sectores con la misma estructura que la matriz de su primer argumento. Dentro de esta matriz, cada entrada indica qué figura de las próximas se ha de situar en ese sector. Las alturas y amplitudes relativas de los sectores se especifican con los parámetros heights y widths, respectivamente. Así, la instrucción layout(matrix(c(2,0,1,3),nrow=2,byrow=T), heights=c(1,3),widths=c(3,1)) divide la figura en 4 sectores. Los sectores de la izquierda serán el triple de anchos que los de la derecha (widths=c(3,1)), y los sectores inferiores serán el triple de altos que los superiores (heights=c(1,3)). En estos sectores, R dibujará los próximos gráficos según el esquema definido por la matriz del argumento: \\[ \\left(\\begin{array}{cc} \\mbox{segundo} &amp; \\mbox{ninguno}\\\\ \\mbox{primero} &amp; \\mbox{tercero} \\end{array}\\right). \\] 8.8 Guía rápida sapply(data_frame,función) aplica la función a las columnas del data_frame. scale sirve para aplicar una transformación lineal a una matriz o a un data frame. Sus parámetros son: center: especifica el vector que restamos a sus columnas; por defecto, el vector de medias muestrales. scale: especifica el vector por el que dividimos sus columnas; por defecto, el vector de desviaciones típicas muestrales. cov, aplicada a dos vectores, calcula su covarianza muestral; aplicada a un data frame o a una matriz, calcula su matriz de covarianzas muestrales. Dispone del parámetro use, que: Para dos vectores: Igualado a &quot;complete.obs&quot;, calcula las covarianzas teniendo en cuenta sólo sus observaciones completas (las posiciones en las que ninguno de los dos vectores tiene un NA). Para más de dos vectores: Igualado a &quot;pairwise.complete.obs&quot;, calcula la covarianza de cada par de columnas teniendo en cuenta sólo sus observaciones completas, independientemente del resto de la tabla; es decir, como si en el cálculo de la covarianza de cada par de columnas usáramos use=&quot;complete.obs&quot;, sin tener en cuenta que forman parte de una tabla de datos con más columnas. Igualado a &quot;complete.obs&quot;, calcula las covarianzas de las columnas teniendo en cuenta sólo las filas completas de toda la matriz. cor, aplicada a dos vectores, calcula su correlación de Pearson; aplicada a un data frame o a una matriz, calcula su matriz de correlaciones de Pearson. Se puede usar el parámetro use de cov. Usando el parámetro method=&quot;spearman&quot; calcula la correlación (o las correlaciones, si se aplica a un data frame o a una matriz) de Spearman. cor.test realiza un contraste de correlación, con hipótesis nula que la correlación poblacional sea 0. Su sintaxis es la usual en funciones de contrastes. pwr.r.test del paquete pwr, sirve para calcular la potencia de un contraste de correlación. Sus parámetros son n, el tamaño de las muestras, r, su correlación de Pearson, sig.level, el nivel de significación, power, la potencia, y alternative, el tipo de contraste. Si se entran los valores de tres de los cuatro primeros parámetros, se obtiene el cuarto. upper.tri, aplicada a una matriz cuadrada M, produce la matriz triangular superior de valores lógicos del mismo orden que M. Con el parámetro diag=TRUE se impone que el triángulo de valores TRUE incluya la diagonal principal. lower.tri, aplicada a una matriz cuadrada M, produce la matriz triangular inferior de valores lógicos del mismo orden que M. Dispone del mismo parámetro diag=TRUE. order ordena el primer vector al que se aplica, desempatando empates mediante el orden de los vectores subsiguientes a los que se aplica; el parámetro decreasing=TRUE sirve para especificar que sea en orden decreciente. plot, aplicado a un data frame de dos variables numéricas, dibuja su diagrama de dispersión; aplicado a un data frame de más de dos variables numéricas, produce la matriz formada por los diagramas de dispersión de todos sus pares de variables. pairs es equivalente a plot en el sentido anterior, y se puede aplicar a matrices. spm del paquete cars, produce matrices de dispersión más informativas y fáciles de modificar. scatterplot3d del paquete scatterplot3d, dibuja diagramas de dispersión tridimensionales. hist2d del paquete gplots, dibuja histogramas bidimensionales. Dispone de los parámetros específicos siguientes: nbins: indica los números de clases. col: especifica la paleta de colores que ha de usar para representar las frecuencias. brewer.pal(n,&quot;paleta predefinida&quot;) del paquete RColorBrewer, carga en una paleta de colores una secuencia de n colores de la paleta predefinida en dicho paquete. colorRampPalette(brewer.pal(...))(m) del paquete RColorBrewer, genera una nueva paleta de \\(m\\) colores a partir del resultado de brewer.pal, interpolando nuevos colores. display.brewer.all() del paquete RColorBrewer, muestra los nombres y contenidos de todas las paletas predefinidas en dicho paquete. par sirve para establecer los parámetros generales básicos de los gráficos. layout divide en sectores la figura a producir, para que pueda incluir varios gráficos independientes simultáneamente. 8.9 Ejercicios Modelo de test (1) Considerad la matriz de datos \\[\\left(\\begin{array}{ccc} 10.6 &amp; 2.4 &amp; 7.5 \\\\ 7.4 &amp; 3.7 &amp; 10.9\\\\ 10.7 &amp; 2.6 &amp; 9.6 \\\\ 8.4 &amp; 4.9 &amp; 9.9\\\\16.7 &amp; 6.2 &amp; 13.2 \\\\ 11.3 &amp; 4.3 &amp; 7.7\\end{array} \\right).\\] Calculad la entrada (4,2) de su matriz de datos tipificada, redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha. (2) Considerad la matriz de datos \\[\\left(\\begin{array}{ccc} 10.6 &amp; 2.4 &amp; 7.5 \\\\ 7.4 &amp; 3.7 &amp; 10.9\\\\ 10.7 &amp; 2.6 &amp; 9.6 \\\\ 8.4 &amp; 4.9 &amp; 9.9\\\\16.7 &amp; 6.2 &amp; 13.2 \\\\ 11.3 &amp; 4.3 &amp; 7.7\\end{array}\\right).\\] Calculad la covarianza muestral \\(\\widetilde{s}_{3,2}\\), redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha. (3) Considerad la matriz de datos \\[\\left(\\begin{array}{ccc} 10.6 &amp; 2.4 &amp; 7.5 \\\\ 7.4 &amp; 3.7 &amp; 10.9\\\\ 10.7 &amp; 2.6 &amp; 9.6 \\\\ 8.4 &amp; 4.9 &amp; 9.9\\\\16.7 &amp; 6.2 &amp; 13.2 \\\\ 11.3 &amp; 4.3 &amp; 7.7\\end{array}\\right).\\] Calculad la correlación de Pearson \\(r_{3,2}\\), redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha. (4) Empleando la función cor.test, realizad el contraste bilateral de correlación entre el perímetro del tronco y la altura de los cerezos negros americanos usando la muestra del dataframe trees. Dad el p-valor redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha, e indicad si la conclusión, con un nivel de significación del 5%, es que hay correlación o no entre estas dos variables, escribiendo SI o NO, según corresponda. Separad el p-valor de la conclusión con un único espacio en blanco. (5) Calculad la correlación de Spearman de los vectores \\(x=(4,8,6,9,5,9 ,4,7,10, 8)\\) e \\(y=(0,6,2,1,4,4,3,7,11,5)\\). Dad el resultado redondeado a 3 cifras decimales sin ceros innecesarios a la derecha. (6) ¿Cuál de las cuatro matrices siguientes es la matriz de covarianzas de una tabla de datos de 2 columnas y 5 filas? Solo hay una. \\[ \\begin{array}{l} A=\\left(\\begin{array}{cc} 0.7 &amp; 3 \\cr 3 &amp; 1.2 \\end{array}\\right)\\\\ B=\\left(\\begin{array}{cc} 0.6 &amp; 0.8\\cr -0.8 &amp; 0.6 \\end{array}\\right)\\\\ C=\\left(\\begin{array}{cc} 0.6 &amp; 0.8\\cr 0.8 &amp; -0.6 \\end{array}\\right)\\\\ D=\\left(\\begin{array}{ccccc} 0.7 &amp; 0.2 &amp; 1.3 &amp; 0.5&amp; -0.1\\cr 0.2 &amp; 0.7 &amp; -0.3 &amp; -0.1 &amp;-0.1\\cr 1.3 &amp; -0.3 &amp; 3.1 &amp; 1.3 &amp; 0.4\\cr 0.5&amp; -0.1&amp; 1.3 &amp; 0.6 &amp; 0.2\\cr -0.1 &amp; -0.1 &amp; 0.4 &amp; 0.2&amp; 2.9\\end{array}\\right) \\end{array} \\] Ejercicio El fichero https://raw.githubusercontent.com/AprendeR-UIB/Material/master/NotasMatesI14.csv recoge las notas medias (sobre 100) obtenidas en las diferentes actividades de evaluación de la asignatura Matemáticas I del grado de Biología, en el curso 2013/14, por parte de los estudiantes que fueron considerados “presentados” en la primera convocatoria. Estas actividades consistieron en: Dos controles (columnas Control1 y Control2). Talleres de resolución de problemas (columna Talleres). Ejercicios para resolver en casa (columna Casa). Cuestionarios en línea sobre los contenidos de la asignatura y sobre R (columnas TestsCont y TestsR, respectivamente). Cargad este fichero en un data frame. Calculad el vector de medias y el vector de desviaciones típicas de esta tabla de datos. ¿Cuáles son las actividades de evaluación cuyas notas presentan mayor y menor variabilidad? Calculad las matrices de covarianzas “verdaderas” y de correlaciones de Pearson de esta tabla de datos. ¿Qué variable tiene la mayor correlación media con las otras variables? ¿Cuál tiene la menor? Ordenad los pares de variables de esta tabla por su correlación. ¿Cuáles son los dos pares con mayor correlación? ¿Cuáles son los dos pares con menor correlación? Comprobad en esta tabla de datos que su matriz de correlaciones es igual a la matriz de covarianzas de su tabla tipificada. Dibujad una matriz de diagramas de dispersión de estas notas añadiendo en cada uno la recta de regresión lineal por mínimos cuadrados (pero sin otras curvas que indiquen la tendencia de los datos). ¿Se pueden ver en este diagrama los dos pares de actividades de evaluación con mayor correlación y los dos pares con menor correlación que habéis encontrado en el apartado (d)? Respuestas al test (1) 0.673 (2) 2.114 (3) 0.692 (4) 0.003 SI (5) 0.488 (6) A "]
]
